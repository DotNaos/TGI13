Titel: Governing Ethical Gaps in Distributed AI Development | Digital Society

Governing Ethical Gaps in Distributed AI Development | Digital Society Skip to main content Log in Menu Find a journal Publish with us Track your research Search Cart Home Digital Society Article Governing Ethical Gaps in Distributed AI Development Original Paper Open access Published: 15 February 2024 Volume 3, article number 7, (2024) Cite this article Download PDF You have full access to this open access article Digital Society Aims and scope Submit manuscript Governing Ethical Gaps in Distributed AI Development Download PDF Nandhini Swaminathan ORCID: orcid.org/0000-0002-9835-67041 & David Danks ORCID: orcid.org/0000-0003-4541-59662,3 200 Accesses Explore all metrics AbstractGood business practice often leads companies to subdivide into separate functional entities for operational efficiency and specialization. However, these kinds of divisions can generate significant ethical and perhaps even regulatory gaps when they occur in AI companies. In particular, one natural division for an AI company is into separate entities responsible for model development, testing, and cybersecurity (to maintain and protect data). In this paper, we argue that this division can lead to some ethical responsibilities always being “someone else’s job.” For concreteness, we consider the US National Institute of Standards and Technology’s AI Risk Management Framework (NIST AI RMF) as a guide to ethical obligations in a corporate context. We show that a common division of labor in AI development and deployment can lead to specific obligations for which no entity is responsible, even though they apply to the effort as a whole. We propose “Join Accountability Agreements”, a mechanism to ensure that ethical obligations do not slip through the cracks because of the way an effort is structured. We thus aim to highlight the significance of comprehensive examinations of and adaptable strategies for our ethical obligations when developing AI systems in a distributed manner. Similar content being viewed by others The Ethical Implications of Using Artificial Intelligence in Auditing Article 08 January 2020 Ivy Munoko, Helen L. Brown-Liburd & Miklos Vasarhelyi Blockchain for healthcare data management: opportunities, challenges, and future recommendations Article 07 January 2021 Ibrar Yaqoob, Khaled Salah, … Yousof Al-Hammadi Accountability in artificial intelligence: what it is and how it works Article Open access 07 February 2023 Claudio Novelli, Mariarosaria Taddeo & Luciano Floridi Use our pre-submission checklist Avoid common mistakes on your manuscript. 1 IntroductionAt a high level, AI governance is an interdisciplinary approach to ensure the ethical and responsible development, deployment, and use of artificial intelligence (AI) technologies. This can involve a range of practices, including creating legal frameworks, adhering to ethical guidelines, and implementing risk management practices (Cihon, 2019; Dafoe, 2018; Perry & Uuk, 2019). The typical goal of AI governance is to align AI systems with human well-being, respect for human autonomy, social responsibility, transparency, and other accountability principles while minimizing adverse effects.In other technical fields (e.g., cybersecurity), risk assessment and management frameworks have provided valuable tools for governance (Ahmed, 2007). Unsurprisingly, we are now seeing the emergence of AI risk assessment and management frameworks to support AI governance (Afzal, 2021; Attard-Frost, 2022; Berk, 2021; Mäntymäki, 2022; ÓhÉigeartaigh et al., 2020; Schmitt, 2022; Taeihagh, 2021). These frameworks aim to provide structured methodologies to identify, assess, and mitigate AI risks, while also promoting various positive features such as transparency, accountability, and sustainability (Schwartz et al., 2022). These frameworks thus require consideration of ethical, legal, and social implications. Many of these AI risk assessment frameworks have been developed by governmental entities, including Australia’s AI Assurance framework, the European Commission’s Assessment List for Trustworthy Artificial Intelligence (ALTAI), and the Algorithm Impact Assessment tool (AIA) by the Government of Canada (European Commission, 2020; Government of Canada, 2023; UK Information Commissioner, 2022; Chik, 2013; McKelvey and MacDonald, 2019; World Economic Forum, 2022). One particularly prominent approach is the AI Risk Management Framework (RMF) developed by the National Institute of Standards and Technology in the US (AI RMF; Tabassi, 2023a). The AI RMF aims to comprehensively articulate both general principles and goals, and also specific methods and processes to achieve those goals, for socio-technical AI systems. The AI RMF thus aspires to provide a comprehensive approach to manage AI risks, encompassing legal compliance, risk management, and ethical considerations (Schuett, 2022).The AI RMF provides a structure for risk assessment and management across an entire AI effort, from design through development into deployment and use. However, many AI systems are built in a relatively distributed fashion, with distinct entities—perhaps in the same company, perhaps in different ones—contributing different aspects. For example, one group or company might collect the data, while another does the analysis and model building, while a third sells and deploys the system worldwide. It is thus important that the different parts of risk assessment and management can be assigned to one or more of these entities to ensure that the effort as a whole is ethical. However, we argue that this cannot always be done. We focus below on one particular kind of organizational structure (Sect. 3), but the lessons clearly would apply in other cases. We first begin, though, with a discussion of the AI RMF (Sect. 2) before concluding with a proposal to ensure that ethical responsibilities do not slip through the organizational cracks (Sect. 4).2 A Primer on the NIST AI Risk Management FrameworkThe NIST AI Risk Management Framework intends to provide a comprehensive and systematic approach for organizations to navigate the complexities of AI risk management. At its core, the AI RMF comprises four essential functions (which we summarize in Fig. 1): GOVERN, MAP, MEASURE, and MANAGE, each playing a pivotal role in ensuring responsible and effective AI design, development and deployment (Tabassi, 2023b). Each of the high-level functions has a number of (sub-)tasks (omitted from Fig. 1) that operationalize the overall functional goals.Fig. 1High-level NIST Risk Management Framework overview: core functionsFull size imageThe GOVERN function includes tasks centered on establishing policies, procedures, and practices that should align with the organization’s guiding principles and strategic objectives. This foundation should enable the organization to take a proactive approach to AI risk management. Building on this foundation, the MAP function involves carefully identifying and analyzing AI-related risks and their potential ramifications so the organization can hopefully better understand the broader implications of its AI technologies on users and society. The MEASURE function is designed to make the risk management process more rigorous by ensuring that appropriate quantitative and qualitative tools are used to obtain objective, transparent insights for decision-making. Finally, tasks in the MANAGE function help to synthesize insights from the other functions to prioritize and address AI risks, including targeted responses and resource allocations.Ideally, the tasks in the four NIST AI RMF functions produce a risk management approach that aligns technical design with organizational values in beneficial ways. At the time of this writing, there have not been any systematic studies of the efficacy of the AI RMF in enhancing AI system trustworthiness, optimizing performance, or contributing positively to societal well-being.3 Division of an AI Effort and Its Ethical ImplicationsIn practice, AI efforts often occur in a distributed manner, as different entities have different expertise and capabilities. Functional modules, or distinct units within an AI project each focusing on a specific aspect of development, allow for this distribution of tasks. Although some companies or organizations can perform every step in the lifecycle of an AI effort, they do this through different sub-groups (e.g., different divisions within the company) that typically have different roles.Footnote 1 By allocating distinct functions to distinct entities, we can reap the benefits of specialization and experience on specific tasks. Figure 2 provides our preferred way of decomposing the lifecycle of an AI system into modular elements, though we note that other divisions could be used instead. The key is that modern AI efforts are almost always pursued in a more modular way, as this approach can promote efficiency and effectiveness within each functional domain through streamlined workflows, optimized resource allocation, and improved performance. Of course, the process is rarely as clean-cut as suggested by Fig. 2: boundaries between functional modules can be blurry; later modules might need to revisit earlier decisions; and so on. Nonetheless, we contend that Fig. 2 is a useful approximation to the practice of modern AI system creation.Fig. 2AI engineering processFull size imageOne very natural division is into three distinct units: a model development group, a testing group, and a cybersecurity team that ensures the security and integrity of the data and models.Footnote 2 These could be three different divisions within the same company, or three different legal entities (e.g., the model development group might be a company that specializes in AI as a Service), or some combination. The model development team constructs and tailors the AI model according to the agreed-upon specifications. The testing unit assesses the AI system’s performance and functionality. And the cybersecurity unit safeguards the AI system against threats and attacks. Collectively, these three units play a pivotal role in the overall engineering process, as detailed in Table 1.Table 1 Overlap of labor during the engineering processFull size tableHowever, this separation of responsibilities brings forth its own set of ethical challenges. The AI industry, especially in its early days and even today to some extent, has seen cases where important aspects of AI development are contracted out to different entities or segregated within the same organization without comprehensive contractual clarification on responsibility and liability. This is not unique to AI; many nascent industries often face similar challenges. This division of cognitive labor can thus raise significant questions regarding accountability in cases where negative consequences arise from the technology. Determination of (ethical) responsibility can be challenging, as each may argue that its role was limited to its specific function. For instance, suppose the AI system built by these entities exhibits biased or discriminatory behavior. The testing team may argue that its role did not involve developing or implementing the technology, and so it should not be held liable for biases. On the other hand, the model development unit could claim that its responsibility was solely to construct a functional model, shifting the blame elsewhere.4 Accountability Gaps4.1 Responsibility Through the CracksThese efforts, though practical and necessary, create a cascade of challenges that are unaddressed by the NIST AI RMF. These include unclear accountability for AI risks, difficulties in fostering a unified culture of risk awareness, and complexities in standardizing engagement with third parties. Additionally, the compartmentalization may obstruct a holistic view of AI system risks, lead to inconsistent tracking and prioritization of risks, and complicate the documentation and monitoring of risk mitigation strategies. As such, it becomes vital to have strong communication and oversight to counter these challenges. In some cases, there will be legal contracts or internal organizational rules and processes that address issues such as legal liability. However, those documents (including terms & conditions) essentially never consider ethical risks or liability.The functions of the NIST AI RMF do not map onto the functional modules through which AI systems are created, and so some of the RMF functions can slip through the cracks with no one assuming ownership. If the use of the NIST AI RMF were legally required, then a regulator (or other governmental entity) could perhaps ensure that nothing slips through the cracks. More generally, if there is a single locus of oversight or coordination for use of a risk assessment framework in AI creation, then one could potentially ensure that no ethical responsibility slips through the cracks. We are not, however, in such a world, and so commitment to using, say, the AI RMF does not thereby ensure that all relevant ethical risks will be considered.4.2 Joint Accountability AgreementsWe propose the establishment of Joint Accountability Agreements (JAA) as a potential solution to address the accountability and responsibility challenges emerging from the scenario of separate entities within an AI effort. In particular, we propose that JAAs can help to ensure accountability when work occurs across multiple entities, taking into account the unique characteristics of AI systems and their societal impacts. Similar ideas for AI accountability have been independently developed (e.g., Berscheid & Roewer-Despres, 2019), though they are not grounded in risk assessment frameworks such as the AI RMF or other mechanisms.Building upon the existing GOVERN function of the NIST AI RMF, which focuses on key aspects such as legal compliance, risk management, and ethical considerations, we propose that distinct functional modules should establish JAAs between them. These agreements would clearly outline the ethical roles, responsibilities, and accountabilities, including the following components: 1. Roles, responsibilities, and resource allocation 2. Legal and regulatory compliance 3. Agreed-upon framework for risk management 4. Safety, ethical standards, and performance metrics 5. Liability and accountability protocols 6. Monitoring and auditing procedures 7. Termination and transition. In many ways, a JAA parallels existing legal agreements when work occurs in a distributed fashion (e.g., contracts, spec sheets, etc.), but we propose that these should focus on accountability and responsibility obligations rather than technical or financial ones. In particular, these seven components can be used to ensure that relevant tasks of the GOVERN function are satisfied, even though no single unit has primary responsibility for them (i.e., they would otherwise slip through the cracks): GOVERN 1.1 (Legal and Regulatory Requirements): Establish JAA alongside understanding, managing, and documenting legal and regulatory requirements. GOVERN 4.1 (Fostering a Safety-first Mindset): Adhere to the agreed-upon standards to mitigate negative impacts. GOVERN 6.1 (Third-Party Entity Risks): Define the responsibilities and obligations of third-party entities through JAA to ensure coordination, communication, and accountability. More generally, in the scenario being discussed, the three entities—the model development team, the testing team, and the cybersecurity team—could decide to divide the NIST AI RMF functions among themselves using JAAs, which would lead to a more comprehensive approach to AI risk governance. This arrangement would allow each entity to focus on its specific areas of expertise, fostering better coordination and a more cohesive risk management strategy, while still ensuring that the overall project is ethical. For instance, the model development entity could oversee most of the GOVERN, MAP, and MEASURE functions. The testing entity could concentrate on the MEASURE and MANAGE functions. The cybersecurity entity could handle portions of the GOVERN and MANAGE functions. This division would ensure that each function gets the right expertise and promotes effective collaboration. It also clarifies roles, fostering accountability within the NIST AI RMF framework. Yet, with this clarity in roles comes the imperative of upholding their respective responsibilities. Any breach of the terms outlined in the JAA could invoke consequences pre-determined by the involved entities, varying from restorative measures for minor infractions to termination of the partnership or even initiation of legal proceedings for major breaches. This acts as a safeguard, ensuring accountability within the NIST AI RMF framework.At the current time, there are no legal mandates to use particular risk assessment frameworks, though such requirements may be forthcoming. Regardless of the legal requirements, however, JAAs provide a way for modular efforts to ensure that they are satisfying their local ethical obligations within an overall plan that covers all relevant ethical requirements. We thus contend that there is ethical value to JAAs, even if they are not legally required (at the moment). We also emphasize that we have deliberately not specified the exact structure of JAAs (beyond the seven components) as we believe that these will often depend on the particular entities and goals for an AI effort.4.3 Recommended Solutions for Additional VulnerabilitiesWhile the establishment of Joint Accountability Agreements (JAAs) could represent a significant step towards addressing accountability and responsibility challenges, there are additional concerns that arise for risk assessment frameworks applied to AI. One issue is the expertise and adherence to the framework required for the involved entities. For example, the testing and cybersecurity modules may not primarily specialize in AI, and so could possess a limited understanding of the NIST AI RMF principles and practices, resulting in suboptimal implementation. This discrepancy could lead to inconsistent application of risk management strategies and potential oversight of critical vulnerabilities. One response would look to voluntary training and certification, as they could help to ensure that all teams involved have appropriate knowledge bases. A different (not mutually exclusive) response would be to better characterize the knowledge, skills, and processes required for the different AI RMF (sub-)tasks so that the functional modules can ensure that they have the needed expertise.Additionally, the lack of common standards or protocols for AI technologies presents a notable challenge, particularly in industries that rely on multi-vendor and multiplatform AI solutions. Without standardization, interoperability (including ethical interoperability; Danks & Trusilo, 2022) between AI systems may be compromised, resulting in inefficiencies and diminished effectiveness. Furthermore, non-standardized AI technologies can introduce security vulnerabilities and unintended consequences that can impact stakeholders and overall system performance. For instance, in the context of predictive maintenance in industrial settings, the absence of common standards may impede the seamless integration of predictive models from different vendors, thereby reducing maintenance effectiveness and increasing the risk of equipment failure. Encouraging the adoption of common standards within the NIST AI RMF can facilitate interoperability, enhance system security, and promote a harmonized approach to AI risk management across industries.5 ConclusionIn our paper, we examine the challenges and potential issues arising from dividing an AI effort into separate entities responsible for model development, cybersecurity, and testing. We found that such a division creates challenges in coordination, communication, responsibility allocation, and the possibility of overlooking critical vulnerabilities due to fragmented oversight.We have focused on the NIST AI RMF to provide specification about these concerns and proposed incorporating Joint Accountability Agreements (JAAs) into the GOVERN function. JAAs aim to improve AI risk governance and accountability among the involved entities by ensuring that all parties share responsibility for the AI system’s performance, ethical alignment, and risk mitigation strategies. The integration of JAAs can foster collaboration by establishing a shared understanding of the entities’ roles, responsibilities, and expectations.At the same time, our focus on the AI RMF inevitably means that some issues have received less attention. For example, the AI RMF is largely focused on analysis of data rather than the data itself. However, there is increasing awareness of the importance of data for the performance (including ethical implications) of AI systems, particularly large language models and other data-intensive models. Ethical responsibilities could fail to be met because of the division of labor between data collectors and data modelers, and so JAAs could also prove useful in this case, even if they are not required to meet the demands of the AI RMF.Furthermore, JAAs are arguably important and valuable across a range of other AI risk frameworks like the European Union’s ALTAI, Australia’s AI Assurance Framework, and Canada’s Algorithmic Impact Assessment (AIA). Each of these frameworks emphasizes somewhat different aspects: ALTAI focuses on trustworthiness, Australia’s framework emphasizes ethical principles and self-assessment, and Canada’s AIA prioritizes public engagement and transparency. However, none of them address the nuances that emerge from multi-entity collaborations; in particular, they do not have mechanisms to ensure that ethical responsibilities do not slip through the cracks.Our analysis highlights the importance of establishing clear accountability and responsibility among separate entities involved in AI development. By proposing the integration of JAAs alongside the NIST AI RMF, we contribute to the ongoing conversation on AI risk management and governance, emphasizing the need for comprehensive and adaptable strategies that align with ethical standards and societal values. However, we acknowledge that implementing JAAs requires a high degree of trust and transparency between entities, which may not always be feasible in competitive environments or when dealing with sensitive information. Furthermore, the effectiveness of JAAs depends on the willingness and capacity of the entities to collaborate and hold each other accountable. Future research should explore alternative mechanisms for fostering accountability and responsibility in such situations.As the AI landscape continues to evolve rapidly, it is crucial to develop comprehensive and adaptable AI governance frameworks that address such unique challenges and ensure that AI systems contribute positively to societal well-being and adhere to ethical principles. At the same time, we must work to ensure that the distributed nature of AI system creation does not create cracks through which ethical responsibility can slip. Data Availability Not applicable. NotesAcademic efforts are perhaps the exception that proves the rule, as a single small academic research group may have to do everything itself.We omit the critically important unit for Data Collection because that step largely falls outside of the scope of the NIST AI RMF, which is the framework that we consider for specificity. We emphasize, though, that the inclusion of additional units for Data Collection (or Monitoring, or other functional tasks) would only worsen matters in terms of ethical gaps in risk assessment frameworks. That is, our discussion here is arguably the easiest case for risk assessment (and it is still problematic).ReferencesAfzal, F., Yunfei, S., Nazir, M., & Bhatti, S. M. (2021). A review of artificial intelligence based risk assessment methods for capturing complexity-risk interdependencies: Cost overrun in construction projects. International Journal of Managing Projects in Business, 14(2), 300–328. https://doi.org/10.1108/IJMPB-02-2019-0047Article Google Scholar Ahmed, A., Kayis, B., & Amornsawadwatana, S. (2007). A review of techniques for risk management in projects. Benchmarking: An International Journal, 14(1), 22–36. https://doi.org/10.1108/14635770710730919Article Google Scholar Attard-Frost, B., De Los Rıos, A., & Walters, D. R. (2022). The ethics of AI business practices: A review of 47 AI ethics guidelines. AI and Ethics, 1–18. https://doi.org/10.2139/ssrn.4034804Aziz, S., & Dowling, M. (2019). Machine learning and AI for risk management. In T. Lynn, J. G. Mooney, P. Rosati, & M. Cummins (Eds.), Disrupting finance: FinTech and strategy in the 21st century (pp. 33–50). Springer International Publishing. https://doi.org/10.2139/ssrn.3201337Chapter Google Scholar Berk, R. A. (2021). Artificial intelligence, predictive policing, and risk assessment for law enforcement. Annual Review of Criminology, 4, 209–237. https://doi.org/10.1146/annurev-criminol-051520-012342Article Google Scholar Berscheid, J., & Roewer-Despres, F. (2019). Beyond transparency: A proposed framework for accountability in decision-making AI systems. AI Matters, 5(2), 13–22. https://doi.org/10.1145/3340470.3340476Article Google Scholar Chik, W. B. (2013). The Singapore Personal Data Protection Act and an assessment of future trends in data privacy reform. Computer Law & Security Review, 29(5), 554–575. https://doi.org/10.1016/j.clsr.2013.07.010Article Google Scholar Cihon, P. (2019). Standards for AI governance: International standards to enable global coordination in AI research & development. Future of Humanity Institute. University of Oxford.Dafoe, A. (2018). AI Governance: A research agenda (Vol. 1442, p. 1443). Governance of AI Program, Future of Humanity Institute, University of Oxford. Google Scholar Danks, D., & Trusilo, D. (2022). The challenge of ethical interoperability. Digital Society, 1, 11.Article Google Scholar European Commission. (2020). The assessment list for trustworthy artificial intelligence. European Commission High-Level Expert Group on Artificial Intelligence. https://altai.insight-centre.org/Garvey, C. (2018). AI risk mitigation through democratic governance: Introducing the 7-dimensional AI risk horizon. In Proceedings of the 2018 AAAI/ACM conference on AI, ethics, and society (pp. 366–367). https://doi.org/10.1145/3278721.3278801Chapter Google Scholar Government of Canada. (2023). Algorithmic impact assessment tool. Government of Canada. https://www.canada.ca/en/government/system/digital-government/digital-government-innovations/responsible-use-ai/algorithmic-impact-assessment.htmlMäntymäki, M., Minkkinen, M., Birkstedt, T., & Viljanen, M. (2022). Defining organizational AI governance. AI and Ethics, 2(4), 603–609. https://doi.org/10.1007/s43681-022-00143-xArticle Google Scholar McGilvray, D. (2021). Executing data quality projects: Ten steps to quality data and trusted information (TM). Academic Press. Google Scholar McKelvey, F., & MacDonald, M. (2019). Artificial intelligence policy innovations at the Canadian federal government. Canadian Journal of Communication, 44(2), PP–43. https://doi.org/10.22230/cjc.2019v44n2a3509Article Google Scholar Model AI governance framework. (n.d.). Personal Data Protection Commission Singapore. https://www.pdpc.gov.sg/help-and-resources/2020/01/model-aigovernance-frameworkNsw artificial intelligence assurance framework. (n.d.). Australia NSW Government. https://www.digital.nsw.gov.au/policy/artificial-intelligence/nsw-artificialintelligence-assurance-frameworkÓhÉigeartaigh, S. S., Whittlestone, J., Liu, Y., Zeng, Y., & Liu, Z. (2020). Overcoming´ barriers to cross-cultural cooperation in AI ethics and governance. Philosophy and Technology, 33, 571–593. https://doi.org/10.1007/s13347-020-00402-xArticle Google Scholar Perry, B., & Uuk, R. (2019). AI governance and the policymaking process: Key considerations for reducing AI risk. Big Data and Cognitive Computing, 3(2), 26. https://doi.org/10.3390/bdcc3020026Article Google Scholar Rfd bus012a artificial intelligence assessment tool. (n.d.). US Pennsylvania Office of Administration. https://www.oa.pa.gov/Policies/Documents/rfd-bus012a.xlsxSambasivan, N., Kapania, S., Highfill, H., Akrong, D., Paritosh, P., & Aroyo, L. (2021). “Everyone wants to do the model work, not the data work”: Data cascades in high-stakes AI. In Proceedings of the 2021 CHI conference on human factors in computing systems (pp. 1–15). ACM. Held virtually; originally Yokohama, Japan, May 8–13. https://doi.org/10.1145/3411764.3445518Chapter Google Scholar Schmitt, L. (2022). Mapping global AI governance: A nascent regime in a fragmented landscape. AI and Ethics, 2(2), 303–314. https://doi.org/10.1007/s43681021-00083-yArticle Google Scholar Schuett, J., & Anderljung, M. (2022). Comments on the initial draft of the NIST AI risk management framework.Schwartz, R., Vassilev, A., Greene, K., Perine, L., Burt, A., & Hall, P. (2022). Towards a standard for identifying and managing bias in artificial intelligence. NIST Special Publication, 1270, 1–77. Google Scholar Tabassi, E. (2023a). Artificial intelligence risk management framework (AI rmf 1.0).Tabassi, E. (2023b). Artificial intelligence risk management framework playbook. https://airc.nist.gov/AI_RMF_Knowledge_Base/Playbook Google Scholar Taeihagh, A. (2021). Governance of artificial intelligence. Policy and Society, 40(2), 137–157. https://doi.org/10.1080/14494035.2021.1928377Article Google Scholar UK Information Commissioner. (2022). AI and data protection risk toolkit. UK Information Commissioner’s Office. https://ico.org.uk/for-organisations/guide-to-data-protection/key-dp-themes/guidance-on-ai-and-data-protection/ai-and-data-protection-risk-toolkit/World Economic Forum. (2022). Artificial intelligence for children toolkit. World Economic Forum. https://www3.weforum.org/docs/WEF-Artificial-Intelligencefor-Children-2022.pdfDownload referencesFundingNot applicable.Author informationAuthors and AffiliationsComputer Science and Engineering Department, University of California, San Diego, CA, USANandhini SwaminathanThe Halıcıoğlu Data Science Institute, University of California, San Diego, CA, USADavid DanksDepartment of Philosophy, University of California, San Diego, CA, USADavid DanksAuthorsNandhini SwaminathanView author publicationsYou can also search for this author in PubMed Google ScholarDavid DanksView author publicationsYou can also search for this author in PubMed Google ScholarContributionsN.S. devised the project, the main conceptual ideas, and the proof outline. The first draft of the manuscript was written by N.S. D.D. supervised the project. Both N.S. and D.D. contributed to the final version of the manuscript. All authors read and approved the final manuscript.Corresponding authorCorrespondence to Nandhini Swaminathan.Ethics declarations Informed Consent Not applicable. Competing Interests Not applicable. Additional informationPublisher’s NoteSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.Rights and permissions Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. Reprints and permissionsAbout this articleCite this articleSwaminathan, N., Danks, D. Governing Ethical Gaps in Distributed AI Development. DISO 3, 7 (2024). https://doi.org/10.1007/s44206-024-00088-0Download citationReceived: 11 July 2023Accepted: 16 January 2024Published: 15 February 2024DOI: https://doi.org/10.1007/s44206-024-00088-0Share this articleAnyone you share the following link with will be able to read this content:Get shareable linkSorry, a shareable link is not currently available for this article.Copy to clipboard Provided by the Springer Nature SharedIt content-sharing initiative KeywordsNIST AI Risk Management Framework (AI RMF)AI governanceRisk management strategyAccountability Use our pre-submission checklist Avoid common mistakes on your manuscript. Advertisement Search Search by keyword or author Search Navigation Find a journal Publish with us Track your research Discover content Journals A-Z Books A-Z Publish with us Publish your research Open access publishing Products and services Our products Librarians Societies Partners and advertisers Our imprints Springer Nature Portfolio BMC Palgrave Macmillan Apress Your privacy choices/Manage cookies Your US state privacy rights Accessibility statement Terms and conditions Privacy policy Help and support Cancel contracts here 46.142.67.112 Not affiliated © 2024 Springer Nature

Titel: Just a moment...

Just a moment...Enable JavaScript and cookies to continue

Titel: Just a moment...

Just a moment...Enable JavaScript and cookies to continue

Titel: Just a moment...

Just a moment...Enable JavaScript and cookies to continue

Titel: Just a moment...

Just a moment...Enable JavaScript and cookies to continue

Titel: Just a moment...

Just a moment...Enable JavaScript and cookies to continue

Titel: Governing Ethical AI: Rules & Regulations Preventing Unethical AI -

Governing Ethical AI: Rules & Regulations Preventing Unethical AI - Explore Discover Blogs Unpacking the latest trends in AI - A knowledge capsule Leadership Podcasts Know the perspective of top leaders Expert Sessions Go deep with industry leaders in live, interactive sessions Comprehensive Guides Master complex topics with comprehensive, step-by-step resources Learn Free Courses Kickstart your AI journey with our comprehensive courses Certified AI & ML BlackBelt Plus Program Master AI like a pro with our mentor-driven program Learning Paths Expert-curated paths for every goal Gen AI Pinnacle Program Learn cutting-edge skills for your AI career Engage Thriving Community Share knowledge, spark ideas, and build connections Exciting Events Dive into the hottest trends and network with industry leaders Challenging Hackathons Test your skills, unleash your creativity, and win big AI Snack Master complex topics, one bite-sized challenge at a time Contribute Become a Wordsmith Share your voice, and win big in blogathons Become a Mentor Craft careers by sharing your knowledge Become a Speaker Inspire minds, share your expertise Become an Instructor Shape next-gen innovators through our programs Corporate Our Offerings Build a data-powered and data-driven workforce Trainings Bridge your team's data skills with targeted training Analytics maturity Unleash the power of analytics for smarter outcomes Data Culture Break down barriers and democratize data access and usage Login Logout d : h : m : s Home Beginner Governing Ethical AI: Rules & Regulations Preventing Unethical AI Governing Ethical AI: Rules & Regulations Preventing Unethical AI C Chandana Surya 14 Feb, 2023 • 7 min read This article was published as a part of the Data Science Blogathon.IntroductionArtificial intelligence (AI) is rapidly becoming a fundamental part of our daily lives, from self-driving cars to virtual personal assistants. However, as AI technology advances, it is crucial to consider the ethical implications of its development and use. The use of AI in decision-making processes can lead to bias, job displacement, and privacy violations. As a result, it is important to establish guidelines and regulations to govern the ethical use of AI in today’s society.This blog post will explore the potential negative consequences of AI, current efforts to govern AI, best practices for ethical AI, the role of government and industry in governing AI ethics, and the future of AI governance. By understanding the importance of AI governance, we can work towards ensuring that AI is developed and deployed responsibly and ethically.Source: Pexels.comThe Ethical Implications of AIOne of the most significant potential negative consequences of AI is job displacement. As machines and algorithms become more sophisticated, they can perform tasks that humans once did. This can lead to significant job loss, particularly in industries that are heavily reliant on manual labor.Another potential consequence of AI is privacy violations. As AI systems collect and process large amounts of data, they can inadvertently or deliberately access and use personal information in ways that are not authorized. This can lead to breaches of privacy and loss of personal data.In addition, AI systems can perpetuate and even amplify societal biases present in their training data, leading to biased decision-making. For example, facial recognition systems have been found to have higher error rates for people with darker skin tones, and predictive policing algorithms have been found to target certain racial groups disproportionately. These biases can have serious consequences, such as discrimination and the erosion of civil liberties.It’s important to note that these potential negative consequences of AI can be mitigated by developing and implementing ethical guidelines and regulations that govern the development and use of AI. This blog post will explore best practices for AI and the role of government and industry in governing AI to help prevent these negative consequences.Current Efforts to Govern AIThere are currently a variety of existing regulations and guidelines for AI development and use. Some are specific to certain industries or types of AI applications, while others are more general in nature.One notable example is the General Data Protection Regulation (GDPR) in the European Union, which includes specific provisions related to AI. The GDPR requires that organizations using AI must provide clear and transparent information about the use of personal data and must obtain explicit consent for certain uses of data.Another example is the IEEE’s Ethically Aligned Design (EAD) guidelines, which provide a framework for designing AI systems that are aligned with human values. The guidelines cover a wide range of topics, including privacy, transparency, and accountability.Source: Pexels.comAdditionally, there are a number of industry-specific guidelines and regulations for AI. For example, the National Institute of Standards and Technology (NIST) has published guidelines for the responsible use of AI in the financial sector. Similarly, the Federal Aviation Administration (FAA) has issued guidelines for drones’ safe and ethical operation.In the United States, the government has yet to pass any federal laws specifically related to AI, but some states have passed laws, such as the Artificial Intelligence Video Interview Fairness Act in California.It’s important to note that these regulations and guidelines are still evolving and will likely change as AI technology advances. Organizations involved in AI development and use should stay informed about the latest regulations and guidelines to ensure that they are in compliance.Principles & Strategies for Ethical AIWhen it comes to ensuring responsible and ethical AI development and deployment, there are several key principles and strategies that organizations should keep in mind.Key Principles: Transparency: Organizations should be transparent about data collection and use, as well as AI decision-making processes, to build trust with users and reduce unintended consequences Accountability: Organizations should be held accountable for the actions of their AI systems and able to explain and justify decisions to ensure alignment with human values and address negative consequences Fairness: Organizations should ensure AI systems do not perpetuate societal biases in decision-making by using diverse data sets and regularly testing and monitoring systems’ performanceSource: Pexels.comKey Strategies: Implement robust testing and validation processes for AI systems to ensure they are working as intended, and errors or biases are identified and addressed Establish internal review processes to ensure compliance with relevant regulations and guidelines Invest in building a culture of ethics within the company by providing training and education on AI ethics and fostering transparency, accountability, and fairness Constantly evaluate and adapt approaches to ensure responsible and ethical AI development and deployment.Ethical AI in the Government & Private SectorBoth government and private sector organizations have important roles to play in promoting ethical AI.Government organizations have a responsibility to establish regulations and guidelines for the development and use of AI, in order to protect citizens’ rights and ensure that AI is used responsibly and ethically. This can include measures to protect citizens’ privacy, prevent discrimination, and ensure that AI systems are transparent and accountable. Government organizations can also invest in research and development to support the development of ethical AI and can provide funding and resources for the training and education of AI professionals.Private sector organizations, on the other hand, have a responsibility to ensure that their own AI systems and practices are in compliance with relevant regulations and guidelines. They should establish internal review processes to ensure that their AI systems are aligned with human values and should be transparent about the data they are collecting and how it is being used. Private sector organizations should also invest in building a culture of ethics within the company and provide their employees with training and education on AI ethics.In addition, both government and private sector organizations can work together to promote ethical AI by collaborating on research and development, sharing best practices, and participating in industry-wide initiatives and standards-setting bodies.It’s important to note that promoting ethical AI is a shared responsibility and requires a collaborative effort between the government, the private sector, and society at large.Navigating Trends & Challenges of AI GovernanceAs AI technology advances, new trends and challenges are emerging in the field of AI governance.One trend is the increasing use of AI in critical infrastructure and high-stakes decision-making, such as healthcare, transportation, and criminal justice. As AI is increasingly used in these areas, it’s crucial to ensure that these systems are safe, reliable, and unbiased.Another trend is the growing use of AI in the public sector, such as in government services and decision-making. This presents new challenges in terms of transparency, accountability, and public trust.In addition, there is a growing concern about the potential for AI to be used for malicious purposes, such as cyber-attacks and disinformation campaigns. As AI becomes more sophisticated, it is becoming easier to create realistic fake videos and images, which can be used to spread misinformation and propaganda.Source: Pexels.comTo address these challenges, governments and private sector organizations can work together to establish regulations and guidelines for using AI in critical infrastructure and high-stakes decision-making and promote transparency, accountability, and public trust. Additionally, organizations can invest in research and development to improve the security and robustness of AI systems and to develop technologies that can detect and mitigate malicious use of AI.Moreover, Governments and organizations should also invest in education and training programs to build a workforce with the necessary skills and knowledge to develop and govern AI ethically and responsibly.It’s important to keep in mind that AI governance is an ongoing process, and new challenges and trends will continue to emerge as AI technology advances. Organizations should stay informed about the latest developments in AI governance and adapt their approach accordingly.ConclusionIn conclusion, the development and use of Artificial intelligence (AI) have the potential to bring significant benefits to society, but it also presents a range of ethical and governance challenges. From job displacement and privacy violations to biased decision-making, it’s crucial to establish guidelines and regulations to govern the ethical use of AI in today’s society. The blog post has discussed the potential negative consequences of AI, current efforts to govern AI, best practices for ethical AI, the role of government and industry in governing AI, and the future of AI governance. It’s essential to understand the importance of AI governance and to work towards ensuring that AI is developed and deployed responsibly and ethically.Moreover, it’s important to note that promoting ethical AI is a shared responsibility and requires a collaborative effort between the government, the private sector, and society. As AI technology advances, new trends and challenges will emerge, and it’s crucial to stay informed and adapt the approach accordingly. The future of AI governance requires ongoing efforts and investments in research, development, education, and training to build a workforce with the necessary skills and knowledge to develop and govern AI ethically and responsibly.Takeaways: Establishing regulations and guidelines for the ethical use of AI is crucial to protect citizens’ rights and prevent negative consequences such as job displacement and privacy violations. Transparency, accountability, and fairness are key principles for ensuring responsible and ethical AI development and deployment. Government and private sector organizations are responsible for promoting ethical AI and should work together to establish regulations, share best practices, and invest in research and development. As AI technology continues to advance, new trends and challenges will emerge in AI governance, such as the use of AI in critical infrastructure and high-stakes decision-making, and malicious use of AI. Ongoing efforts and investments in research, development, education and training are necessary to build a workforce with the necessary skills and knowledge to develop and govern AI ethically and responsibly.The media shown in this article is not owned by Analytics Vidhya and is used at the Author’s discretion. AIAI EthicsArtificial Intelligenceblogathoncyber securityData Governance C Chandana Surya 14 Feb 2023 BeginnerCyber SecurityIntermediateFrequently Asked QuestionsLorem ipsum dolor sit amet, consectetur adipiscing elit,Responses From Readers Cancel reply ClearSubmit reply ΔRelated Courses 0 Hrs 17 Lessons 4.96Introduction to AI & MLFree Artificial Intelligence Machine Learning Enroll now Write, Shine, Succeed Write, captivate, and earn accolades and rewards for your work Reach a Global Audience Get Expert Feedback Build Your Brand & Audience Cash In on Your Knowledge Join a Thriving Community Level Up Your Data Science Game Rahul Shah 27 Sion Chakrabarti 16 CHIRAG GOYAL 87 Barney Darlington 5 Suvojit Hore 9 Arnab Mondal 15 Prateek Majumder 68 [tta_listen_btn class="listen"] Company About Us Contact Us CareersDiscover Blogs Expert session Podcasts Comprehensive GuidesLearn Free courses Learning path BlackBelt program Gen AIEngage Community Hackathons Events Daily challengesContribute Contribute & win Become a speaker Become a mentor Become an instructorEnterprise Our offerings Case studies Industry report quexto.aiDownload App Terms & conditions Refund Policy Privacy Policy Cookies Policy © Analytics Vidhya 2023.All rights reserved. Loading... Welcome to India's Largest Data Science Community google-icon Continue with Google email Continue with Email I accept the Terms and Conditions Please accept the TnC's to continue Receive Updates on Whatsapp Skip Loading... back Welcome Back email 2 key- Forgot Password? Log InDon't have an account yet?Register here I accept the Terms and Conditions Please accept the TnC's to continue Receive Updates on Whatsapp Loading... back Start your journey here ! user user email 2 key- Sign upAlready have an account?Login here I accept the Terms and Conditions Please accept the TnC's to continue Receive Updates on Whatsapp A verification link has been sent to your email id If you have not recieved the link please goto Sign Up page again Loading... back Please enter the OTP that is sent to your registered email id email 2 Next Loading... back Please enter the OTP that is sent to your email id email 2 Next Loading... back Please enter your registered email id email 2 NextThis email id is not registered with us. Please enter your registered email id.Don't have an account yet?Register here Loading... × back Please enter the OTP that is sent your registered email id email 2 Next Loading... × Please create the new password here key- key- SubmitWe use cookies on Analytics Vidhya websites to deliver our services, analyze web traffic, and improve your experience on the site. By using Analytics Vidhya, you agree to our Privacy Policy and Terms of Use.AcceptPrivacy & Cookies Policy Close Privacy OverviewThis website uses cookies to improve your experience while you navigate through the website. Out of these, the cookies that are categorized as necessary are stored on your browser as they are essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may affect your browsing experience. Necessary Necessary Always Enabled Necessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information. Non-necessary Non-necessary Any cookies that may not be particularly necessary for the website to function and is used specifically to collect user personal data via analytics, ads, other embedded contents are termed as non-necessary cookies. It is mandatory to procure user consent prior to running these cookies on your website. SAVE & ACCEPT

Titel: Frontiers | Governing Ethical AI Transformation: A Case Study of AuroraAI

Frontiers | Governing Ethical AI Transformation: A Case Study of AuroraAI Skip to main content Download Article Download Article Download PDF ReadCube EPUB XML (NLM) Share on Export citation EndNote Reference Manager Simple TEXT file BibTex Total views Downloads Citations Citation numbers are available from Dimensions View article impact View altmetric score SHARE ON Edited by Rebekah A. Rousi University of Vaasa, Finland Reviewed by Kimberly Houser University of North Texas, United States Aleksandr N. Raikov National Supercomputer Center, China TABLE OF CONTENTS Abstract Introduction: AI and Sociotechnical Change The Ethical Use of AI in the Context of Public Services AI as Part of a Citizen-Centric Program: Case AuroraAI Key Ethical Elements Discussion Author Contributions Funding Conflict of Interest Publisher's Note Footnotes References Export citation EndNote Reference Manager Simple TEXT file BibTex Check for updates People also looked at REVIEW article Front. Artif. Intell., 10 February 2022Sec. AI for Human Learning and Behavior Change Volume 5 - 2022 | https://doi.org/10.3389/frai.2022.836557 Governing Ethical AI Transformation: A Case Study of AuroraAI Jaana Leikas1* Aditya Johri2,3 Marko Latvanen4 Nina Wessberg1 Antti Hahto5 1VTT Technical Research Centre of Finland Ltd., Tampere, Finland 2Department of Computer Science, Aalto University, Helsinki, Finland 3Department of Information Sciences & Technology, George Mason University, Fairfax, VA, United States 4Digital and Population Data Services Agency, Helsinki, Finland 5Ministry of Finance, Helsinki, Finland How can the public sector use AI ethically and responsibly for the benefit of people? The sustainable development and deployment of artificial intelligence (AI) in the public sector requires dialogue and deliberation between developers, decision makers, deployers, end users, and the public. This paper contributes to the debate on how to develop persuasive government approaches for steering the development and use of AI. We examine the ethical issues and the role of the public in the debate on developing public sector governance of socially and democratically sustainable and technology-intensive societies. To concretize this discussion, we study the co-development of a Finnish national AI program AuroraAI, which aims to provide citizens with tailored and timely services for different life situations, utilizing AI. With the help of this case study, we investigate the challenges posed by the development and use of AI in the service of public administration. We draw particular attention to the efforts made by the AuroraAI Ethics Board in deliberating the AuroraAI solution options and working toward a sustainable and inclusive AI society. Introduction: AI and Sociotechnical Change AI has a great promise to offer solutions to the problems of humankind. It will ultimately change the structures of society and everyday lives of people in a profound way. In this development, the human focus should be on values that emphasize humanity and the understanding of the social impact of AI. AI is difficult to define, and no single universally accepted definition has been established within the scientific community. We often hear talk of weak and strong AI, the latter of which has not even been implemented in practice. AI systems are built on a variety of methods. Central areas to AI include machine learning, natural language processing, computer vision, speech recognition, planning and scheduling, optimization, robotics, and expert systems (Holton and Boyd, 2021; Pietikäinen and Silvén, 2021). To achieve a given complex goal, AI systems observe the environment, acquire data, and make inferences and decisions based on the data and information. They collect and process both structured and unstructured data and make inferences based on this data. Holton and Boyd ask where the people are in these elements of AI. The answer is not straightforward, but a great fear is that the human viewpoint remains distant in AI applications. Holton and Boyd (2021) argue that “while human consciousness retains distinctive features, these do not support an anthropocentric perspective on human–machine interactions.” One of the key questions is whether we accept the benefits of AI if we do not totally understand the impacts of the technology on society and citizens. This is something that the governments need to deal with: to see that the potential of AI can flourish. That is, to help individuals prepare, understand, accept and embrace AI, and maybe even to refuse the use AI. Beal et al. (2016) list the challenges in building AI systems as follows: 1. Difficulty in capturing expert knowledge: much of the knowledge held by experts is not explicitly written down anywhere. 2. Structural barriers to knowledge exchange: designers may not be able to access or share their knowledge due to cultural, organizational, or legal barriers. 3. Gaps in scientific knowledge: AI techniques can only produce effective improvement or automation of processes carried out by humans if the processes are well-understood in the first place. 4. Rapidly advancing knowledge and methods: how the system performance is ensured. 5. Cost of adoption vs. rapid advance: how the system performance is controlled. According to this list of challenges it seems that the most demanding goal is to understand the system and to gain the knowledge to control its performance. A human-centric AI transformation in the governance of public services requires participation from the public to ensure that the governance works as intended. As governance becomes more complex, which is the case with most developed societies, it is often hard in democracies to engage the public even in the basics of governing processes such as voting. This is especially true for systems that interact with or interface with the public at some level as it is not always obvious to the public how the process works or they might not have the expertise needed to engage with a service. This problem is further complicated by the increasingly abstract nature of the digital world where there are even more layers of complexity that can be used to hide what is going on. In addition, AI introduces another layer of complexity as even many experts lack a clear understanding of the workings of many systems that are in use. This whole process also assumes that there is a basic common understanding of the ethical and moral issues involved and this might not be true given the diversity within the population and divisions between the technology-haves and have-nots or those who can use technology more effectively than others. What practices might be address these issues in the future? What kind of a cultural change is necessary to create a mechanism for this? We want and need new or novel data and need to redefine relationships with those who are being governed. It is also about expertise and who has it and what their motivations are for creating or designing a new system. In other words, a lot of this works on trust and how we keep the governance systems trustworthy. In addition, as AI evolves, we need a new kind of ethical reflection. This means constant ethical self-examination and vigilance alongside AI development. Ethical experts, scientists, technology developers, and other relevant stakeholders need to be brought together to deliberate the ethics of AI in a multidisciplinary way. The purpose of this paper is to draw particular attention to the ethical steering efforts in developing an AI system for public administrations and finding a sustainable and inclusive solution. The paper (1) describes the way the Ethics Board of AuroraAI operates, and (2) raises essential ethical questions about the design of AI for citizens under the guidance of a public administrator. The Ethical Use of AI in the Context of Public Services The Role of Public in the Governance of AI Research has shown that although the use of AI in government has many potential benefits, it also creates challenges such as reducing citizens' trust in government (Al-Mushayt, 2019; Gupta, 2019; Sun and Medaglia, 2019), including citizens' trust in the decisions made by government (Sun and Medaglia, 2019). This distrust often emerges due to violation or perceived violation of privacy and/or through a perception of lack of fairness in the outcomes of AI systems for public governance (Kuziemski and Misuraca, 2020). In particular, the challenge in the use of AI is a lack of transparency of black-box AI systems whereby it is difficult to assign responsibility and accountability for decision-making (Dignum, 2018; Wirtz et al., 2018). Overall, the reality of using AI raises the stakes for AI use in the public sector as the risk is heightened in case of system failures as it will result in negative implications for governments and society. In theory, as AI use increases across aspects of society the public sector implementation should become better as well. In practice, this is difficult because whereas the private sector has more leeway to experiment with AI practices, the public sector has to focus on maximizing public value and public good as opposed to other outcomes (Fatima et al., 2020). In other words, the penalty for causing harm can be very high in the public sector. Consequently, the use of AI in the public sector needs to be transparent to the extent possible, in effect to gain citizens' trust (Bryson and Winfield, 2017), and to comply with the need for regular scrutiny and oversight (Desouza et al., 2020, p. 206). Finally, the complexity of using AI in the public sector arises from a diverse set of stakeholders who are involved and who often have competing interests and agendas (Desouza et al., 2020). These challenges, coupled with the rapid development of AI has created a situation where public services and administration find it difficult to keep up (Wirtz et al., 2018, p. 826) and policymakers need to pay more attention to the potential threats and challenges posed by AI. These concerns call for design and implementation of better governance structures and policy development but also for rethinking the role of the public, and the challenges they face, in working with AI-driven public services. The need for increased public engagement in the deployment and even in the design and development of AI services has been well-recognized by a range of organizations. Engagement with the public and raising public awareness serves two major functions. First, a transparent debate builds trust by involving the public, and second, better outcomes for design and implementation can be reached through public engagement. Organizations working on this issue have come up with guidelines on facilitating and including public voices. As an example, the RSA has outlined three issues that are particularly relevant for public deliberation: transparency and explainability; agency and accountability; and fairness1. Whereas, the RSA's work is focused on the short term, one time, engagement, others have argued for longer-term and ongoing engagement with the public. A sustained debate is important to influence policy decision-making and to ensure a more democratic and trustworthy process from the public's perspective. Of course, as others have pointed out, sustained engagement is important but not without its challenges including the pace of change of technology, complexity of the technology, and the inability to predict AI's trajectory and consequential potential impacts and the benefits, risks and harms which may result. These challenges are reinforced given the stakeholders involved. The organization Involve ran a series of roundtable discussions with a wide range of stakeholders working with AI and with a clear interest in public engagement in AI2. They found that there is a need to develop a shared understanding of what public engagement with AI and governance means, and how it can be achieved. They argue that engaging the public with different framings and questions could make it difficult for policy makers to make sense of the findings, diminishing the impact of public perspectives on policy decisions. They argue that those “making decisions about the design, deployment, and use of AI need to work collectively to develop more of a shared understanding of how the public should be involved in their decisions.” They concede that even though a shared understanding might not be achieved, there is value in bringing people together to voice their perspectives and acknowledge their differences. They make recommendations for public participation starting with creating “focused engagement” to overcome the generally diffuse and unfocused conversation around AI. For instance, engagement around facial recognition, credit scores, etc. is more preferred than overall generic discussions. It is also important that those working with AI “demonstrate possible futures and do more to deconstruct the hype surrounding AI.” This is needed to illustrate trade-offs and explain where AI is and is not in people's lives to increase awareness and understanding. Third, it is important to identify which publics are important to engage with on an issue. They argue that there are multiple publics and communities with multiple perspectives and views and therefore, identifying and engaging the relevant perspective is important. Data Ownership and Privacy Data openness and issues of privacy are important but not always easy to implement given the expertise that is needed to put them into practice. If we want “humans” or the public to be able to make use of these provisions or be able to safeguard their own privacy, then we need to create not just awareness but also the ability to engage meaningfully. This is a challenging goal as it would require imparting a lot of education and training. In terms of data and privacy issues, the challenges faced by the public are well-documented and range from the unethical use of data (Gupta, 2019), lack of data privacy (Valle-Cruz et al., 2019), to challenges with data security (Toll et al., 2019). Public users are worried about novel challenges to the privacy of data in AI systems for governments (Fatima et al., 2020) and how to curtail privacy violations (Kuziemski and Misuraca, 2020). The challenge to creating AI-driven services for the public come largely from what Zuiderwijk et al. (2021) refer to as the skills challenge. In their review, they document different aspects of this problem including limited knowledge about machine learning and AI among the staff (Ojo et al., 2019). Differential skills levels of people in the organization based on their function and background, inhibits cross-sectoral collaboration around AI (Mikhaylov et al., 2018). Researchers have also documented a lack of in-house AI talent (Gupta, 2019; Sun and Medaglia, 2019) coupled with gaps in education for highly technical skills (Montoya and Rivas, 2019). Overall, a lack of expertise (Al-Mushayt, 2019) coupled with increased demand for a limited number of AI experts (Wirtz et al., 2018) has resulted in the need to train more people. The challenges of the ethical use of AI in the context of public services also stems from the fear that administrative discretion may be misused (Aoki, 2020), the dependency of people on AI that would be created as increasingly services start using it (Ben Rjab and Mellouli, 2018), and the possible severe unfairness of public services that might result. Challenges to Public Understanding of and Engagement With AI Ethics There are challenges and often misconceptions of AI that need to be corrected while the symbolism surrounding it leads to myths that need to be debunked. What is novel here is how much the public are expected to learn to be able to engage with the services—about how it works, about the technology and AI in general. What does ethical AI mean for people? Given the rise in problems and challenges that are being created with the rise in the use of AI across sectors of society, many organizations, both public and private have come up with and published frameworks, principles, and guidelines on the ethical use of AI (Jobin et al., 2019; Hickok, 2020; Zicari et al., 2021). These include, e.g., the Asilomar AI Principles (Asilomar Conference, 2017), the guidelines of the European Group on Ethics in Science and New Technologies (EGE)3, the European Commission's High-Level Expert Group on Artificial Intelligence (ALTAI)4, and AI4People (Floridi et al., 2018), to mention but a few. Although the use of guidelines has increased and is finding favor with organizations, they are limited in the sense that most of them have been developed through discussions and input from industry and academic experts, and rarely include feedback from users, including citizens. Drobotowicz et al. (2021) conducted a qualitative study to investigate citizens' requirements for trustworthy AI services in the public sector. They interviewed 21 Finnish residents and conducted a design workshop on four public AI services which included cases in housing, health, education, and social service domains. Their study was part of a larger project that aimed to provide ethical guidelines for AI usage in the public sector in Finland. They found that transparency was a critical requirement for trustworthy AI services from the perspective of citizens. Participants wanted to know the purpose of a service—why it existed and what impacts it would have on them and others—and this was especially important when the benefits of a service were not clear. Participants also expressed an interest in knowing more about the data including data sources and collection and if consent was provided for data use. Finally, privacy was another topic participants were keen to know about and they wanted to know where the data would be stored and who would have access. Consistent with findings from Chazette et al. (2019), who found that their respondents wanted service-result explanations, the results from this study also show that for participants understanding what was taking place and why was more important than how (the process). Overall, the findings from this study are interesting and relevant as they show that AI transparency and AI explainability are tightly related. Interestingly, few participants in Drobotowicz et al.'s study brought up the topics of bias and fairness, even though these are some of the most common issues found in AI guidelines and principles (Jobin et al., 2019) suggesting these topics might not be known among non-specialists. The other issues that came up were the need to interact with a person to discuss a service, a certain level of control over their data, consent before data collection or sharing, and the ability to choose which data could be used and withdrawn at any point. Jobin et al. reviewed 84 ethical AI guidelines proposed by industrial and scientific institutions, 10 of which targeted the public sector. They found five principles that were present in over half the guidelines: (1) transparency, which aims to increase system explainability, interpretability, or disclosure; (2) justice and fairness, which are connected to mitigating bias and discrimination and enabling challenge or redress; (3) non-maleficence, which focuses on system security and safety; (4) responsibility, which is often presented alongside accountability and refers to legal liability and integrity; and (5) privacy, which mostly relates to data protection and data use and is presented both as a value and a user right. In relation to the public sector, the Alan Turing Institute (Leslie, 2019) have articulated a set of guidelines in three parts: (1) support, underwrite, and motivate values for a responsible data ecosystem; (2) fairness, accountability, sustainability, and transparency principles for designing and using services; and (3) a process-based governance framework to operationalize these guidelines. The Harvard ASH center (Mehr, 2017) also has a set of guidelines that explores the use of IA for citizen services, and they suggest six strategies for the government: (1) make AI part of a citizen-centric program, (2) solicit citizen input, (3) build on existing resources, (4) be data-prepared and tread carefully with privacy, (5) mitigate ethical risks and avoid AI decision making, and (6) focus on augmenting employees, not replacing them. AI as Part of a Citizen-Centric Program: Case AuroraAI Supporting Citizens in Different Life Events Finland has been amongst the forerunners in developing AI. Finland's national artificial intelligence program AuroraAI (AuroraAI)5 aims for a people-oriented society in which public and private organizations cooperate to help ensure people can deal with life events easily and conveniently at all stages of their lives. AuroraAI points citizens to potential public services. The AuroraAI program aims to be a service network that interconnects services so that they can support and interact with each other, and to implement AI innovations based on the key life events of different human transitions (family situations, progression to education), using flexibly interacting multi-stakeholder ecosystems (SAIP, 2019) and building new service chains that automatically support life event transitions. Service ecosystems use AI solutions to develop entirely new types of services that are tailored to people's personal life situations and to what businesses can offer. This will give citizens better access to personalized services based both on the personal data they provide (MyData; collected for example through personal smart health devices) and on population-level data. Individuals will be able to produce data themselves and access it in usable digital format from the data controller. As an ambitious program, AuroraAI aims to build a national digital infrastructure where society's current service structures are transformed into unified service entities using AI. It is hoped that this will improve the capacity of organizations to strengthen people's wellbeing by providing services for people's various life events intelligently, in collaboration between different service sectors and providers, and by using emerging technologies in a people-oriented way. In addition, this can reduce service costs and create opportunities to integrate public and private services. The Finnish government's policy summarizes the objectives as follows. “Success in achieving the goal of public services requires the interconnection of public organizations (AI-Aurora network) to interact with services from other sectors through AI. The AuroraAI program aims to create a network of services that interconnect services so that they can support and interact with each other” (SAIP, 2019). The program simultaneously intertwines governance and policy change, regulatory issues, technological innovation in a multi-vendor environment, the pooling of private and public sector interests, data-based modeling of individual and population situations, new procedures and actors' roles in the production and management of public services, as well as increasing the overall wellbeing of the individual. One of the key principles is user-given, unvalidated and anonymous data, through which the sharing of a person's own information with the AuroraAI network would take place. The model is expected to lead changes in the authorities' operational model. To support these changes, it is intended that the model will operate as ethically as possible. The ultimate grand idea is to promote a digital Finland, where everyone can use advanced services on their own terms and under their own autonomy with the opportunity to participate in the development of services. To make this goal possible, the program follows and implements the ideas and methods of open co-innovation. People's lives are composed of all kinds of events, such as starting daycare and school, building a family, working life, taking care of family members, and retiring. The idea of the AuroraAI program is that with the help of AuroraAI, people's ability and desire to take care of their own wellbeing will improve, as people and services meet better with the help of AI. In AuroraAI, people are divided into different clusters based on their multidimensional wellbeing (Figure 1). This life-event view is needed to contextualize the need for AI-based services and thus help the design, and eventually to provide seamless service paths for citizens. With the help of clusters, AuroraAI enables the targeting of suitable and timely service packages for a person's individual life events and situations. A life situation can be considered as a state of a state machine, which consists of a nearly infinite number of life situations and transitions between them (life events). In this context, a “state” covers all the information Aurora services have stored about the user in a distributed, anonymous manner. A change of this data yields a state transition. AuroraAI aims to facilitate these transitions by orchestrating optimal micro service combinations from an available pool to meet users' personal needs. People with similar data attributes are considered to reside, partially in similar life situations and therefore to benefit from similar service combinations. FIGURE 1 Figure 1. The AuroraAI program will support citizens throughout their life events (Ministry of Finance). The AuroraAI program has several focal points and dimensions: • On the technology and data side, the Aurora Platform: ◦ connects different services together intelligently in an AI powered network that removes the public/private fence; this allows for smarter, user-centric service findability, and provision; ◦ makes possible the development of DigiMe, a data-based digital profile of a real person (services that are based on personal data provide users with better understanding and control over their welfare and the ability to activate services in real time). DigiMe is intended to be used in situations where the connection between a real-world person and their digital persona needs to be made invisible. The user aggregates their personal data and produces a summary that can be processed by the network without being linked to the user's source data. The development and controlled testing of such a concept is important for the privacy of the users and their trust in the system; ◦ connects a user's intentions and needs (identified from personal info and attributes) with semantically harmonized and machine-readable service descriptions to generate personalized service recommendations; and ◦ forms the framework for the AuroraAI account, with profile management and service sessions transfers between different providers. • On the side of user functionalities, the intended impacts and vision for future society include: ◦ personalized wellbeing estimates and service recommendations (using the DigiMe); ◦ data-based “shared wellbeing snapshots” and service needs predictions for various population clusters and down to individuals; ◦ sharing of personal life situation info and attributes to the Aurora Platform in order to get service recommendations; ◦ AuroraAI working as your personalized “map and compass” that helps you reach your own specific goals or “states in life”; and ◦ service ecosystems for various population clusters and social issues, based on masses of individual data gathered through interviews, questionnaires and web surveys, with a human-centric, AI-powered society with predictive capabilities as the ultimate goal. The program ranges from smart service findability and provision that would help a person in a specific situation or service need, to the DigiMe, with its dimensions of individual wellbeing and empowerment, and finally to a new kind of AI-driven society where all service production is a reflection of data-analyzed states of wellbeing and service needs of the population and individuals. From an individual and societal point of view, human-centricity in AuroraAI means that AuroraAI enables services to be more accessible, effective and better-targeted at people's real needs. It aims to put an end to the way people are passed between agencies in order to enable people to manage their lives more easily. From a technical point of view, human-centricity in AuroraAI means taking account the person as a whole upon the provision of the services. This is opposite to a business-centric view, where a person is seen as a customer, and where, for example, the tax administration, a national church, employment service, or a golf club each has a different view of the user, only providing services in their own context and from their own organizational silos. This specialization works in terms of efficiency, but for the end-user it offers only a suboptimal solution to any complex life situation: there is rarely any single service or entity that can help the user with a wide range of different problems. In AuroraAI, it is assumed that providing a technical solution to connect current, siloed services together, combining the viewpoints of several organizations and providing in situ service combinations from several cross-sectoral organizations, truly results in more holistic help. The use of networked micro services makes it possible to better adapt to individual user needs, and AuroraAI network learns the best possible combinations from historical data. Since loosely coupled services can be developed independently and can be re-usable in a multitude of situations, this leads to advantages that have traditionally only been seen in SOA software architectures—not to mention cost savings. It is worth mentioning that the AuroraAI service referred to here are a much broader concept than in the software context: a local swimming pool can exist as an AuroraAI service, connected to the AuroraAI network in the same way as data sources or software. At first, this seems very counter-intuitive. However, a common API between all AuroraAI-enabled services makes this possible and leads to true digitalization: a combination of digital and non-digital services interacting with each other. Ethics Board as a Tool for Ethical Deliberation In autumn 2020, the AuroraAI program set up an Ethics Board with the aim of helping AuroraAI's governance to move in a human-centric, ethical and responsible direction and to verify the use of AI for human wellbeing. This was 2 years after the official start of the program whose concept had already been in development since the summer of 2017 at the Ministry of Finance and an emerging public-private multi-organization network of interested parties and companies. The Board decided to follow a forward-looking, proactive ethical deliberation process, which includes participatory ethical design and aims not only at identifying problems but also at finding ethically sustainable solutions for implementation (Sengers et al., 2005; Stahl et al., 2010). The key elements of the process are: • Anticipation: Proactive ethical thinking in the development of design solutions; looking carefully at both the objectives and the potential unintended consequences of deploying an application or a service • Involvement: Involvement of technology users or user representatives and developers in identifying and discussing ethical challenges s in a specified context, and • Expertise: Involvement of experts of ethics, technology, social and behavioral sciences, and law in the discussions. The size of the Board was decided to be limited to be large enough to accommodate different perspectives and fields of expertise, but small enough to function and communicate as a group rather than a network. Because public engagement as such was perceived as challenging in the development work, it was decided that the Ethics Board would invite organizations that represent the public broadly. That is why the Board is represented not only by experts and researchers, but also by representatives of various non-governmental organizations. Altogether 15 organizations participate in the Board, with representations from two universities, NGOs—Nongovernmental Organizations, ministries, the Association of Municipalities, the Association of Technology Companies, the Technical Research Centre of Finland Ltd. (VTT), the Lutheran Church of Finland, and the Finnish Digital Agency (DVV). The Board's coordinator and secretary is from the DVV. Individual Board members have a range of expertise, extending from knowledge of technology, data and legal issues to minority stakeholders, social sciences, and cultural studies academics. As all short, medium, and long-term objectives were included in the program, there was a risk that the AuroraAI concept as a whole would be difficult to comprehend and discuss. The program itself is vastly ambitious and includes a range of goals and targets. Thus, it necessarily lays itself open to equally many points of analysis, which are sometimes critical and even biting. One of the main strategic questions was whether the focus should be on macroeconomic (a better national economy through improved welfare and individually empowered citizens) or highly personalized (AI helping users to achieve their own life goals). Therefore, in order to address the ethical issues effectively enough, the Ethics Board decided to take a systematic approach to the debate. The focus of the work was set on the non-technical dimensions of the AuroraAI program. This included: • Background assumptions, value base, and social vision, • Human and social impact, • Power structures, • The content of governance transformation, • The meaning and interpretation of human-centricity, • Interpretations of foresight and a foresighted society, and • General application of AI to socio-economic and political issues. Based on this, the Board held six themed meetings in 2020–2021, focusing on jointly selected, specific issues. These were: 1. The AuroraAI concept, aims and underlying philosophy of the program, 2. The “How Am I Doing” functionality and the DigiMe, 3. Service Recommendation Engine and the technical Aurora Platform, 4. Service Ecosystems, 5. Legal issues concerning (a) personal data and privacy and (b) competition and EU single market issues: commercial service providers in the Aurora Platform and the removal of the public/private service provision barrier, and 6. Equality and non-discrimination. On the basis of this work, the Board produced a set of concrete, pragmatic recommendations for practical measures to be taken in the program. These measures should enforce the ethical foundation of the program and help to tackle some issues the Board has judged as particularly problematic. Key Ethical Elements The Board received a deep insight into the program's various dimensions, aims and the AuroraAI worldview. It discussed these themes, deliberated on what was learned, and produced two reports for the program leadership containing relevant ethical issues, opinions and suggestions for improvement. Quite a few things were deemed worthy of closer deliberation, analysis, and actions. Some of them appear frequently in AI ethics debates and discussions worldwide, while some are unique to the AuroraAI program. The main issues are presented in the following. The Greenlighted Elements and the Dilemma of Doublethink Inside the Program The Board positively noted the following: • The goal of better and smarter service findability is obviously valuable in an environment of severe information overload. The use of AI to help a citizen find the relevant services or information and service providers in their situation is a worthy target. This includes finding responsibly and ethically sound ways of having personal information enrich the inquiries so that citizens can be given as accurate and relevant selection of services as possible without compromising their privacy and autonomy. • The goal of creating service ecosystems for specific life events, combining several service providers from different sectors in a network and facilitating the automation of complex service processes, was met with warm approval. Almost everyone typically encounters at least one life event, many come with processes that are complex, stressful and sometimes painful. Turning these processes into smooth, digitally driven, AI-boosted events that ease the burden of both citizens and organizations could be a strong positive element for society as whole. • The attempt through the lens of data to better understand the state of wellbeing and needs of specific populations in order to design and produce services that come with genuine positive impact, was commended. The data policy and privacy issues that come with this were seen, however, as a matter to be looked into very closely. As the program includes all short, medium and long-term objectives, it has been difficult to understand the overall picture. One of the key strategic questions has been whether the AuroraAI program's emphasis is macroeconomic (a better national economy through improved wellbeing and individually empowered citizens) or ultra-individualistic (AI helps you achieve your own life goals, whatever they may be). At the time of writing this article, this was not yet very clear. Self-Empowerment Through DigiMe and Data-Managed Life The DigiMe, or a “holistic 360° profile,” is essentially a digital profile or mirror, composed of certain data, attributes and their values of an individual and their situation in life. The DigiMe centers around a group of eight parameters based on the “The Stiglitz Model” eight-point list of wellbeing factors introduced by Stiglitz et al. (2009). These are: (i) Material living standards (income, consumption, and wealth); (ii) Health; (iii) Education; (iv) Personal activities including work; (v) Political voice and governance; (vi) Social connections and relationships; (vii) Environment (present and future conditions); and (viii) Insecurity, of an economic as well as a physical nature. Stiglitz et al. (2009, p. 7) argue that these dimensions should be measured to gain insights into the socio-economic realities people live in. They frame this in the context of policies: “In effect, statistical indicators are important for designing and assessing policies aiming at advancing the progress of society.” In the AuroraAI program, however, the eight dimensions are used not only to measure the wellbeing of populations and clusters but also those of individuals. A person's 360° profile in the DigiMe program would then consist of various data sources reflecting the “Stiglitz Model”: health, education, work, income, social connections. It may be argued that this kind of use probably was not the intention of Stiglitz et al. who specifically mentioned these factors as elements to be noted when formulating policies that aim to enhance people's quality of life. Empowerment has also been noted in the Board as a slightly problematic concept. In the AuroraAI case the concept seems to refer to a particular chain of events or actions that runs approximately as follows: I share my personal attributes with the Aurora Platform, using the DigiMe program that employs the “Stiglitz dimensions” in the background. I receive AI-generated wellbeing estimates and service recommendations in return. I use the recommended services to improve my wellbeing (or to attain a life goal, which may be the same thing). I become better empowered as I learn to control my life through data as I reflect on the contents of the DigiMe service. The AuroraAI catchphrases “Let Your Digital Twin Empower You” and “become the data manager of your life” have been used in the program's rhetoric. It should be noted that service recommendations may not be based solely on the user's attributes; they would add the aggregate data of the user's reference cluster(s) the AI has identified to the mix. In this sense, the recommendation logic is quite similar to that of streaming media platforms: it combines your history and features with that of people it assumes are like you in certain critical respects. It remains unclear how the cluster data will be collected, what the data update frequency for that data would be and how a cluster's profile (aggregated attributes) would function for people whose profile places them, data-wise, at the outer edges of the cluster and makes them effectively anomalous in comparison to the people at the cluster's center. Effects on Real World Service Provision One specific point the Board has noted are the “trigger criteria” of recommendations. Young people and their services are a priority target group, especially in the early stages of AuroraAI. In many cases, low threshold mental health and social support services are relevant and valuable for the young. However, these very services in Finland are often quite congested and under-resourced, with sometimes lengthy queues and waiting periods that can stretch to weeks and even months. If the trigger criteria are set too low, the recommendation engine might recommend these services to a young person going through normal “teenage pains” and who has not considered seeking professional help before. If a state-owned AI directs people like this to these services, an already difficult service situation may become notably worse. Use and Sharing of Personal Attributes in the Aurora Platform The Board has noted that the service recommendation mechanism hinges on people's willingness and ability to either answer questionnaires on their situation, or wellbeing, to describe their situation or personal attributes in natural language, or to share their data from registers (this requires informed consent by the user). All these actions require different capabilities, ranging from linguistic ability to understanding what the sharing of personal information to a multi-actor network (Aurora Platform) means. With the DigiMe concept, it is somewhat unclear whether it will be used as an application, a user experience (UX) element, or an invisible background element that stores and uses the person's attributes (profile management). Either way, the user should be aware of it and its contents, logic and role in the process which includes personalized wellbeing estimates and service recommendations. The required ability to understand the overall algorithmic concept and working logic and critically consider the service recommendations may turn this into a service likely to benefit the more capable while leaving the digitally disenfranchised and some minorities by the wayside. Things become more complicated if we assume that the artificial intelligence in AuroraAI is largely another black box whose generated recommendations and estimations, let alone predictions, are not transparent and remain difficult to fully explain. The questions of transparency and control also become immediate in the context of the shared personal attributes. What tools will the user have to be able to track the use of their data? Will the users have access to the full list of service providers connected to the Aurora Platform? Can a user block a particular service provider from receiving their data? Can a user see which organizations and companies currently have that user's data on the platform? In short, the Board is asking how ordinary people can monitor the use of their data in the Aurora Platform and how they can understand why certain services are recommended to them. The AuroraAI Account, Profile Management, and Anonymity In the early AuroraAI scenarios, the users were totally anonymous actors who might only be identified using IP addresses, especially if they returned for service recommendations repeatedly from the same IP. Strong and protected anonymity was taken as a value that creates trust and avoids privacy and security risks. Further on, however, the notion of user profiles and profile management came up and, even later, the concept of an AuroraAI account. The Board believes that with these steps, the issues of privacy and anonymity must be brought up and analyzed from scratch as the notion of automatically guaranteed anonymity, if there ever is such a thing, has become critically compromised. The reasons for setting up profile management and the account arise from the scenario of data-based improvement of wellbeing: if there is no way to follow up users' states of wellbeing after they have received service recommendations based on their attributes given at a certain moment, the AI cannot utilize machine learning to generate better and more accurate recommendations and, eventually, predictions. It needs data points from individuals, albeit anonymous users over time, preferably from a mass of users since data volume is of the essence. Without this capability, the system would basically be just a smart service search, or assistant, that serves one person at a time in their unique situations and then totally forgets about them. This would give no input for machine learning and AuroraAI would not learn how the recommendations have affected the wellbeing of the user. Ergo, it needs to store user attributes, events and feedback in an account with profile management, which then would most likely be at the heart of the DigiMe solution/service. Ensuring the anonymity of users should be an essential part of the ethical use of AuroraAI. In the AuroraAI program, time periods covering even decades of a person's profile data have been mentioned in the vision of AuroraAI evolving into a personal “life guide” that learns from a person's attributes and events history over time and can thus generate a very accurate and timely guidance, service provision and predictive/proactive recommendations. It is therefore essential that particular attention is paid to this aspect from the anonymity and privacy perspectives. Civic Engagement, Inclusion, and Informing the Public The interests, values, and perspectives of citizens is essential (Levi and Stoker, 2000; Owen et al., 2013) in governmental actions related to AI. Governments should foster and facilitate societal discourse on the desirability of AI, and include active participation of various stakeholders and citizens. In reciprocal governance, AI experts should take the time to listen to and learn from users, especially their informal and emotional views on how the new service solution differs from existing (non-AI) arrangements, and what is expected of it. User involvement at every stage of the design process is therefore recognized as essential in public sector projects in Finland. However, in AuroraAI, this involvement has not been implemented. With the exception of NGO representatives on the Ethics Board, there is no information on the abilities different population groups may have in terms of operating within this kind of process. AuroraAI's current focus is on two issues: (1) carrying out the first phase of tech development of the Aurora Platform and the service recommendation engine, (2) developing operative models for leading the human-centric, data-based service production of the future, and restructuring all public sector service production accordingly. The more ambitious technical parts of the program are for the more distant future. This applies especially to the DigiMe concept, and the automatic, personal service recommendations and the human-machine interaction's effects on service production and society in general. It is argued by the program that, as this long-range vision may easily take a couple of decades to realize, assuming the world does not see a massive paradigm shift in the meanwhile, it makes no sense to bring these high concepts to citizens for discussion and evaluation now. This is a very pragmatic argument and can be defended from that perspective. However, as the current developments, both technical and non-technical, are actually rungs on a ladder that rises toward the eventual vision, it could well be counter-argued that presenting that vision to the public is necessary to justify and measure civic acceptance for the currently ongoing work, as the work is taking place precisely because of the overall vision and not separate from it. As the overall vision is deeply transformative and represents a new socio-technological paradigm, it needs to be accepted as legitimate by civil society before it is widely realized. Discussion The AuroraAI program is striving toward a human-centric, AI-powered society with predictive capabilities and a re-created public sector as the ultimate goal. Thus, the value base of the AuroraAI program is centered on the idea of a human-centric AI society: a world in which the public sector and other service providers are aware of the actual real-time needs, challenges and wellbeing of citizens. This society would • utilize AI and other advanced technologies to empower people individually (via the DigiMe program) to achieve their life goals, • offer relevant services and entire ecosystems in real time or proactively to both population clusters and individuals to support wellbeing, collective and individual, and • steer and plan service production, combining all sectors, to respond to people's actual needs, based on all available data on people and population segments. As such, AuroraAI can be seen as a socio-technological utopian vision where artificial intelligence liberates people, enhances wellbeing, boosts the impact of services and drives their cost-effectiveness. This vision, as enticing as it is, comes with a lot of questions and ethical, also legal, puzzles. However, none of this happens suddenly and the current and first baby steps stages of the program may produce lower-level outcomes that may themselves prove useful: • A smarter, semantically boosted service search engine, enriched by a dash of information on the user, can beat Google in search accuracy and practicality. Even if the service recommendation engine is never implemented at full scale, the work, and research put into it will produce progressive results. • Because people must be able to communicate their information to the Aurora Platform, a UI is needed and chatbots have been picked as the relevant component. The program will produce a generic AuroraAI chatbot component that organizations can customize and use in their own services. This is a big plus as it reduces the need for every organization to have their own unique chatbot, and it also assures the bots are ready to communicate with each other. • The concept of the life event ecosystem has been floating around not only in Finland but in a few other countries as well. If the AuroraAI program can come up with a practical solution to technically, legally, and administratively bring together service providers to build largely automatized service networks for life events, it would be a big step forward in making citizens' lives easier. • The principle of planning and leading services and ecosystems development and production by human-centric data is in itself both logical and commendable. If realized in a manner that preserves privacy and individual autonomy and does not discriminate, it could result in a more streamlined and effective service production. This, however, is not a given and there are some serious issues to clarify and ascertain. These goals would be high enough for any AI project to tackle within one program. However, as AuroraAI moves beyond these challenges and into the realm of individual self-empowerment through data and predictive/proactive personal service recommendations, things get more complicated. Dilemmas arise around not only profiling, data policies, minorities, and equality but also human autonomy, self-rule, the imbalance of power, accountability and transparency, and explicability. Although various guidelines, codes, or declarations guide the ethical dimension of the implementation and the system to be produced, current AI ethical principles are each an actor's or network's own reaction to this set-up where many unfinished and unresolved issues create opportunities for both completely unintentional negative consequences and deliberate misconduct. Trust, trustworthiness, and desirability in relation to AuroraAI play a significant role here. Of particular interest are also the questions concerning information management for one's own wellbeing. Even if all these aspects are carefully considered, we cannot rely on AI to explain the dynamics of human life. Instead, we need to know and understand these issues in order to be able to build the AI system. AI will not free us from understanding the complexities of human life. On the contrary, we should understand them well in order to explain and teach them to the AI machine. This is a tall order given the lack of AI literacy among the public. To move toward this goal, AI literacy needs to be built among the public, the users of the services, as well as the developers and those who deploy it. Long and Magerko (2020), define AI literacy as “a set of competencies that enables individuals to critically evaluate AI technologies; communicate and collaborate effectively with AI; and use AI as a tool online, at home, and in the workplace (p. 2)” and outline a range of competencies and design considerations as the basis for building that skillset. Ng et al. (2021) have reviewed the current literature and outlined four aspects of AI literacy: knowing and understanding AI (i.e., know the basic functions of AI and how to use AI applications in everyday life ethically); applying AI (i.e., applying AI knowledge, concepts, and applications in different scenarios); evaluating and creating AI including higher-order thinking skills (e.g., evaluate, appraise, predict, design with AI applications); and AI ethics (i.e., human-centered considerations such as fairness, accountability, transparency, and ethics). Thus, one can perfectly reasonably argue for a society that has robust, data-based insights into populations, regions, and phenomena and develops certain structures and functions around those insights. However, when the powers that use technologies to gain insights into individuals and their lives for the purpose of understanding or knowing them better, we need to ask: what kind of power do they then wield? This question must be analyzed, discussed and debated, hard, before we can decide if the rewards that may await at the end of the road are indeed worth taking the risk. Research literature catalogs both positive and negative outcomes in public services from AI use (Eubanks, 2018). Kuziemski and Misuraca (2020) suggest that the first wave of AI innovation will focus on reducing costs (including speeding up and improving digital accuracy). This also seems to have been the ultimate goal of the AuroraAI development. Although there is an emphasis on human-centeredness and with it citizen participation and development in the rhetoric, this aspect has not been addressed much in practice. One way to involve citizens in the debate on AuroraAI could be to use the so-called “citizen technology.” It could perhaps provide new practical tools for the governance debate on AI, which could help to build common understandings in civil society. Such an information and knowledge infrastructure and “collective-centricity” for the development of networked democracy could accelerate discussions and debates to ensure that they are more meaningful and lead to collective decisions that form the basis for action strategies among multi-level groups of people (Raikov, 2018). There is already evidence that AI-labeled technologies, in combination with other information and communication technologies (ICTs), can promote deliberative and participatory decision-making (Savaget et al., 2019; Arana-Catania et al., 2021). AI tools can potentially improve democratic processes and increase democratic responsiveness and accountability if they are aligned with social and political changes and values that support change (König and Wenzelburger, 2020). Finally, timing is critical in ethics deliberation. The AuroraAI Ethics Board was set up quite late, considering the program's 3 year history at that point. More specifically, as the concept, vision, and planning for technical solutions have been mostly set already, it is questionable whether the Board will be able to have an impact on the program, especially its aims and goals and values, and there are openly expressed suspicions of ethics washing. All in all, the work of the Ethics Board has proven not only to provide ethical solutions, but has also served as a learning tool providing ethical thinking and discussion in a multidisciplinary and multidisciplinary group which has enabled shared learning that would otherwise not be possible. Author Contributions Order of authorship reflects the relative extent of contribution. All authors contributed to manuscript revision, read, and approved the submitted version. Funding JL and NW wish to acknowledge the project Ethical AI for the Governance of the Society (ETAIROS), funded by the Strategic Research Council at the Academy of Finland. AJ's work was supported by a Fulbright-Nokia Distinguished Chair award and a U.S. NSF Awards#1937950, 1939105; USDA/NIFA Award#2021-67021-35329. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the funding agencies. Conflict of Interest The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest. Publisher's Note All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher. Footnotes 1. ^RSA's Forum for Ethical AI Report, “Artificial Intelligence: Real Public Engagement”: https://www.thersa.org/reports/artificial-intelligence-real-public-engagement. 2. ^Involve, “What Does Meaningful Public Engagement Look Like on AI and Ethics?” https://www.involve.org.uk/our-work/our-projects/practice/what-does-meaningful-public-engagement-look-ai-and-ethics. 3. ^EGE European Group on Ethics in Science and New Technologies, Statement on Ar-tificial Intelligence, Robotics, and 'Autonomous' Systems. Available onlne at: https://ec.europa.eu/research/ege/pdf/ege_ai_statement_2018.pdf. 4. ^The assessment list on trustworthy artificial intelligence. Available online at: https://futurium.ec.europa.eu/en/european-ai-alliance/pages/altai-assessment-list-trustworthy-artificial-intelligence. 5. ^https://vm.fi/en/auroraai-en; https://vm.fi/en/national-artificial-intelligence-programme-auroraai References Al-Mushayt, O. S. (2019). Automating E-government services with artificial intelligence. IEEE Access 7, 146821–146829. doi: 10.1109/ACCESS.2019.2946204 PubMed Abstract | CrossRef Full Text | Google Scholar Aoki, N. (2020). An experimental study of public trust in AI chatbots in the public sector. Govern. Inform. Quart. 37:101490. doi: 10.1016/j.giq.2020.101490 CrossRef Full Text | Google Scholar Arana-Catania, M., Lier, F. A. V., Procter, R., Tkachenko, N., He, Y., Zubiaga, A., et al. (2021). Citizen participation and machine learning for a better democracy. Digit. Govern. 2, 1–22. doi: 10.1145/3452118 CrossRef Full Text | Google Scholar Asilomar Conference (2017). Asilomar AI Principles. Available online at: https://futureoflife.org/ai-principles/?cn-reloaded=1 Google Scholar Beal, J., Dler, A., and Yaman, F. (2016). Managing bioengineering complexity with AI techniques. BioSystems 148, 40–46. doi: 10.1016/j.biosystems.2015.08.006 PubMed Abstract | CrossRef Full Text | Google Scholar Ben Rjab, A., and Mellouli, S. (2018). “Smart cities in the era of artificial intelligence and internet of things: literature review from 1990 to 2017,” in Paper Presented at the 19th Annual International Conference on Digital Government Research: Governance in the Data Age (Delft). doi: 10.1145/3209281.3209380 CrossRef Full Text | Google Scholar Bryson, J., and Winfield, A. (2017). Standardizing ethical design for artificial intelligence and autonomous systems. Computer 50, 116–119 doi: 10.1109/MC.2017.154 PubMed Abstract | CrossRef Full Text | Google Scholar Chazette, L., Karras, O., and Schneider, K. (2019). “Do end-users want explanations? Analyzing the role of explainability as an emerging aspect of non-functional requirements,” in Proceedings of the IEEE International Conference on Requirements Engineering (Jeju Island), 223–233. doi: 10.1109/R.E.2019.00032 PubMed Abstract | CrossRef Full Text | Google Scholar Desouza, K. C., Dawson, G. S., and Chenok, D. (2020). Designing, developing, and deploying artificial intelligence systems: lessons from and for the public sector. Bus. Horiz. 63, 205–213. doi: 10.1016/j.bushor.2019.11.004 CrossRef Full Text | Google Scholar Dignum, V. (2018). Ethics in artificial intelligence: introduction to the special issue. Ethics and Inform. Technol. 20, 1–3. doi: 10.1007/s10676-018-9450-z CrossRef Full Text | Google Scholar Drobotowicz, K., Kauppinen, M., and Kujala, S. (2021). “Trustworthy AI services in the public sector: what are citizens saying about it?,” in Requirements Engineering: Foundation for Software Quality - 27th International Working Conference, REFSQ 2021, Lecture Notes in Computer Science; Vol. 12685, eds F. Dalpiaz and P. Spoletini, 99–115. doi: 10.1007/978-3-030-73128-1_7 CrossRef Full Text | Google Scholar Eubanks, V. (2018). Automating Inequality: How High-Tech Tools Profile, Police and Punish the Poor. New York, NY: St Martin's Press. Google Scholar Fatima, S., Desouza, K. C., and Dawson, G. G. (2020). National strategic artificial intelligence plans: a multi-dimensional analysis. Econ. Anal. Policy 67, 178–194. doi: 10.1016/j.eap.2020.07.008 CrossRef Full Text | Google Scholar Floridi, L., Cowls, J., Beltrametti, M., Chatila, R., Chazerand, P., Dignum, V., et al. (2018). AI4People-an ethical framework for a good, AI Soc. Minds Mach. 28, 689–707. doi: 10.1007/s11023-018-9482-5 PubMed Abstract | CrossRef Full Text | Google Scholar Gupta, K. P. (2019). Artificial intelligence for governance in India: prioritizing the challenges using analytic hierarchy process (AHP). Int. J. Recent Technol. Eng. 8, 3756–3762. doi: 10.35940/ijrte.B3392.078219 CrossRef Full Text Hickok, M. (2020). Lessons learned from AI ethics principles for future actions. AI Ethics 1, 41–47. doi: 10.1007/s43681-020-00008-1 CrossRef Full Text | Google Scholar Holton, R., and Boyd, R. (2021). ‘Where are the people? What are they doing? Why are they doing?’(Mindell) Situating artificial intelligence within a socio-technical framework. J. Sociol. 57, 179–195. doi: 10.1177/1440783319873046 CrossRef Full Text | Google Scholar Jobin, A., Ienca, M., and Vayena, E. (2019). The global landscape of AI ethics guidelines. Nat. Mach. Intell. 1, 389–399. doi: 10.1038/s42256-019-0088-2 CrossRef Full Text | Google Scholar König, P. D., and Wenzelburger, G. (2020). Opportunity for renewal or disruptive force? How artificial intelligence alters democratic politics. Govern. Inform. Quart. 37:101489. doi: 10.1016/j.giq.2020.101489 CrossRef Full Text | Google Scholar Kuziemski, M., and Misuraca, G. (2020). AI governance in the public sector: three tales from the frontiers of automated decision-making in democratic settings. Telecomm. Policy 44:101976. doi: 10.1016/j.telpol.2020.101976 PubMed Abstract | CrossRef Full Text | Google Scholar Leslie, D. (2019). Understanding Artificial Intelligence Ethics and Safety: A Guide for the Responsible Design and Implementation of AI Systems in the Public Sector. Available online at: https://ssrn.com/abstract=3403301 Google Scholar Levi, M., and Stoker, L. (2000). Political trust and trustworthiness. Annu. Rev. Polit. Sci. 3, 492–3. Available online at: https://www.annualreviews.org/doi/pdf/10.1146/annurev.polisci.3.1.475 Long, D., and Magerko, B. (2020). “What is AI literacy? Competencies and design considerations,” in Proceedings of CHI”20. doi: 10.1145/3313831.3376727 CrossRef Full Text | Google Scholar Mehr, H. (2017). Artificial Intelligence for Citizen Services and Government. Harvard Ash Center Technology and Democracy. Google Scholar Mikhaylov, S. J., Esteve, M., and Campion, A. (2018). Artificial intelligence for the public sector: opportunities and challenges of cross-sector collaboration. Philos. Trans. R. Soc. A 376:20170357. doi: 10.1098/rsta.2017.0357 PubMed Abstract | CrossRef Full Text | Google Scholar Montoya, L., and Rivas, P. (2019). “Government AI readiness meta-analysis for Latin America and The Caribbean,” in Paper Presented at the 2019 IEEE International Symposium on Technology and Society (ISTAS) (Boston, MA). doi: 10.1109/ISTAS48451.2019.8937869 PubMed Abstract | CrossRef Full Text | Google Scholar Ng, D., Leung, J., Chu, K., and Qiao, M. (2021). AI literacy: definition, teaching, evaluation and ethical issues. Proc. Assoc. Inform. Sci. Technol. 58, 504–509. doi: 10.1002/pra2.487 PubMed Abstract | CrossRef Full Text | Google Scholar Ojo, A., Mellouli, S., and Ahmadi Zeleti, F. (2019). “A realist perspective on AI-era public management,” in Paper Presented at the 20th Annual International Conference on Digital Government Research (Dubai). doi: 10.1145/3325112.3325261 CrossRef Full Text | Google Scholar Owen, R., Stilgoe, J., Macnaghten, P., Fisher, E., Gorman, M., and Guston, D. (2013). “A Framework for Responsible Innovation,” in Responsible Innovation, eds R. Owen, J. Bessant, and M. Heintz (John Wiley), 27–50. Google Scholar Pietikäinen, M., and Silvén, O. (2021). Challenges of Artificial Intelligence: From Machine Learning and Computer Vision to Emotional Intelligence. University of Oulu. Available online at: http://jultika.oulu.fi/Record/isbn978-952-62-3199-0 Google Scholar Raikov, A. (2018). Accelerating technology for self-organising networked democracy. Futures 103, 17–26. doi: 10.1016/j.futures.2018.03.015 CrossRef Full Text | Google Scholar SAIP (2019). Leading the Way Into the Era of Artificial Intelligence: Final Report of Finland's Artificial Intelligence Programme 2019. Steering Group and Secretariat of the Artificial Intelligence Program. Publications of the Ministry of Economic Affairs and Employment 2019, Helsinki. Available online at: http://urn.fi/URN:ISBN:978-952-327-437-2 Savaget, P., Chiarini, T., and Evans, S. (2019). Empowering political participation through artificial intelligence. Sci. Publ. Policy 46, 369–380. doi: 10.1093/scipol/scy064 PubMed Abstract | CrossRef Full Text | Google Scholar Sengers, P., Boehner, K., David, S., and Kaye, J. (2005). “Reflective design,” in CC '05 Proceedings of the 4th Decennial Conference on Critical Computing: Between Sense and Sensibility (New York, NY), 49–58. doi: 10.1145/1094562.1094569 CrossRef Full Text | Google Scholar Stahl, B. C., Heersmink, R., Goujon, P., Flick, C., Hoven van den, J., Wakunuma, K., et al. (2010). Identifying the ethics of emerging information and communication technologies: an essay on issues, concepts and method. Int. J. Technoethics 4, 20–38. doi: 10.4018/jte.2010100102 CrossRef Full Text | Google Scholar Stiglitz, J., Sten, A., and Fitoussi, J.-P. (2009). Report by the Commission on the Measurement of Economic Performance and Social Progress. European Commission. Google Scholar Sun, T. Q., and Medaglia, R. (2019). Mapping the challenges of artificial intelligence in the public sector: evidence from public healthcare. Govern. Inform. Q. 36, 368–383. doi: 10.1016/j.giq.2018.09.008 CrossRef Full Text | Google Scholar Toll, D., Lindgren, I., Melin, U., and Madsen, C. Ø. (2019). “Artificial intelligence in Swedish policies: values, benefits, considerations and risks,” in Paper Presented at the International Conference on Electronic Government (San Benedetto del Tronto). doi: 10.1007/978-3-030-27325-5_23 CrossRef Full Text | Google Scholar Valle-Cruz, D., Alejandro Ruvalcaba-Gomez, E., Sandoval-Almazan, R., and Ignacio Criado, J. (2019). “A review of artificial intelligence in government and its potential from a public policy perspective,” in Paper Presented at the 20th Annual International Conference on Digital Government Research (Dubai). doi: 10.1145/3325112.3325242 CrossRef Full Text | Google Scholar Wirtz, B. W., Weyerer, J. C., and Geyer, C. (2018). Artificial intelligence and the public sector-Applications and challenges. Int. J. Publ. Administr. 42, 596–615. doi: 10.1080/01900692.2018.1498103 CrossRef Full Text | Google Scholar Zicari, R. V., Brodersen, J., Brusseau, J., Düdder, B., Eichhorn, T., Ivanov, T., et al. (2021). Z-Inspection®: a process to assess trustworthy AI. IEEE Trans. Technol. Soc. 2, 83–97. doi: 10.1109/TTS.2021.3066209 PubMed Abstract | CrossRef Full Text | Google Scholar Zuiderwijk, A., Chen, Y.-C., and Salem, F. (2021). Implications of the use of artificial intelligence in public governance: a systematic literature review and a research agenda. Govern. Inform. Quart. 38:101577. doi: 10.1016/j.giq.2021.101577 CrossRef Full Text | Google Scholar Keywords: artificial intelligence, ethics, governance, co-design, public administration (generally) Citation: Leikas J, Johri A, Latvanen M, Wessberg N and Hahto A (2022) Governing Ethical AI Transformation: A Case Study of AuroraAI. Front. Artif. Intell. 5:836557. doi: 10.3389/frai.2022.836557 Received: 15 December 2021; Accepted: 17 January 2022; Published: 10 February 2022. Edited by: Rebekah Ann Rousi, University of Vaasa, Finland Reviewed by: Kimberly Houser, University of North Texas, United States Alexander Nikolaevich Raikov, V. A. Trapeznikov Institute of Control Sciences (RAS), Russia Copyright © 2022 Leikas, Johri, Latvanen, Wessberg and Hahto. This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms. *Correspondence: Jaana Leikas, jaana.leikas@vtt.fi This article is part of the Research Topic Governance AI Ethics View all 9 Articles People also looked at Download

Titel: Key principles for ethical AI development

Key principles for ethical AI development🎪 Headed to IAPP's Global Privacy Summit? Find out where to find Transcend →ProductsSolutionsResourcesDocsLog InGet a demoProductsSolutionsResourcesDocsLog InGet a demo Key principles for ethical AI development Morgan SullivanOctober 20, 2023 • 10 min readAt a glanceAs we continue to harness the power of artificial intelligence (AI), it's crucial that AI creators and regulators consider, implement, and promote AI ethics at every step.Key principles for ethical AI include transparency, explainability, fairness, non-discrimination, privacy, and data protection.Keep reading to learn more about the ethical considerations for building responsible AI, including clear accountability frameworks, ethical data sourcing, and a shift left mentality within AI development.Table of contentsThe ethical landscape of AI technologyChallenges in AI ethicsKey principles for ethical AIEthical data sourcing and managementGlobal perspectives on AI ethicsPractical implementation of AI ethicsCase studies: AI ethics in practiceThe future of AI ethicsThe ethical landscape of AI technologyDefining AI ethicsAI ethics is a multidisciplinary field aimed at ensuring AI technologies respect human values, avoid undue harm, and act as a beneficial force in society. A broad topic, ethical AI encompasses privacy, fairness, accountability, transparency, and human rights—while seeking to limit outcomes like bias and discrimination.Artificial intelligence, machine learning, and large-language models (LLMs) have already had a significant impact on the world—and it's still early days! That's why, as these tools become further integrated into our businesses, technologies, and everyday lives, developing ethical guidelines that support responsible AI governance, development, and use will be key. Historical perspectiveThe ethical questions and considerations surrounding AI have evolved in tandem with the technology.In the early days, AI ethics was primarily concerned with the theoretical implications of machine intelligence. It was largely the realm of science fiction—exploring scenarios where AI surpassed human intelligence, posing existential threats or novel ethical dilemmas. But with the advent of machine learning and LLMs, the hypotheticals have edged that much closer to reality. As we integrate AI tools in high-consequence sectors like healthcare, finance, and national defense, the theoretical discourse has shifted towards more practical concerns, including bias, data privacy, and potential socioeconomic effects like job displacement.Challenges in AI ethicsUnderstanding the dilemmasArtificial intelligence presents unique moral and practical challenges. For instance, how do we ensure fairness when AI can inadvertently perpetuate or amplify existing biases? How do we maintain privacy when AI needs vast amounts of data to function?On the moral side, the autonomy of AI can blur the lines of responsibility when a system fails or makes a mistake, raising questions about accountability. And, if not managed appropriately, biased AI can have very real affects on the lives and livelihood of everyday people.On a practical level, the complexity and opacity of some AI algorithms, often referred to as 'black boxes', can make it difficult to understand or explain AI decisions, throttling transparency and trust.Balancing benefits and risks of artificial intelligenceStriking a balance between benefits and risk is key to a sustainable future with AI. While the technology offers immense benefits for the human race, from revolutionizing various sectors, catalyzing breakthroughs in healthcare, and supporting environmental conservation, these upsides must be actively considered against the ethical risks. We must navigate this complex landscape with an ethical compass, ensuring the technology serves society's best interests without compromising human dignity or causing undue harm.Understanding the dangers of ungoverned AIArtificial intelligence (AI) is one of the greatest technological advancements of the last decade. But the rapid development and expansive global adoption has left headlines, governments, and everyday people asking—is AI dangerous?The truth is that artificial intelligence can be a powerful tool, but without the appropriate AI governance structures and human oversight, it can present significant risks, including cyber breaches, job displacement, and biased decision-making. Read the full guide to learn moreKey principles for developing ethical AI systemsAI researchers have identified a handful of principles that can help guide the development of ethical AI. These principles are not yet legally enforceable, but they can still act as critical guideposts as AI creators navigate this new frontier. Transparency and explainability: AI models should be transparent, and their decisions explainable. People affected by an AI system should be able to understand why it made a particular decision.Fairness and non-discrimination: Artificial intelligence should treat all individuals fairly, avoiding biases that could lead to discriminatory outcomes. This includes both explicit and unconscious bias, which is often embedded in the data used to train an AI model.Privacy and data protection: AI tools must respect user privacy and personal data. This includes not only securing data from unauthorized access, but also respecting a user's right to control how their data is used.Ethical data sourcing and managementSourcing with integrityData is the backbone of any AI model—meaning ethical data sourcing is critical. Sourcing data ethically means obtaining data in a way that respects individuals' privacy, consent, and applicable data rights. While ethical data sourcing helps to maintain an AI system's integrity and public trust, it can also mitigate potential legal risks.Irresponsible practices like inadequate data security or violation of privacy rights can erode public trust, cause data breaches, damage the reputation of the organization, and lead to legal repercussions. Managing data lifecycleProper data management for AI tools involves secure storage, controlled access, and regulated deletion practices. Data should be properly secured, employing encryption methods and firewall systems to prevent unauthorized access or breaches. Access to data should be limited to necessary personnel, with a system for tracking who has accessed the data and for what purpose. Additionally, a clear data deletion policy should be implemented. Once data has outlived its utility or an individual requests that their data is deleted, it should be permanently removed to maintain privacy and respect individual rights. Global perspectives on the ethics of artificial intelligenceInternational standards and guidelinesMany countries and international organizations are recognizing the importance of establishing ethical guidelines for AI development—formulating their own policies and recommendations for ethical AI. For instance, the European Union (EU) has proposed a framework that emphasizes transparency, accountability, and protection of individual rights. Meanwhile, countries like Singapore and Canada have published their own AI ethics guidelines, emphasizing principles of fairness, accountability, and human-centric values.At the global level, the UNESCO has released draft recommendations on the Ethics of Artificial Intelligence—emphasizing the need for a human-centered approach to AI that focuses on human rights, cultural diversity, and fairness. It also stresses the importance of transparency, accountability, and the need for AI to be understandable and controllable by human beings.While the specifics may vary, the global consensus leans towards a human-centric approach that stresses transparency, accountability, and the protection of individual rights.Collaboration and consensusAs AI technologies continue to permeate international borders, fostering global collaboration and consensus on the ethics of artificial intelligence is crucial. It’s essential to have standardized, universally adopted ethical guidelines to ensure the responsible use of AI across all nations.These globally recognized standards can help bridge cultural and societal differences, while establishing a common ground for the ethical use and development of AI. Such an international approach not only promotes the responsible development and use of AI technologies, but also fosters trust, cooperation, and mutual understanding among nations.AI Regulation: Comparing the US, EU, and ChinaThough many headlines make AI seem like an unruly, unregulated Wild West, there are several laws in effect today that directly affect the use and creation of AI, and can help guide companies towards effective AI governance.The guide will explore key AI laws in the United States, the European Union, and China—looking at laws already on the books, as well as those coming down the pipeline. Read the full guide to learn morePractical implementation of AI ethicsFrom theory to practiceTranslating ethical principles into actionable guidelines is key to realizing ethical AI. This involves integrating ethical considerations into every stage of the AI lifecycle, from initial design to deployment, to monitoring.Implementing ethical principles begins at the conceptualization and design stage. AI developers should incorporate ethical considerations from the start, ensuring their AI code is designed to be fair, transparent, and respectful of user privacy. During the development phase, it’s essential to source and manage data ethically. This involves obtaining data sets responsibly, ensuring secure storage, and managing its lifecycle properly.Once the AI system is deployed, its performance and ethical behavior should be consistently monitored. Continuous auditing can help identify any ethical issues or biases that arise and address them promptly.Additionally, clear communication about how the AI works, its limitations, and the data it uses will help ensure transparency and maintain user trust. This can be accomplished through comprehensive, user-friendly documentation and, where appropriate, interfaces that allow users to review and understand the AI’s decisions.Lastly, it's crucial to have an accountability framework in place, so there are clear lines of responsibility if the AI system fails or causes harm. This is a helpful way to support both internal and legal accountability.By integrating these steps into the development process, ethical principles can be translated into practical, actionable guidelines.Case studies: AI ethics in practice Google’s AI PrinciplesGoogle's AI Principles, first published in 2018, serve as an ethical framework to guide the responsible development and use of AI across the company's products and services. These principles emphasize the social benefits of AI, noting potential transformative impacts in fields like health care, security, energy, transportation, manufacturing, and entertainment. Google's approach to implementing these principles involves a combination of education programs, AI ethics reviews, and technical tools. Furthermore, the company collaborates with NGOs, industry partners, academics, and ethicists throughout the product development process. Microsoft’s AI EthicsMicrosoft's approach to AI ethics is guided by six key principles: accountability, inclusiveness, reliability and safety, fairness, transparency, and privacy and security. These principles provide internal guidance on how to design, build, and test AI models responsibly. The company also proactively establishes guardrails to anticipate and mitigate AI risks, while maximizing benefits. Furthermore, Microsoft reviews its AI systems to identify those that may have an adverse impact on people, organizations, and society, and applies additional oversight to these systems.IBM’s Trustworthy AIIBM is recognized as a leader in the field of trustworthy AI, with a focus on ethical principles and practices in its use of technology. The company has developed a Responsible Use of Technology framework to guide its decision-making and governance processes, fostering a culture of responsibility and trust. Trustworthiness in AI, according to IBM, involves continuous monitoring and frequent validation of AI models to ensure they can be trusted by various stakeholders. IBM's approach to trustworthy AI also emphasizes trust in data, models, and processes. The World Economic Forum has highlighted IBM's efforts in a case study, providing practical resources for organizations to operationalize ethics in their use of technology.The Future of AI EthicsEmerging Ethical ConcernsAs AI technology continues to evolve and mature, new ethical challenges are likely to emerge. One such challenge is the advent of deepfakes, which pose threats to personal identity and can be powerful tools for disinformation campaigns. Additionally, the expansion of AI into more critical sectors like healthcare or autonomous vehicles raises concerns about safety and reliability.The rise of autonomous weapons and the ethical implications of AI in warfare are another important issue to consider, and there's growing debate on the morality and legality of using AI in lethal decisions. Furthermore, there are concerns about AI's impact on employment and labor markets. Balancing efficiency and potential productivity gains with the risk of job displacement and economic inequality will be a fine line to navigate in the years ahead.Lastly, as AI systems become more autonomous, questions surrounding the legal status and rights of advanced AI systems may begin to surface. This could challenge existing legal frameworks and provoke new ethical discussions.The Role of Education and AwarenessPromoting ethical AI requires active engagement in education, training, and public discourse. Education serves as the foundation, instilling an understanding of ethical AI principles among students, developers, and technology users. Offering specialized training courses for AI practitioners can reinforce this foundation, highlighting the importance of ethical considerations in the creation and implementation of AI models.More than that, creating platforms for public discourse on AI ethics can ensure a democratic approach to AI governance. By fostering open discussions, we allow diverse viewpoints to shape the ethical standards for AI, promoting a more inclusive, equitable, and fair technology landscape.ConclusionEthics is not an optional extra, but rather a fundamental requirement, for AI technologies. By considering ethical principles at every step, we can ensure AI technology benefits society without compromising individual rights and freedoms.The field of AI ethics is still evolving, and there is much work to be done. As we continue to innovate, we must also continue the dialogue about what it means to develop and use AI ethically.About TranscendTranscend is the governance layer for enterprise data—helping companies automate and future-proof their privacy compliance and implement robust AI governance across an entire tech stack.Transcend Pathfinder gives your company the technical guardrails to adopt new AI technologies with confidence. While Transcend Data Mapping goes beyond observability to power your privacy program with smart governance suggestions.Ensure nothing is tracked without user consent using Transcend Consent, automate data subject request workflows with Privacy Requests, and mitigate risk with smarter privacy Assessments.ResourcesAI Governance 101Transcend's approach to AI governanceUnderstanding the dangers of ungoverned AIHow to use the NIST AI Risk Management FrameworkAI Regulation: Comparing the US, EU, and China5 steps for choosing an AI governance softwareShare this articleDiscover more articlesSnippetsSign up for Transcend's weekly privacy newsletter.EmailSign upBy clicking "Sign Up" you agree to the processing of your personal data by Transcend as described in our Data Practices and Privacy Policy. You can unsubscribe at any time.Discover more articlesSign up for SnippetsBite sized privacy. Get our weekly newsletter on privacy and technology.EmailSign upBy clicking "Sign Up" you agree to the processing of your personal data by Transcend as described in our Data Practices and Privacy Policy. You can unsubscribe at any time.ProductsData InventoryData LineageSilo DiscoveryStructured DiscoveryUnstructured DiscoveryDSR AutomationConsent ManagementPreference StorePrivacy CenterAssessmentsContract ScanningWeb AuditorPathfinderIntegrationsSecuritySolutionsConsumerHealthcareFintechMediaB2BResourcesCase studiesCost calculatorBlogPrivacy newsletterTech talksAll resourcesCompanyAbout UsPressCareersContact UsLegalDevelopersOpen sourceTechnical docsPlatform statusLog InSupportSan Francisco, CaliforniaCopyright © 2024 Transcend, Inc.Our Privacy CenterThis site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply.

Titel: What Is AI Governance? | IBM

What Is AI Governance? | IBM What is AI governance? Explore IBM's AI governance solution Subscribe for AI updates Published: 28 November 2023 Contributors: Tim Mucci, Cole Stryker What is AI governance? Artificial intelligence (AI) governance refers to the guardrails that ensure AI tools and systems are and remain safe and ethical. It establishes the frameworks, rules and standards that direct AI research, development and application to ensure safety, fairness and respect for human rights. AI governance encompasses oversight mechanisms that address risks like bias, privacy infringement and misuse while fostering innovation and trust. An ethical AI-centered approach to AI governance requires the involvement of a wide range of stakeholders, including AI developers, users, policymakers and ethicists, ensuring that AI-related systems are developed and used to align with society's values. AI governance addresses the inherent flaws arising from the human element in AI creation and maintenance. Since AI is a product of highly engineered code and machine learning created by people, it is susceptible to human biases and errors. Governance provides a structured approach to mitigate these risks, ensuring that machine learning algorithms are monitored, evaluated and updated to prevent flawed or harmful decisions. AI solutions must be developed and used responsibly and ethically. That means addressing the risks associated with AI: bias, discrimination and harm to individuals. Governance addresses these risks through sound AI policy, regulation, data governance and well-trained and maintained data sets. Governance aims to establish the necessary oversight to align AI behaviors with ethical standards and societal expectations and to safeguard against potential adverse impacts. White paper Why AI governance is a business imperative for scaling enterprise AI Learn about barriers to AI adoptions, particularly lack of AI governance and risk management solutions. Related content Register for the IDC report Why is AI governance important? AI governance is essential for reaching a state of compliance, trust and efficiency in developing and applying AI technologies. With AI's increasing integration into organizational and governmental operations, its potential for negative impact has become more visible. High-profile missteps like the Tay chatbot incident (link resides outside ibm.com), where a Microsoft AI chatbot learned toxic behavior from public interactions on social media and the COMPAS (link resides outside ibm.com) software's biased sentencing decisions have highlighted the need for sound governance to prevent harm and maintain public trust. These instances show that AI can cause significant social and ethical harm without proper oversight, emphasizing the importance of governance in managing the risks associated with advanced AI. By providing guidelines and frameworks, AI governance aims to balance technological innovation with safety, ensuring AI systems do not violate human dignity or rights. Transparent decision-making and explainability are critical for ensuring AI systems are used responsibly. AI systems make decisions all the time, from deciding which ads to show to determining whether to approve a loan. It is essential to understand how AI systems make decisions to hold them accountable for their decisions and ensure that they make them fairly and ethically. Moreover, AI governance is not just about ensuring one-time compliance; it's also about sustaining ethical standards over time. AI models can drift, leading to output quality and reliability changes. Current trends in governance are moving beyond mere legal compliance towards ensuring AI's social responsibility, thereby safeguarding against financial, legal and reputational damage, while promoting the responsible growth of technology. Examples of AI governance Examples of AI governance encompass a range of policies, frameworks and practices that organizations and governments implement to ensure the responsible use of AI technologies. These examples demonstrate how AI governance happens in different contexts: The General Data Protection Regulation (GDPR): An example of AI governance, particularly in the context of personal data protection and privacy. While the GDPR is not exclusively focused on AI, many of its provisions are highly relevant to AI systems, especially those that process the personal data of individuals within the European Union. The Organisation for Economic Co-operation and Development (OECD): AI Principles, adopted by over 40 countries, emphasize responsible stewardship of trustworthy AI, including transparency, fairness and accountability in AI systems. Corporate AI Ethics Boards: Many companies have established ethics boards or committees to oversee AI initiatives, ensuring they align with ethical standards and societal values. For example, IBM has launched an AI Ethics Council to review new AI products and services and ensure they align with IBM's AI principles. These boards often include cross-functional teams from legal, technical and policy backgrounds. Who oversees responsible AI governance? In an enterprise-level organization, the CEO and senior leadership are ultimately responsible for ensuring their organization applies sound AI governance throughout the AI lifecycle. Legal and general counsel are critical in assessing and mitigating legal risks, ensuring AI applications comply with relevant laws and regulations. Audit teams are essential for validating the data integrity of AI systems and confirming that the systems operate as intended without introducing errors or biases. The CFO oversees the financial implications, managing the costs associated with AI initiatives and mitigating any financial risks. However, the responsibility for AI governance does not rest with a single individual or department; it is a collective responsibility where every leader must prioritize accountability and ensure that AI systems are used responsibly and ethically across the organization. The CEO and senior leadership are responsible for setting the overall tone and culture of the organization. When prioritizing accountable AI governance, it sends all employees a clear message that everyone must use AI responsibly and ethically. The CEO and senior leadership can also invest in employee AI governance training, actively develop internal policies and procedures and create a culture of open communication and collaboration. Principles of responsible AI governance AI governance is essential for managing rapid advancements in AI technology, particularly with the emergence of generative AI. Generative AI, which includes technologies capable of creating new content and solutions, such as text, images and code, has vast potential across many use cases. From enhancing creative processes in design and media to automating tasks in software development, generative AI is transforming how industries operate. However, with its broad applicability comes the need for robust AI governance. The principles of responsible AI governance are essential for organizations to safeguard themselves and their customers. The following principles can guide organizations in the ethical development and application of AI technologies, which include: Empathy: Organizations should understand the societal implications of AI, not just the technological and financial aspects. They need to anticipate and address the impact of AI on all stakeholders. Bias control: It is essential to rigorously examine training data to prevent embedding real-world biases into AI algorithms, ensuring fair and unbiased decisions. Transparency: There must be clarity and openness in how AI algorithms operate and make decisions, with organizations ready to explain the logic and reasoning behind AI-driven outcomes. Accountability: Organizations should proactively set and adhere to high standards to manage the significant changes AI can bring, maintaining responsibility for AI's impacts. In late 2023, The White House issued an executive order to ensure AI safety and security. This comprehensive strategy provides a framework for establishing new standards to manage the risks inherent in AI technology. The U.S. government's new AI safety and security standards exemplify how governments approach this highly sensitive issue. AI safety and security: Mandates developers of powerful AI systems share safety test results and critical information with the U.S. government. Requires the development of standards, tools and tests to ensure AI systems are safe and trustworthy​. Privacy protection: Prioritizes developing and using privacy-preserving techniques and strengthens privacy-preserving research and technologies. It also sets guidelines for federal agencies to evaluate the effectiveness of privacy-preserving techniques. Equity and civil rights: Prevents AI from exacerbating discrimination and biases in various sectors. Includes guiding landlords and federal programs, addresses algorithmic discrimination and ensures fairness in the criminal justice system​. Consumer, patient and student protection: Helps advance responsible AI in healthcare and education, such as developing life-saving drugs and supporting AI-enabled educational tools​. Worker support: Develops principles to mitigate AI's harmful effects on jobs and workplaces, including addressing job displacement and workplace equity​. Promoting innovation and competition: Catalyzes AI research across the U.S., encourages a fair and competitive AI ecosystem and facilitates the entry of skilled AI professionals into the U.S.​ Global leadership in AI: Expands international collaboration on AI and promotes the development and implementation of vital AI standards with international partners​. Government use of AI: Ensures responsible government deployment of AI by issuing guidance for agencies' use of AI, improving AI procurement and accelerating hiring of AI professionals. While regulations and market forces standardize many governance metrics, organizations must still determine how to best balance measures for their business. Measuring AI governance effectiveness can vary by organization; each organization must decide what focus areas they must prioritize. With focus areas such as data quality, model security, cost-value analysis, bias monitoring, individual accountability, continuous auditing and adaptability to adjust depending on the organization's domain, it is not a one-size-fits-all decision.: Catalyzes AI research across the U.S., encourages a fair and competitive AI ecosystem and facilitates the entry of skilled AI professionals into the U.S.​ Levels of AI governance AI governance doesn't have universally standardized "levels" in the way that, for example, cybersecurity might have defined levels of threat response. Instead, AI governance has structured approaches and frameworks developed by various entities that organizations can adopt or adapt to their specific needs. Organizations can use several frameworks and guidelines to develop their governance practices. Some of the most widely used frameworks include the NIST AI Risk Management Framework, the OECD Principles on Artificial Intelligence, and the European Commission's Ethics Guidelines for Trustworthy AI. These frameworks provide guidance for a range of topics, including transparency, accountability, fairness, privacy, security and safety. The levels of governance can vary depending on the organization's size, the complexity of the AI systems in use and the regulatory environment in which the organization operates. An overview of these approaches: Informal governance This is the least intensive approach to governance based on the values and principles of the organization. There may be some informal processes, such as ethical review boards or internal committees, but there is no formal structure or framework for AI governance. Ad hoc governance This is a step up from informal governance and involves the development of specific policies and procedures for AI development and use. This type of governance is often developed in response to specific challenges or risks and may not be comprehensive or systematic. Formal governance This is the highest level of governance and involves the development of a comprehensive AI governance framework. This framework reflects the organization's values and principles and aligns with relevant laws and regulations. Formal governance frameworks typically include risk assessment, ethical review and oversight processes. How organizations are deploying AI governance The concept of AI governance becomes increasingly vital as automation, driven by AI, becomes prevalent in sectors ranging from healthcare and finance to transportation and public services. The automation capabilities of AI can significantly enhance efficiency, decision-making and innovation, but they also introduce challenges related to accountability, transparency and ethical considerations. The governance of AI involves establishing robust control structures containing policies, guidelines and frameworks to address these challenges. It involves setting up mechanisms to continuously monitor and evaluate AI systems, ensuring they comply with established ethical norms and legal regulations. Effective governance structures in AI are multidisciplinary, involving stakeholders from various fields, including technology, law, ethics and business. As AI systems become more sophisticated and integrated into critical aspects of society, the role of AI governance in guiding and shaping the trajectory of AI development and its societal impact becomes ever more crucial. AI governance best practices involve an approach beyond mere compliance to encompass a more robust system for monitoring and managing AI applications. For enterprise-level businesses, the AI governance solution should enable broad oversight and control over AI systems. Here is a sample roadmap to consider: Visual dashboard: Utilize a dashboard that provides real-time updates on the health and status of AI systems, offering a clear overview for quick assessments. Health score metrics: Implement an overall health score for AI models using intuitive and easy-to-understand metrics to simplify monitoring. Automated monitoring: Employ automatic detection systems for bias, drift, performance and anomalies to ensure models function correctly and ethically. Performance alerts: Set up alerts for when a model deviates from its predefined performance parameters, enabling timely interventions. Custom metrics: Define custom metrics that align with the organization's key performance indicators (KPIs) and thresholds to ensure AI outcomes contribute to business objectives. Audit trails: Maintain easily accessible logs and audit trails for accountability and to facilitate reviews of AI systems' decisions and behaviors. Open-source tools compatibility: Choose open-source tools compatible with various machine learning development platforms to benefit from the flexibility and community support. Seamless integration: Ensure that the AI governance platform integrates seamlessly with the existing infrastructure, including databases and software ecosystems, to avoid silos and enable efficient workflows. By adhering to these practices, organizations can establish a robust AI governance framework that supports responsible AI development, deployment and management, ensuring that AI systems are compliant and aligned with ethical standards and organizational goals. What regulations require AI governance? AI governance practices and regulations have been adopted by a number of countries to prevent bias and discrimination. Below are just a few examples. It's important to remember that regulation is always in flux, and organizations who manage complex AI systems will need to keep a close eye as regional frameworks evolve. The U.S.'s SR-11-7 SR-11-71 is the U.S. regulatory model governance standard for effective and strong model governance in banking. The regulation requires bank officials to apply company-wide model risk management initiatives and maintain an inventory of models implemented for use, under development for implementation, or recently retired. Leaders of the institutions also must prove their models are achieving the business purpose they were intended to solve, and that they are up-to-date and have not drifted. Model development and validation must enable anyone unfamiliar with a model to understand the model’s operations, limitations and key assumptions. Canada's Directive on Automated Decision-Making Canada’s Directive on Automated Decision-Making2 describes how that country’s government uses AI to guide decisions in several departments. The directive uses a scoring system to assess the human intervention, peer review, monitoring and contingency planning needed for an AI tool built to serve citizens. Organizations creating AI solutions with a high score must conduct two independent peer reviews, offer public notice in plain language, develop a human intervention failsafe and establish recurring training courses for the system. As Canada’s Directive on Automated Decision-Making is a guidance for the country’s own development of AI, the regulation doesn’t directly affect companies the way SR 11-7 does in the U.S. Europe’s evolving AI regulations In April 2021, the European Commission presented its AI package3, including statements on fostering a European approach to excellence, trust and a proposal for a legal framework on AI. The statements declare that while most AI systems will fall into the category of "minimal risk," AI systems identified as "high risk" will be required to adhere to stricter requirements, and systems deemed "unacceptable risk" will be banned. Organizations must pay close attention to these rules or risk fines. AI governance guidelines in the Asia-Pacific region In the Asia-Pacific region, countries have released several principles and guidelines for governing AI. In 2019, Singapore’s federal government released a framework with guidelines for addressing issues of AI ethics in the private sector. India’s AI strategy framework recommends setting up a center for studying how to address issues related to AI ethics, privacy and more. China, Japan, South Korea, Australia and New Zealand are also exploring guidelines for AI governance.4 Related solutions watsonx Easily deploy and embed AI across your business, manage all data sources, and accelerate responsible AI workflows—all on one platform. Explore watsonx IBM OpenPages Simplify data governance, risk management and regulatory compliance with IBM OpenPages — a highly scalable, AI-powered, and unified GRC platform. Explore OpenPages Related resources Article Build responsible AI workflow Our latest eBook outlines the key building blocks of AI governance and shares a detailed AI governance framework that you can apply in your organization. Blog 3 key reasons why your organization needs Responsible AI Explore the transformative potential of Responsible AI for your organization and learn about the crucial drivers behind adopting ethical AI practices. Thought leadership Is your AI trustworthy? Join the discussion on why businesses need to prioritize AI governance to deploy responsible AI. Article What is AI ethics? AI ethics is a multidisciplinary field that studies how to optimize AI's beneficial impact while reducing risks and adverse outcomes. Learn about IBM's approach to AI ethics. Article What is explainable AI? Explainable AI is crucial for an organization in building trust and confidence when putting AI models into production. Article What is data governance? Learn how data governance ensures companies get the most from their data assets. Take the next step Accelerate responsible, transparent and explainable AI workflows across the lifecycle for both generative and machine learning models. Direct, manage, and monitor your organization’s AI activities to better manage growing AI regulations and detect and mitigate risk. Explore watsonx.governance Book a live demo Footnotes 1. “SR 11-7: Guidance on Model Risk Management.” (link resides outside ibm.com), Board of Governors of the Federal Reserve System Washington, D.C., Division of Banking Supervision and Regulation, 4 April 2011. 2. “Canada's New Federal Directive Makes Ethical AI a National Issue.” Digital, 8 March 2019. 3. "A European approach to artificial intelligence." (link resides outside ibm.com), European Commission, 14 December 2023. 4. “Wrestling With AI Governance Around The World.” (link resides outside ibm.com), Forbes, 27 March 2019.

