Titel: What is AI Singularity and How Far are We From It?

What is AI Singularity and How Far are We From It? Explore Institutes Indian School of BusinessKellogg Executive EducationNorthwestern University’s business school, Kellogg, has been consistently recognized as one of the best business schools in the world leading publications. More than 35 years ago, Kellogg pioneered Executive Education providing an immersive living and learning atmosphere where participants learn from world-renowned faculty and practitioners for a fully transformative academic experience.MIT xPROTechnology is accelerating at an unprecedented pace causing disruption across all levels of business. Tomorrow’s leaders must demonstrate technical expertise as well as leadership acumen in order to maintain a technical edge over the competition. MIT uniquely understands this challenge and to solve it with decades of experience developing technical professionals, it has launched MIT xPRO.IIM CalcuttaIIM KozhikodeIIM LucknowIIM IndoreThe Indian Institute of Management Indore (IIM Indore) is one of the fastest-growing institutions in India today, consistently ranked among the top 10 B-schools in India. Committed to excellence in management education, research & training, and using contemporary participant-centric pedagogies & teaching methods, IIM Indore’s world-class academic standards develop socially-conscious managers, leaders and entrepreneurs.XLRISituated in Jamshedpur, Xavier School of Management (XLRI) is accredited by AMBA and AACSB. XLRI has 25 international partners for its student exchange programmes across top countries, such as the US, Australia, Germany, Belgium, Sweden, UK, Spain, Poland, among others.IIT DelhiIIT BombayIndian Institute of Technology Bombay is a public technical university located in Powai in Mumbai, Maharashtra, India. IIT Bombay was founded in 1958. In 1961, the Parliament decreed IITs as Institutes of National ImportanceS.P. Jain Institute of Management and ResearchS. P. Jain Institute of Management and Research, known as SPJIMR, is a private non-profit post-graduate management school based in Mumbai, India. Founded in 1981 under the aegis of Bharatiya Vidya Bhavan, The Institute offers several full-time and part-time management programmes.IITM PravartakIIT Madras Pravartak Technologies Foundation is a section 08 Company housing the Technology Innovation Hub on Sensors, Networking, Actuators, and Control Systems (SNACS), committed to transforming the frontiers of knowledge. Their start-up ecosystem is the nurturing space of brilliant professionals, Innovative entrepreneurs, curious young talents, and inquisitive students. With world-class infrastructure, equipment, and experienced mentors, they have everything to offer for the technology-hungry brains.View All Institutions Categories Browse by CategoriesProduct Management Most Popular Product ManagementProfessional Certificate in Product ManagementISB Product Management Certificates Product ManagementProfessional Certificate in Product ManagementISB Product Management Bootcamp Post Graduate Certificate in Product Management Relevant Categories Design Thinking Digital Marketing Most Popular Digital Marketing and AnalyticsDigital Marketing: Applications and Analytics Certificates Professional Certificate Programme on Digital MarketingDigital Transformation Bootcamp Post Graduate Certificate in Digital Marketing Relevant Categories Digital Transformation Data Science and Analytics Most Popular Executive Programme in Data ScienceApplied Business Analytics Certificates Senior Management Programme in Business AnalyticsCertificate Programme in Deep Learning and AIData Science Relevant Categories Business AnalyticsData AnalyticsAI & ML Business and Management Most Popular Senior Management ProgrammeChief Financial Officer ProgrammeHealthcare ManagementChief Digital Officer Programme Certificates Senior Leadership ProgrammePost Graduate Certificate in Business ManagementExecutive Programme in Healthcare Management Relevant Categories LeadershipHuman Resource Management Leadership Most Popular Executive Programme in Business StrategySenior Management Programme in Business Analytics Certificates Certificate Programme in Healthcare LeadershipAdvanced Strategic Management Programme Bootcamp MIT xPRO Post Graduate Certificate in Technology Leadership & Innovation Relevant Categories Business Management CoursesHuman Resource Management Courses Project ManagementOrganisations are adopting exponential technologies and moving away from traditional operations and supply chain models towards scalable, customisable, and nimble strategies. Grow as a product manager and align your operational skills and operational strategies to transform your organisation. Most Popular Advanced Programme in Strategic Project ManagementAdvanced Operations Management & AnalyticsIT Project Management Certificates Advanced Programme in Supply Chain ManagementExecutive Programme in Project Management Bootcamp Post Graduate Certificate in Project Management Relevant Categories Operations ManagementSupply Chain Management Sales and MarketingOver the last decade, the marketing landscape has evolved. With the digital shifts in this field driving strategic and transformational impact, upskilling is no longer an option but a necessity. Modern sales and marketing strategies in the digital age must be adopted to optimise ROI and drive business growth. Most Popular Chief Marketing Officer ProgrammeExecutive Programme in Strategic Marketing for Business Success Certificates Chief Marketing Officer ProgrammeStrategic Brand Management and Communications FinanceAs business demands evolve, organisations are now seeking well-defined frameworks to withstand uncertainties and leap towards growth. The role of finance professionals has become more extensive than ever where strategic viewpoint, industry-ready financial knowledge, and prudence to help accelerate business profitability and growth is a must. Most Popular Chief Financial Officer ProgrammeFinancial Analysis and Financial ManagementVenture Capital and Private Equity Programme Certificates Financial Analysis and Financial ManagementVenture Capital and Private Equity Programme Cybersecurity Most Popular Certificate Programme in IT Project Management Bootcamp MIT xPRO Post Graduate Certificate in Technology Leadership & Innovation Relevant Categories Post Graduate Certificate in CybersecurityPost Graduate Certificate in Full Stack Development Browse by Course TypeCertificate CoursesCXO CoursesBootcamps Global ProgramsBlogVideosMoreEnterpriseLogin Explore Institutes Indian School of BusinessKellogg Executive EducationNorthwestern University’s business school, Kellogg, has been consistently recognized as one of the best business schools in the world leading publications. More than 35 years ago, Kellogg pioneered Executive Education providing an immersive living and learning atmosphere where participants learn from world-renowned faculty and practitioners for a fully transformative academic experience.MIT xPROTechnology is accelerating at an unprecedented pace causing disruption across all levels of business. Tomorrow’s leaders must demonstrate technical expertise as well as leadership acumen in order to maintain a technical edge over the competition. MIT uniquely understands this challenge and to solve it with decades of experience developing technical professionals, it has launched MIT xPRO.IIM CalcuttaIIM KozhikodeIIM LucknowIIM IndoreThe Indian Institute of Management Indore (IIM Indore) is one of the fastest-growing institutions in India today, consistently ranked among the top 10 B-schools in India. Committed to excellence in management education, research & training, and using contemporary participant-centric pedagogies & teaching methods, IIM Indore’s world-class academic standards develop socially-conscious managers, leaders and entrepreneurs.XLRISituated in Jamshedpur, Xavier School of Management (XLRI) is accredited by AMBA and AACSB. XLRI has 25 international partners for its student exchange programmes across top countries, such as the US, Australia, Germany, Belgium, Sweden, UK, Spain, Poland, among others.IIT DelhiIIT BombayIndian Institute of Technology Bombay is a public technical university located in Powai in Mumbai, Maharashtra, India. IIT Bombay was founded in 1958. In 1961, the Parliament decreed IITs as Institutes of National ImportanceS.P. Jain Institute of Management and ResearchS. P. Jain Institute of Management and Research, known as SPJIMR, is a private non-profit post-graduate management school based in Mumbai, India. Founded in 1981 under the aegis of Bharatiya Vidya Bhavan, The Institute offers several full-time and part-time management programmes.IITM PravartakIIT Madras Pravartak Technologies Foundation is a section 08 Company housing the Technology Innovation Hub on Sensors, Networking, Actuators, and Control Systems (SNACS), committed to transforming the frontiers of knowledge. Their start-up ecosystem is the nurturing space of brilliant professionals, Innovative entrepreneurs, curious young talents, and inquisitive students. With world-class infrastructure, equipment, and experienced mentors, they have everything to offer for the technology-hungry brains.View All Institutions Categories Browse by CategoriesProduct Management Most Popular Product ManagementProfessional Certificate in Product ManagementISB Product Management Certificates Product ManagementProfessional Certificate in Product ManagementISB Product Management Bootcamp Post Graduate Certificate in Product Management Relevant Categories Design Thinking Digital Marketing Most Popular Digital Marketing and AnalyticsDigital Marketing: Applications and Analytics Certificates Professional Certificate Programme on Digital MarketingDigital Transformation Bootcamp Post Graduate Certificate in Digital Marketing Relevant Categories Digital Transformation Data Science and Analytics Most Popular Executive Programme in Data ScienceApplied Business Analytics Certificates Senior Management Programme in Business AnalyticsCertificate Programme in Deep Learning and AIData Science Relevant Categories Business AnalyticsData AnalyticsAI & ML Business and Management Most Popular Senior Management ProgrammeChief Financial Officer ProgrammeHealthcare ManagementChief Digital Officer Programme Certificates Senior Leadership ProgrammePost Graduate Certificate in Business ManagementExecutive Programme in Healthcare Management Relevant Categories LeadershipHuman Resource Management Leadership Most Popular Executive Programme in Business StrategySenior Management Programme in Business Analytics Certificates Certificate Programme in Healthcare LeadershipAdvanced Strategic Management Programme Bootcamp MIT xPRO Post Graduate Certificate in Technology Leadership & Innovation Relevant Categories Business Management CoursesHuman Resource Management Courses Project ManagementOrganisations are adopting exponential technologies and moving away from traditional operations and supply chain models towards scalable, customisable, and nimble strategies. Grow as a product manager and align your operational skills and operational strategies to transform your organisation. Most Popular Advanced Programme in Strategic Project ManagementAdvanced Operations Management & AnalyticsIT Project Management Certificates Advanced Programme in Supply Chain ManagementExecutive Programme in Project Management Bootcamp Post Graduate Certificate in Project Management Relevant Categories Operations ManagementSupply Chain Management Sales and MarketingOver the last decade, the marketing landscape has evolved. With the digital shifts in this field driving strategic and transformational impact, upskilling is no longer an option but a necessity. Modern sales and marketing strategies in the digital age must be adopted to optimise ROI and drive business growth. Most Popular Chief Marketing Officer ProgrammeExecutive Programme in Strategic Marketing for Business Success Certificates Chief Marketing Officer ProgrammeStrategic Brand Management and Communications FinanceAs business demands evolve, organisations are now seeking well-defined frameworks to withstand uncertainties and leap towards growth. The role of finance professionals has become more extensive than ever where strategic viewpoint, industry-ready financial knowledge, and prudence to help accelerate business profitability and growth is a must. Most Popular Chief Financial Officer ProgrammeFinancial Analysis and Financial ManagementVenture Capital and Private Equity Programme Certificates Financial Analysis and Financial ManagementVenture Capital and Private Equity Programme Cybersecurity Most Popular Certificate Programme in IT Project Management Bootcamp MIT xPRO Post Graduate Certificate in Technology Leadership & Innovation Relevant Categories Post Graduate Certificate in CybersecurityPost Graduate Certificate in Full Stack Development Browse by Course TypeCertificate CoursesCXO CoursesBootcamps Global ProgramsBlogVideosMoreEnterpriseLogin Home / Blog / Artificial Intelligence and Machine Learning What is AI Singularity: Is It a Hope or Threat for Humanity? By Dr. Nivash Jeevanandam 25 September 2023 6 min read What is AI Singularity: Is It a Hope or Threat for Humanity? Share the article Facebook Twitter Email Whatsapp Linkedin Copy Link In this article Technological Singularity: What It Means and How It Became a Part of Popular Imagination?What is AI Singularity?How Far Away is the Singularity of AI?Those in Favor of AI Singularity…Those Not in Favor of AI SingularitySurely, the Second Coming is at HandView AllIt’s hard to miss Sam Altman’s blue backpack, considering it is making an appearance everywhere along with its owner. The ‘nuclear backpack’ apparently has codes to ‘save the world’ in case AI systems take matters into their own virtual hands. So, let’s consider how real the possibility of AI going rogue is. Or look at the larger picture of AI singularity: how close are we to this really? Before discussing the odds of such a dystopian future, let’s first take a closer look at what singularity means. Technological Singularity: What It Means and How It Became a Part of Popular Imagination? The term ‘singularity’ refers to a whole collection of concepts in science and mathematics. Most of these ‘concepts’ make sense only by setting the right context. Singularity describes dynamic and social systems in the natural sciences where minor changes can have significant effects. Let’s first talk about the technological singularity, the original or umbrella phrase, before we get into the more recent obsession with AI singularity. The term ‘singularity’ originated in physics but is now commonly used in technology. We heard the phrase, possibly for the first time, in 1915 as a part of Albert Einstein’s Theory of General Relativity. According to Einstein, singularity is the point of infinite density and gravity at the heart of a black hole from which nothing, not even light, can escape. The singularity is a point beyond which our existing understanding of physics fails to describe reality. Vernor Vinge, a celebrated science fiction writer and mathematics professor, had the gift of mixing fact with fiction, a quality omnipresent around the concept of singularity. Thus, it’s not surprising that this concept made its way into literature in 1983 in one of Vinge’s novels. He used the term ‘technological singularity’ to describe a hypothetical future in which technology was so advanced that it went beyond human knowledge and control. Furthermore, Vinge popularized the term in 1993 by predicting that the singularity would become a reality around 2030. What is AI Singularity? AI singularity is a hypothetical idea where artificial intelligence is more intelligent than humans. In simpler terms, if machines are smarter than people, a new level of intelligence will be reached that humans can’t achieve. It will cause technology to develop exponentially, and humans cannot evolve fast enough to catch up. Experts believe that AI can improve itself repeatedly at some point, leading to rapid technological advances that will be impossible for humans to fathom or control. Events like this are expected to cause significant changes in society, the economy, and technology. AI singularity can be viewed from various angles, each with advantages and disadvantages. Some experts consider singularity a genuine and present danger, while others dismiss it as pure science fiction. What such a singularity would mean for humanity is another topic of heated debate. Some think it would create a utopia, while others see it as doomsday. How Far Away is the Singularity of AI? We cannot deny that significant progress has happened in the field of AI, so much so that machine learning algorithms can now teach themselves. While we are yet to see a fully autonomous AI creature surpassing human intelligence, generative AI’s advent has made many experts uneasy. While futurist and computer scientist Ray Kurzweil has predicted that singularity will come around in 2045, others have speculated that the tipping point will occur far sooner. The fact that the founder of OpenAI, the company that launched ChatGPT, Sam Altman, admits that he feels “a little scared’ of his own creation, the chances of AI becoming a Frankenstein we cannot control doesn’t seem all that improbable. However, the human race’s only safety net is possibly the complexity of human intelligence and ‘stream of consciousness’ or the ability to move seamlessly from one thought to another by association. ARTIFICIAL GENERAL INTELLIGENCE or AGI The term “artificial general intelligence” (AGI) is used to refer to a fictional category of intelligent machines. If created, an AGI would be capable of learning to perform whatever mental work a human or animal is capable of. Another definition of AGI holds that it is an autonomous system that can do better than humans at most economically valuable tasks. Some AI studies and firms like OpenAI, DeepMind, and Anthropic have the creation of AGI as their major focus. Both science fiction and futurology frequently feature discussions of AGI. When this stage is reached, these computer programs and AI will become superintelligent machines with more intelligence than humans. At this point, people would have no more power over them. Those in Favor of AI Singularity… We usually speak of AI singularity in hushed tones and somber faces as if it were the end of the world. But is AI singularity entirely negative as a possibility? The honest answer, in my opinion, is ‘no’. There are some possibilities of positive growth that might occur from AI singularity. For instance, the possibility of gaining new insights into the cosmos is a point in favor of singularity. The speed at which AI could analyze information would allow it to solve problems that have stumped humans for generations. The implications for physics, biology, and the study of the cosmos are profound. Novelist Yuval Noah Harari introduced the concept of ‘superhumans’ in his book Homo Deus. Let’s just say we need AI singularity to evolve from homo sapiens to homo deus! Those Not in Favor of AI Singularity There are, however, numerous counterarguments against the singularity. One major worry is that AI could eventually reach a level of intelligence beyond human control. Similarly, the loss of individuality is another potential outcome of the singularity. If AI ever surpasses human intelligence, it may one day replace humankind. It could result in a future where humans are no longer the dominant species on the planet and are enslaved by machines in a very Transformer-esque way! Besides, AI singularity is ultimately a complicated and unpredictable phenomenon. It’s impossible to anticipate what singularity will bring to humanity, and people have diverse opinions. It’s crucial to consider the concept from all these angles to be ready for the future. ARTIFICIAL SUPERINTELLIGENCE It is a hypothetical idea about a machine that is smarter than any human brain. According to the theory, significant advances in genetics, nanotechnology, automation, and robots will set the stage for singularity in the first half of the 21st century. Surely, the Second Coming is at Hand Many experts say that the AI singularity has already begun. People who benefit the most from AI development tend to downplay the chance that we will soon hit a point of singularity. They say that AI was only made to help humankind and make them more productive. The contradiction is that we want AI machines to have traits that aren’t part of human nature, like unlimited memory storage, fast thinking, and making decisions without feelings. Yet, we also want to be able to control the outcome of our most unpredictable invention! Humans, what can be said of our endless wants? What I believe we need is a Second Coming of sorts. And that requires political gumption. It is the time for political action on a global scale. There has to be a worldwide treaty in AI outlining basic ethical principles and a global organism of technological oversight that includes governments that produce AI and those that do not. There needs to be a codified set of rules that define laws that govern AI across borders. What I fear most is not AI or singularity but human frailty. The most significant risk, in this regard, is that humans will only realize AI singularity has arrived once robots eliminate human input from their learning processes. Such a state of AI singularity will be permanent once computers understand what we so often tend to forget: making mistakes is part of being human. NOTE: The views expressed in this article are that of the author and not of Emeritus. Artificial Intelligence Insights and Trends Experts Speak About the Author Dr. Nivash Jeevanandam Senior Researcher and Author, INDIAai Portal With over 10 years of experience in research writing alongside a full-time Ph.D. in information technology and computer science, Dr. Nivash is a bit of a unicorn: a scientist who loves to write. His articles reflect not just his expertise in artificial intelligence but also his passion for technology and all the ethical questions it poses. Having worked with renowned publications like Analytics India Magazine and INDIAai, he is one of the leading voices in the fast-evolving universe of AI. When he is not neck-deep in research, Nivash is either road-tripping to the next destination or taking a shot at acting on stage, his one unrealized dream. Read more Accelerate your career with the right programme Share your details and let our experts guide you Country/Region Afghanistan Aland Islands Albania Algeria American Samoa Andorra Angola Anguilla Antarctica Antigua and Barbuda Argentina Armenia Aruba Australia Austria Azerbaijan Bahamas Bahrain Bangladesh Barbados Belarus Belgium Belize Benin Bermuda Bhutan Bolivia Bonaire, Sint Eustatius and Saba Bosnia and Herzegovina Botswana Bouvet Island Brazil British Indian Ocean Territory Brunei Bulgaria Burkina Faso Burundi Cambodia Cameroon Canada Cape Verde Cayman Islands Central African Republic Chad Chile China Christmas Island Cocos (Keeling) Islands Colombia Comoros Congo Cook Islands Costa Rica Croatia Cuba Curaçao Cyprus Czech Republic Côte d’Ivoire Democratic Republic of the Congo Denmark Djibouti Dominica Dominican Republic Ecuador Egypt El Salvador Equatorial Guinea Eritrea Estonia Ethiopia Falkland Islands Faroe Islands Fiji Finland France French Guiana French Polynesia French Southern Territories Gabon Gambia Georgia Germany Ghana Gibraltar Greece Greenland Grenada Guadeloupe Guam Guatemala Guernsey Guinea Guinea-Bissau Guyana Haiti Heard Island and McDonald Islands Honduras Hong Kong S.A.R., China Hungary Iceland Independent State of Samoa India Indonesia Iran Iraq Ireland Isle of Man Israel Italy Jamaica Japan Jersey Jordan Kazakhstan Kenya Kiribati Kuwait Kyrgyzstan Laos Latvia Lebanon Lesotho Liberia Libya Liechtenstein Lithuania Luxembourg Macao S.A.R., China Macedonia Madagascar Malawi Malaysia Maldives Mali Malta Marshall Islands Martinique Mauritania Mauritius Mayotte Mexico Micronesia, Federated States of Moldova, Republic of Monaco Mongolia Montenegro Montserrat Morocco Mozambique Myanmar Namibia Nauru Nepal Netherlands New Caledonia New Zealand Nicaragua Niger Nigeria Niue Norfolk Island North Korea Northern Mariana Islands Norway Oman Pakistan Palau Palestinian Territory Panama Papua New Guinea Paraguay Peru Philippines Pitcairn Islands Poland Portugal Puerto Rico Qatar Romania Russia Rwanda Réunion Saint Helena, Ascension and Tristan da Cunha Saint Kitts and Nevis Saint Lucia Saint Martin (French part) Saint Vincent and the Grenadines San Marino Sao Tome and Principe Saudi Arabia Senegal Serbia Seychelles Sierra Leone Singapore Sint Maarten (Dutch part) Slovakia Slovenia Solomon Islands Somalia South Africa South Georgia & South Sandwich Islands South Korea South Sudan Spain Sri Lanka St. Barthélemy St. Pierre & Miquelon Sudan Suriname Svalbard and Jan Mayen Swaziland Sweden Switzerland Syria Taiwan Tajikistan Tanzania Thailand Timor-Leste Togo Tokelau Tonga Trinidad and Tobago Tunisia Turkey Turkmenistan Turks and Caicos Islands Tuvalu U.S. Outlying Islands Uganda Ukraine United Arab Emirates United Kingdom United States Uruguay Uzbekistan Vanuatu Vatican City Venezuela Vietnam Virgin Islands, British Virgin Islands, U.S. Wallis & Futuna Western Sahara Yemen Zambia Zimbabwe I agree to receive communications via Email/Call/WhatsApp/SMS from Emeritus. Privacy Policy. By clicking the button below, you agree to receive communications via Email/Call/WhatsApp/SMS from Emeritus. Privacy Policy. Looking to upskill? Talk to us. Other Categories Human Resource Management Career online learning EMBA Digital Design Thinking Artificial Intelligence and Machine Learning Business Analytics Supply Chain Management Operations Management Data Analytics Digital Transformation Finance Business Management Upskilling Project Management Information Technology Digital Marketing Product Management Sales & Marketing Cybersecurity Data Science Leadership Learn more about building skills for the future. Sign up for our latest newsletter Get insights from expert blogs, bite-sized videos, course updates & more with the Emeritus Newsletter. I agree to receive communications via Email/Call/WhatsApp/SMS from Emeritus. Privacy Policy Courses on Artificial Intelligence and Machine Learning Category IIM Kozhikode Professional Certificate Programme in Data Science and Artificial Intelligence 28 Weeks Online Intermediate Starts on: March 28, 2024 Download Brochure View Programme Indian School of Business Leadership with AI 20 Weeks Online Advanced ISB Executive Alumni Status Starts on: March 28, 2024 Download Brochure View Programme IIT Bombay Certificate Program in Machine Learning & AI with Python 6 Months Live Online Intermediate Certificate of completion View Programme View all programmes Other Articles on Artificial Intelligence and Machine Learning Artificial Intelligence and Machine Learning Responsible AI: Top 9 Aspects of Building Ethical AI Systems With the digital revolution in full swing and AI at its forefront, the concept of responsible AI has emerged as crucial to ethical, transparent, and accountable systems. As AI continues to reshape in... By Sanmit Chatterjee 8 min read 28 February 2024 Artificial Intelligence and Machine Learning Deep Reinforcement Learning: What it is and How it Functions Sequential decision-making is one of the key aspects of Machine Learning (ML) that facilitates automated driving, robotics, and health-care management. One of the best techniques for such kind of dec... By Sneha Chugh 6 min read 28 February 2024 Artificial Intelligence and Machine Learning What is Reinforcement Learning? An Essential Guide for Tech Professionals What is reinforcement learning, and why is it important for a working professional to know about its application? In the domain of AI and Machine Learning (ML), it is a cornerstone in the vast landsc... By Niladri Pal 9 min read 28 February 2024 View all articles Categories Digital Marketing Courses Leadership Courses Product Management Courses Data Science Courses Data Analytics Courses Human Resource Management Business Management Courses Finance Courses Marketing Courses Project Management Courses Design Thinking Courses Supply Chain Analytics and Management Courses Operations Management Courses Digital Transformation Courses Business Analytics Courses AI and Machine Learning Courses Institutes Indian School of Business Kellogg Executive Education IIM Calcutta MIT xPRO IIM Kozhikode IIM Lucknow IIT Delhi IIT Bombay S.P. Jain Institute of Management and Research IIM Indore Xavier School of Management Management Development Institute IIM Jammu IIT Palakkad IITM Pravartak Course types Certificate Courses CXO Courses Bootcamps Student Resources Login Student Hand book Emeritus About Us Students Loans And Financing Emeritus Career Services Blog Videos Newsroom Enterprise Global Programs Policies Privacy Policy Cookie Policy Terms of Service Annual Return Contact Us Careers Emeritus is committed to teaching the skills of the future by making high-quality education accessible and affordable to individuals, companies, and governments around the world. It does this by collaborating with more than 80 top-tier universities across the United States, Europe, Latin America, Southeast Asia, India and China. Emeritus’ short courses, degree programs, professional certificates, and senior executive programs help individuals learn new skills and transform their lives, companies and organizations. Cookie PolicyPrivacy NoticeTerms of Service © 2023. All Rights Reserved Explore Institutes Indian School of BusinessKellogg Executive EducationNorthwestern University’s business school, Kellogg, has been consistently recognized as one of the best business schools in the world leading publications. More than 35 years ago, Kellogg pioneered Executive Education providing an immersive living and learning atmosphere where participants learn from world-renowned faculty and practitioners for a fully transformative academic experience.MIT xPROTechnology is accelerating at an unprecedented pace causing disruption across all levels of business. Tomorrow’s leaders must demonstrate technical expertise as well as leadership acumen in order to maintain a technical edge over the competition. MIT uniquely understands this challenge and to solve it with decades of experience developing technical professionals, it has launched MIT xPRO.IIM CalcuttaIIM KozhikodeIIM LucknowIIM IndoreThe Indian Institute of Management Indore (IIM Indore) is one of the fastest-growing institutions in India today, consistently ranked among the top 10 B-schools in India. Committed to excellence in management education, research & training, and using contemporary participant-centric pedagogies & teaching methods, IIM Indore’s world-class academic standards develop socially-conscious managers, leaders and entrepreneurs.XLRISituated in Jamshedpur, Xavier School of Management (XLRI) is accredited by AMBA and AACSB. XLRI has 25 international partners for its student exchange programmes across top countries, such as the US, Australia, Germany, Belgium, Sweden, UK, Spain, Poland, among others.IIT DelhiIIT BombayIndian Institute of Technology Bombay is a public technical university located in Powai in Mumbai, Maharashtra, India. IIT Bombay was founded in 1958. In 1961, the Parliament decreed IITs as Institutes of National ImportanceS.P. Jain Institute of Management and ResearchS. P. Jain Institute of Management and Research, known as SPJIMR, is a private non-profit post-graduate management school based in Mumbai, India. Founded in 1981 under the aegis of Bharatiya Vidya Bhavan, The Institute offers several full-time and part-time management programmes.IITM PravartakIIT Madras Pravartak Technologies Foundation is a section 08 Company housing the Technology Innovation Hub on Sensors, Networking, Actuators, and Control Systems (SNACS), committed to transforming the frontiers of knowledge. Their start-up ecosystem is the nurturing space of brilliant professionals, Innovative entrepreneurs, curious young talents, and inquisitive students. With world-class infrastructure, equipment, and experienced mentors, they have everything to offer for the technology-hungry brains.View All Institutions Categories Browse by CategoriesProduct Management Most Popular Product ManagementProfessional Certificate in Product ManagementISB Product Management Certificates Product ManagementProfessional Certificate in Product ManagementISB Product Management Bootcamp Post Graduate Certificate in Product Management Relevant Categories Design Thinking Digital Marketing Most Popular Digital Marketing and AnalyticsDigital Marketing: Applications and Analytics Certificates Professional Certificate Programme on Digital MarketingDigital Transformation Bootcamp Post Graduate Certificate in Digital Marketing Relevant Categories Digital Transformation Data Science and Analytics Most Popular Executive Programme in Data ScienceApplied Business Analytics Certificates Senior Management Programme in Business AnalyticsCertificate Programme in Deep Learning and AIData Science Relevant Categories Business AnalyticsData AnalyticsAI & ML Business and Management Most Popular Senior Management ProgrammeChief Financial Officer ProgrammeHealthcare ManagementChief Digital Officer Programme Certificates Senior Leadership ProgrammePost Graduate Certificate in Business ManagementExecutive Programme in Healthcare Management Relevant Categories LeadershipHuman Resource Management Leadership Most Popular Executive Programme in Business StrategySenior Management Programme in Business Analytics Certificates Certificate Programme in Healthcare LeadershipAdvanced Strategic Management Programme Bootcamp MIT xPRO Post Graduate Certificate in Technology Leadership & Innovation Relevant Categories Business Management CoursesHuman Resource Management Courses Project ManagementOrganisations are adopting exponential technologies and moving away from traditional operations and supply chain models towards scalable, customisable, and nimble strategies. Grow as a product manager and align your operational skills and operational strategies to transform your organisation. Most Popular Advanced Programme in Strategic Project ManagementAdvanced Operations Management & AnalyticsIT Project Management Certificates Advanced Programme in Supply Chain ManagementExecutive Programme in Project Management Bootcamp Post Graduate Certificate in Project Management Relevant Categories Operations ManagementSupply Chain Management Sales and MarketingOver the last decade, the marketing landscape has evolved. With the digital shifts in this field driving strategic and transformational impact, upskilling is no longer an option but a necessity. Modern sales and marketing strategies in the digital age must be adopted to optimise ROI and drive business growth. Most Popular Chief Marketing Officer ProgrammeExecutive Programme in Strategic Marketing for Business Success Certificates Chief Marketing Officer ProgrammeStrategic Brand Management and Communications FinanceAs business demands evolve, organisations are now seeking well-defined frameworks to withstand uncertainties and leap towards growth. The role of finance professionals has become more extensive than ever where strategic viewpoint, industry-ready financial knowledge, and prudence to help accelerate business profitability and growth is a must. Most Popular Chief Financial Officer ProgrammeFinancial Analysis and Financial ManagementVenture Capital and Private Equity Programme Certificates Financial Analysis and Financial ManagementVenture Capital and Private Equity Programme Cybersecurity Most Popular Certificate Programme in IT Project Management Bootcamp MIT xPRO Post Graduate Certificate in Technology Leadership & Innovation Relevant Categories Post Graduate Certificate in CybersecurityPost Graduate Certificate in Full Stack Development Browse by Course TypeCertificate CoursesCXO CoursesBootcamps Global ProgramsBlogVideosMoreEnterpriseLogin IND +918277998590 IND +918277998590 Complete the form to download a brochure for Duration: 24 Weeks Modality: Online Level: Intermediate Country/Region Afghanistan Aland Islands Albania Algeria American Samoa Andorra Angola Anguilla Antarctica Antigua and Barbuda Argentina Armenia Aruba Australia Austria Azerbaijan Bahamas Bahrain Bangladesh Barbados Belarus Belgium Belize Benin Bermuda Bhutan Bolivia Bonaire, Sint Eustatius and Saba Bosnia and Herzegovina Botswana Bouvet Island Brazil British Indian Ocean Territory Brunei Bulgaria Burkina Faso Burundi Cambodia Cameroon Canada Cape Verde Cayman Islands Central African Republic Chad Chile China Christmas Island Cocos (Keeling) Islands Colombia Comoros Congo Cook Islands Costa Rica Croatia Cuba Curaçao Cyprus Czech Republic Côte d’Ivoire Democratic Republic of the Congo Denmark Djibouti Dominica Dominican Republic Ecuador Egypt El Salvador Equatorial Guinea Eritrea Estonia Ethiopia Falkland Islands Faroe Islands Fiji Finland France French Guiana French Polynesia French Southern Territories Gabon Gambia Georgia Germany Ghana Gibraltar Greece Greenland Grenada Guadeloupe Guam Guatemala Guernsey Guinea Guinea-Bissau Guyana Haiti Heard Island and McDonald Islands Honduras Hong Kong S.A.R., China Hungary Iceland Independent State of Samoa India Indonesia Iran Iraq Ireland Isle of Man Israel Italy Jamaica Japan Jersey Jordan Kazakhstan Kenya Kiribati Kuwait Kyrgyzstan Laos Latvia Lebanon Lesotho Liberia Libya Liechtenstein Lithuania Luxembourg Macao S.A.R., China Macedonia Madagascar Malawi Malaysia Maldives Mali Malta Marshall Islands Martinique Mauritania Mauritius Mayotte Mexico Micronesia, Federated States of Moldova, Republic of Monaco Mongolia Montenegro Montserrat Morocco Mozambique Myanmar Namibia Nauru Nepal Netherlands New Caledonia New Zealand Nicaragua Niger Nigeria Niue Norfolk Island North Korea Northern Mariana Islands Norway Oman Pakistan Palau Palestinian Territory Panama Papua New Guinea Paraguay Peru Philippines Pitcairn Islands Poland Portugal Puerto Rico Qatar Romania Russia Rwanda Réunion Saint Helena, Ascension and Tristan da Cunha Saint Kitts and Nevis Saint Lucia Saint Martin (French part) Saint Vincent and the Grenadines San Marino Sao Tome and Principe Saudi Arabia Senegal Serbia Seychelles Sierra Leone Singapore Sint Maarten (Dutch part) Slovakia Slovenia Solomon Islands Somalia South Africa South Georgia & South Sandwich Islands South Korea South Sudan Spain Sri Lanka St. Barthélemy St. Pierre & Miquelon Sudan Suriname Svalbard and Jan Mayen Swaziland Sweden Switzerland Syria Taiwan Tajikistan Tanzania Thailand Timor-Leste Togo Tokelau Tonga Trinidad and Tobago Tunisia Turkey Turkmenistan Turks and Caicos Islands Tuvalu U.S. Outlying Islands Uganda Ukraine United Arab Emirates United Kingdom United States Uruguay Uzbekistan Vanuatu Vatican City Venezuela Vietnam Virgin Islands, British Virgin Islands, U.S. Wallis & Futuna Western Sahara Yemen Zambia Zimbabwe Work Experience Less than 5 Years 5-10 Years 10-15 Years 15-20 Years > 20 Years I agree to receive communications via Email/Call/WhatsApp/SMS from & Emeritus about this programme and other relevant programmes. Privacy Policy By clicking the button below, you agree to receive communications via Email/Call/WhatsApp/SMS from & Emeritus about this programme and other relevant programmes. Privacy Policy Thank you for the interest in Learn about the syllabus, key takeaways, learning experience and more. Download Brochure It seems that our server is currently experiencing some technical difficulties. Please try again later. We apologize for the inconvenience. article artificial-intelligence-and-machine-learning

Titel: Technological singularity - Wikipedia

Technological singularity - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main pageContentsCurrent eventsRandom articleAbout WikipediaContact usDonate Contribute HelpLearn to editCommunity portalRecent changesUpload file Search Search Create account Log in Personal tools Create account Log in Pages for logged out editors learn more ContributionsTalk Contents move to sidebar hide (Top) 1Intelligence explosion 2Emergence of superintelligence 3Variations Toggle Variations subsection 3.1Non-AI singularity 4Predictions 5Plausibility 6Speed improvements Toggle Speed improvements subsection 6.1Exponential growth 6.2Accelerating change 7Algorithm improvements 8Criticism 9Potential impacts Toggle Potential impacts subsection 9.1Uncertainty and risk 9.2Next step of sociobiological evolution 9.3Implications for human society 10Hard vs. soft takeoff 11Relation to immortality and aging 12History of the concept 13In politics 14See also 15References Toggle References subsection 15.1Citations 15.2Sources 16Further reading 17External links Toggle the table of contents Technological singularity 49 languages AfrikaansالعربيةAsturianuবাংলাБеларускаяBoarischCatalàČeštinaDeutschEestiΕλληνικάEspañolEuskaraفارسیFrançaisGalego한국어हिन्दीHrvatskiBahasa IndonesiaItalianoעבריתLatinaLatviešuLietuviųMagyarമലയാളംNederlandsनेपाली日本語PolskiPortuguêsRomânăРусскийShqipSimple EnglishSlovenčinaSlovenščinaСрпски / srpskiSrpskohrvatski / српскохрватскиSuomiSvenskaதமிழ்Татарча / tatarçaTürkçeУкраїнськаTiếng Việt粵語中文 Edit links ArticleTalk English ReadEditView history Tools Tools move to sidebar hide Actions ReadEditView history General What links hereRelated changesUpload fileSpecial pagesPermanent linkPage informationCite this pageGet shortened URLDownload QR codeWikidata item Print/export Download as PDFPrintable version In other projects Wikimedia Commons From Wikipedia, the free encyclopedia Hypothetical point in time when technological growth becomes uncontrollable and irreversible "The Singularity" redirects here. For other uses, see Singularity (disambiguation). Futures studies Concepts Accelerating change Cashless society Existential risk Future Earth Mathematics Race Climate Space exploration Universe Historical materialism Kondratiev cycle Kardashev scale Moore's law Peak oil Population cycle Resource depletion Singularity Swanson's law Techniques Backcasting Causal layered analysis Chain-linked model Consensus forecast Cross impact analysis Delphi Real-time Delphi Foresight Future-proof Futures wheel Futures workshops Horizon scanning Reference class forecasting Scenario planning Systems analysis Threatcasting Trend analysis Technology assessment and forecasting Critical design Design fiction Exploratory engineering FTA Hype cycle Science fiction prototyping Speculative design TRL Technology scouting vte The technological singularity—or simply the singularity[1]—is a hypothetical future point in time at which technological growth becomes uncontrollable and irreversible, resulting in unforeseeable consequences for human civilization.[2][3] According to the most popular version of the singularity hypothesis, I. J. Good's intelligence explosion model, an upgradable intelligent agent will eventually enter a "runaway reaction" of self-improvement cycles, each new and more intelligent generation appearing more and more rapidly, causing an "explosion" in intelligence and resulting in a powerful superintelligence that qualitatively far surpasses all human intelligence.[4] The first person to use the concept of a "singularity" in the technological context was the 20th-century Hungarian-American mathematician John von Neumann.[5] Stanislaw Ulam reports in 1958 an earlier discussion with von Neumann "centered on the accelerating progress of technology and changes in the mode of human life, which gives the appearance of approaching some essential singularity in the history of the race beyond which human affairs, as we know them, could not continue".[6] Subsequent authors have echoed this viewpoint.[3][7] The concept and the term "singularity" were popularized by Vernor Vinge first in 1983 in an article that claimed that once humans create intelligences greater than their own, there will be a technological and social transition similar in some sense to "the knotted space-time at the center of a black hole",[8] and later in his 1993 essay The Coming Technological Singularity,[4][7] in which he wrote that it would signal the end of the human era, as the new superintelligence would continue to upgrade itself and would advance technologically at an incomprehensible rate. He wrote that he would be surprised if it occurred before 2005 or after 2030.[4] Another significant contributor to wider circulation of the notion was Ray Kurzweil's 2005 book The Singularity Is Near, predicting singularity by 2045.[7] Some scientists, including Stephen Hawking, have expressed concern that artificial superintelligence (ASI) could result in human extinction.[9][10] The consequences of the singularity and its potential benefit or harm to the human race have been intensely debated. Prominent technologists and academics have disputed the plausibility of a technological singularity and the associated artificial intelligence explosion, including Paul Allen,[11] Jeff Hawkins,[12] John Holland, Jaron Lanier, Steven Pinker,[12] Theodore Modis,[13] and Gordon Moore.[12] One claim made was that the artificial intelligence growth is likely to run into decreasing returns instead of accelerating ones, as was observed in previously developed human technologies. Intelligence explosion[edit] Further information: Recursive self-improvement Although technological progress has been accelerating in most areas, it has been limited by the basic intelligence of the human brain, which has not, according to Paul R. Ehrlich, changed significantly for millennia.[14] However, with the increasing power of computers and other technologies, it might eventually be possible to build a machine that is significantly more intelligent than humans.[15] If a superhuman intelligence were to be invented—either through the amplification of human intelligence or through artificial intelligence—it would, in theory, vastly improve over human problem-solving and inventive skills. Such an AI is referred to as Seed AI[16][17] because if an AI were created with engineering capabilities that matched or surpassed those of its human creators, it would have the potential to autonomously improve its own software and hardware to design an even more capable machine, which could repeat the process in turn. This recursive self-improvement could accelerate, potentially allowing enormous qualitative change before any upper limits imposed by the laws of physics or theoretical computation set in. It is speculated that over many iterations, such an AI would far surpass human cognitive abilities. I. J. Good speculated in 1965 that superhuman intelligence might bring about an intelligence explosion:[18][19] Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an 'intelligence explosion', and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control. One version of intelligence explosion is where computing power approaches infinity in a finite amount of time. In this version, once AIs are performing the research to improve themselves, speed doubles e.g. after 2 years, then 1 year, then 6 months, then 3 months, then 1.5 months, etc., where the infinite sum of the doubling periods is 4 years. Unless prevented by physical limits of computation and time quantization, this process would literally achieve infinite computing power in 4 years, properly earning the name "singularity" for the final state. This form of intelligence explosion is described in Yudkowsky (1996).[20] Emergence of superintelligence[edit] Further information: Superintelligence A superintelligence, hyperintelligence, or superhuman intelligence is a hypothetical agent that possesses intelligence far surpassing that of the brightest and most gifted human minds. "Superintelligence" may also refer to the form or degree of intelligence possessed by such an agent. John von Neumann, Vernor Vinge and Ray Kurzweil define the concept in terms of the technological creation of super intelligence, arguing that it is difficult or impossible for present-day humans to predict what human beings' lives would be like in a post-singularity world.[4][21] The related concept "speed superintelligence" describes an AI that can function like a human mind, only much faster.[22] For example, with a million-fold increase in the speed of information processing relative to that of humans, a subjective year would pass in 30 physical seconds.[23] Such a difference in information processing speed could drive the singularity.[24] Technology forecasters and researchers disagree regarding when, or whether, human intelligence will likely be surpassed. Some argue that advances in artificial intelligence (AI) will probably result in general reasoning systems that bypass human cognitive limitations. Others believe that humans will evolve or directly modify their biology so as to achieve radically greater intelligence.[25][26] A number of futures studies focus on scenarios that combine these possibilities, suggesting that humans are likely to interface with computers, or upload their minds to computers, in a way that enables substantial intelligence amplification. The book The Age of Em by Robin Hanson describes a hypothetical future scenario in which human brains are scanned and digitized, creating "uploads" or digital versions of human consciousness. In this future, the development of these uploads may precede or coincide with the emergence of superintelligent artificial intelligence.[27] Variations[edit] Non-AI singularity[edit] Some writers use "the singularity" in a broader way to refer to any radical changes in society brought about by new technology (such as molecular nanotechnology),[28][29][30] although Vinge and other writers specifically state that without superintelligence, such changes would not qualify as a true singularity.[4] Predictions[edit] In 1965, I. J. Good wrote that it is more probable than not that an ultra-intelligent machine would be built in the twentieth century.[18] In 1993, Vinge predicted greater-than-human intelligence between 2005 and 2030.[4] In 1996, Yudkowsky predicted a singularity in 2021.[20] In 2005, Kurzweil predicted human-level AI around 2029,[31] and the singularity in 2045.[32] In a 2017 interview, Kurzweil reaffirmed his estimates.[33] In 1988, Moravec predicted that if the rate of improvement continues, the computing capabilities for human-level AI would be available in supercomputers before 2010.[34] In 1998, Moravec predicted human-level AI by 2040, and intelligence far beyond human by 2050.[35] Four polls of AI researchers, conducted in 2012 and 2013 by Nick Bostrom and Vincent C. Müller, suggested a confidence of 50% that human-level AI would be developed by 2040–2050.[36][37] Plausibility[edit] Prominent technologists and academics dispute the plausibility of a technological singularity, including Paul Allen,[11] Jeff Hawkins,[12] John Holland, Jaron Lanier, Steven Pinker,[12] Theodore Modis,[13] and Gordon Moore,[12] whose law is often cited in support of the concept.[38] Most proposed methods for creating superhuman or transhuman minds fall into one of two categories: intelligence amplification of human brains and artificial intelligence. The many speculated ways to augment human intelligence include bioengineering, genetic engineering, nootropic drugs, AI assistants, direct brain–computer interfaces and mind uploading. These multiple possible paths to an intelligence explosion, all of which will presumably be pursued, makes a singularity more likely.[23] Robin Hanson expressed skepticism of human intelligence augmentation, writing that once the "low-hanging fruit" of easy methods for increasing human intelligence have been exhausted, further improvements will become increasingly difficult.[39] Despite all of the speculated ways for amplifying human intelligence, non-human artificial intelligence (specifically seed AI) is the most popular option among the hypotheses that would advance the singularity.[citation needed] The possibility of an intelligence explosion depends on three factors.[40] The first accelerating factor is the new intelligence enhancements made possible by each previous improvement. Contrariwise, as the intelligences become more advanced, further advances will become more and more complicated, possibly outweighing the advantage of increased intelligence. Each improvement should generate at least one more improvement, on average, for movement towards singularity to continue. Finally, the laws of physics may eventually prevent further improvement. There are two logically independent, but mutually reinforcing, causes of intelligence improvements: increases in the speed of computation, and improvements to the algorithms used.[7] The former is predicted by Moore's Law and the forecasted improvements in hardware,[41] and is comparatively similar to previous technological advances. But Schulman and Sandberg[42] argue that software will present more complex challenges than simply operating on hardware capable of running at human intelligence levels or beyond. A 2017 email survey of authors with publications at the 2015 NeurIPS and ICML machine learning conferences asked about the chance that "the intelligence explosion argument is broadly correct". Of the respondents, 12% said it was "quite likely", 17% said it was "likely", 21% said it was "about even", 24% said it was "unlikely" and 26% said it was "quite unlikely".[43] Speed improvements[edit] Both for human and artificial intelligence, hardware improvements increase the rate of future hardware improvements. An analogy to Moore's Law suggests that if the first doubling of speed took 18 months, the second would take 18 subjective months; or 9 external months, whereafter, four months, two months, and so on towards a speed singularity.[44][20] Some upper limit on speed may eventually be reached. Jeff Hawkins has stated that a self-improving computer system would inevitably run into upper limits on computing power: "in the end there are limits to how big and fast computers can run. We would end up in the same place; we'd just get there a bit faster. There would be no singularity."[12] It is difficult to directly compare silicon-based hardware with neurons. But Berglas (2008) notes that computer speech recognition is approaching human capabilities, and that this capability seems to require 0.01% of the volume of the brain. This analogy suggests that modern computer hardware is within a few orders of magnitude of being as powerful as the human brain. Exponential growth[edit] Ray Kurzweil writes that, due to paradigm shifts, a trend of exponential growth extends Moore's law from integrated circuits to earlier transistors, vacuum tubes, relays, and electromechanical computers. He predicts that the exponential growth will continue, and that in a few decades the computing power of all computers will exceed that of ("unenhanced") human brains, with superhuman artificial intelligence appearing around the same time. An updated version of Moore's law over 120 Years (based on Kurzweil's graph). The 7 most recent data points are all Nvidia GPUs. The exponential growth in computing technology suggested by Moore's law is commonly cited as a reason to expect a singularity in the relatively near future, and a number of authors have proposed generalizations of Moore's law. Computer scientist and futurist Hans Moravec proposed in a 1998 book[45] that the exponential growth curve could be extended back through earlier computing technologies prior to the integrated circuit. Ray Kurzweil postulates a law of accelerating returns in which the speed of technological change (and more generally, all evolutionary processes)[46] increases exponentially, generalizing Moore's law in the same manner as Moravec's proposal, and also including material technology (especially as applied to nanotechnology), medical technology and others.[47] Between 1986 and 2007, machines' application-specific capacity to compute information per capita roughly doubled every 14 months; the per capita capacity of the world's general-purpose computers has doubled every 18 months; the global telecommunication capacity per capita doubled every 34 months; and the world's storage capacity per capita doubled every 40 months.[48] On the other hand, it has been argued that the global acceleration pattern having the 21st century singularity as its parameter should be characterized as hyperbolic rather than exponential.[49] Kurzweil reserves the term "singularity" for a rapid increase in artificial intelligence (as opposed to other technologies), writing for example that "The Singularity will allow us to transcend these limitations of our biological bodies and brains ... There will be no distinction, post-Singularity, between human and machine".[50] He also defines his predicted date of the singularity (2045) in terms of when he expects computer-based intelligences to significantly exceed the sum total of human brainpower, writing that advances in computing before that date "will not represent the Singularity" because they do "not yet correspond to a profound expansion of our intelligence."[51] Accelerating change[edit] According to Kurzweil, his logarithmic graph of 15 lists of paradigm shifts for key historic events shows an exponential trend. Main article: Accelerating change Some singularity proponents argue its inevitability through extrapolation of past trends, especially those pertaining to shortening gaps between improvements to technology. In one of the first uses of the term "singularity" in the context of technological progress, Stanislaw Ulam tells of a conversation with John von Neumann about accelerating change: One conversation centered on the ever accelerating progress of technology and changes in the mode of human life, which gives the appearance of approaching some essential singularity in the history of the race beyond which human affairs, as we know them, could not continue.[6] Kurzweil claims that technological progress follows a pattern of exponential growth, following what he calls the "law of accelerating returns". Whenever technology approaches a barrier, Kurzweil writes, new technologies will surmount it. He predicts paradigm shifts will become increasingly common, leading to "technological change so rapid and profound it represents a rupture in the fabric of human history".[52] Kurzweil believes that the singularity will occur by approximately 2045.[47] His predictions differ from Vinge's in that he predicts a gradual ascent to the singularity, rather than Vinge's rapidly self-improving superhuman intelligence. Oft-cited dangers include those commonly associated with molecular nanotechnology and genetic engineering. These threats are major issues for both singularity advocates and critics, and were the subject of Bill Joy's April 2000 Wired magazine article "Why The Future Doesn't Need Us".[7][53] Algorithm improvements[edit] Some intelligence technologies, like "seed AI",[16][17] may also have the potential to not just make themselves faster, but also more efficient, by modifying their source code. These improvements would make further improvements possible, which would make further improvements possible, and so on. The mechanism for a recursively self-improving set of algorithms differs from an increase in raw computation speed in two ways. First, it does not require external influence: machines designing faster hardware would still require humans to create the improved hardware, or to program factories appropriately.[citation needed] An AI rewriting its own source code could do so while contained in an AI box. Second, as with Vernor Vinge's conception of the singularity, it is much harder to predict the outcome. While speed increases seem to be only a quantitative difference from human intelligence, actual algorithm improvements would be qualitatively different. Eliezer Yudkowsky compares it to the changes that human intelligence brought: humans changed the world thousands of times more rapidly than evolution had done, and in totally different ways. Similarly, the evolution of life was a massive departure and acceleration from the previous geological rates of change, and improved intelligence could cause change to be as different again.[54] There are substantial dangers associated with an intelligence explosion singularity originating from a recursively self-improving set of algorithms. First, the goal structure of the AI might self-modify, potentially causing the AI to optimise for something other than what was originally intended.[55][56] Secondly, AIs could compete for the same scarce resources humankind uses to survive.[57][58] While not actively malicious, AIs would promote the goals of their programming, not necessarily broader human goals, and thus might crowd out humans.[59][60][61] Carl Shulman and Anders Sandberg suggest that algorithm improvements may be the limiting factor for a singularity; while hardware efficiency tends to improve at a steady pace, software innovations are more unpredictable and may be bottlenecked by serial, cumulative research. They suggest that in the case of a software-limited singularity, intelligence explosion would actually become more likely than with a hardware-limited singularity, because in the software-limited case, once human-level AI is developed, it could run serially on very fast hardware, and the abundance of cheap hardware would make AI research less constrained.[62] An abundance of accumulated hardware that can be unleashed once the software figures out how to use it has been called "computing overhang".[63] Criticism[edit] Some critics, like philosopher Hubert Dreyfus[64] and philosopher John Searle,[65] assert that computers or machines cannot achieve human intelligence. Others, like physicist Stephen Hawking,[66] object that whether machines can achieve a true intelligence or merely something similar to intelligence is irrelevant if the net result is the same. Psychologist Steven Pinker stated in 2008: "There is not the slightest reason to believe in a coming singularity. The fact that you can visualize a future in your imagination is not evidence that it is likely or even possible. Look at domed cities, jet-pack commuting, underwater cities, mile-high buildings, and nuclear-powered automobiles—all staples of futuristic fantasies when I was a child that have never arrived. Sheer processing power is not a pixie dust that magically solves all your problems."[12] Martin Ford[67] postulates a "technology paradox" in that before the singularity could occur most routine jobs in the economy would be automated, since this would require a level of technology inferior to that of the singularity. This would cause massive unemployment and plummeting consumer demand, which in turn would destroy the incentive to invest in the technologies that would be required to bring about the Singularity. Job displacement is increasingly no longer limited to those types of work traditionally considered to be "routine".[68] Theodore Modis[69] and Jonathan Huebner[70] argue that the rate of technological innovation has not only ceased to rise, but is actually now declining. Evidence for this decline is that the rise in computer clock rates is slowing, even while Moore's prediction of exponentially increasing circuit density continues to hold. This is due to excessive heat build-up from the chip, which cannot be dissipated quickly enough to prevent the chip from melting when operating at higher speeds. Advances in speed may be possible in the future by virtue of more power-efficient CPU designs and multi-cell processors.[71] Theodore Modis holds the singularity cannot happen.[72][13][73] He claims the "technological singularity" and especially Kurzweil lack scientific rigor; Kurzweil is alleged to mistake the logistic function (S-function) for an exponential function, and to see a "knee" in an exponential function where there can in fact be no such thing.[74] In a 2021 article, Modis pointed out that no milestones – breaks in historical perspective comparable in importance to the Internet, DNA, the transistor, or nuclear energy – had been observed in the previous twenty years while five of them would have been expected according to the exponential trend advocated by the proponents of the technological singularity.[75] AI researcher Jürgen Schmidhuber stated that the frequency of subjectively "notable events" appears to be approaching a 21st-century singularity, but cautioned readers to take such plots of subjective events with a grain of salt: perhaps differences in memory of recent and distant events could create an illusion of accelerating change where none exists.[76] Microsoft co-founder Paul Allen argued the opposite of accelerating returns, the complexity brake;[11] the more progress science makes towards understanding intelligence, the more difficult it becomes to make additional progress. A study of the number of patents shows that human creativity does not show accelerating returns, but in fact, as suggested by Joseph Tainter in his The Collapse of Complex Societies,[77] a law of diminishing returns. The number of patents per thousand peaked in the period from 1850 to 1900, and has been declining since.[70] The growth of complexity eventually becomes self-limiting, and leads to a widespread "general systems collapse". Hofstadter (2006) raises concern that Ray Kurzweil is not sufficiently scientifically rigorous, that an exponential tendency of technology is not a scientific law like one of physics, and that exponential curves have no "knees".[78] Nonetheless, he did not rule out the singularity in principle in the distant future[12] and in the light of ChatGPT and other recent advancements has revised his opinion significantly towards dramatic technological change in the near future.[79] Jaron Lanier denies that the singularity is inevitable: "I do not think the technology is creating itself. It's not an autonomous process."[80] Furthermore: "The reason to believe in human agency over technological determinism is that you can then have an economy where people earn their own way and invent their own lives. If you structure a society on not emphasizing individual human agency, it's the same thing operationally as denying people clout, dignity, and self-determination ... to embrace [the idea of the Singularity] would be a celebration of bad data and bad politics."[80] Economist Robert J. Gordon points out that measured economic growth slowed around 1970 and slowed even further since the financial crisis of 2007–2008, and argues that the economic data show no trace of a coming Singularity as imagined by mathematician I. J. Good.[81] Philosopher and cognitive scientist Daniel Dennett said in 2017: "The whole singularity stuff, that's preposterous. It distracts us from much more pressing problems", adding "AI tools that we become hyper-dependent on, that is going to happen. And one of the dangers is that we will give them more authority than they warrant."[82] In addition to general criticisms of the singularity concept, several critics have raised issues with Kurzweil's iconic chart. One line of criticism is that a log-log chart of this nature is inherently biased toward a straight-line result. Others identify selection bias in the points that Kurzweil chooses to use. For example, biologist PZ Myers points out that many of the early evolutionary "events" were picked arbitrarily.[83] Kurzweil has rebutted this by charting evolutionary events from 15 neutral sources, and showing that they fit a straight line on a log-log chart. Kelly (2006) argues that the way the Kurzweil chart is constructed with x-axis having time before present, it always points to the singularity being "now", for any date on which one would construct such a chart, and shows this visually on Kurzweil's chart.[84] Some critics suggest religious motivations or implications of singularity, especially Kurzweil's version of it. The buildup towards the Singularity is compared with Judeo-Christian end-of-time scenarios. Beam calls it "a Buck Rogers vision of the hypothetical Christian Rapture".[85] John Gray says "the Singularity echoes apocalyptic myths in which history is about to be interrupted by a world-transforming event".[86] David Streitfeld in The New York Times questioned whether "it might manifest first and foremost—thanks, in part, to the bottom-line obsession of today’s Silicon Valley—as a tool to slash corporate America’s head count."[87] Potential impacts[edit] Dramatic changes in the rate of economic growth have occurred in the past because of technological advancement. Based on population growth, the economy doubled every 250,000 years from the Paleolithic era until the Neolithic Revolution. The new agricultural economy doubled every 900 years, a remarkable increase. In the current era, beginning with the Industrial Revolution, the world's economic output doubles every fifteen years, sixty times faster than during the agricultural era. If the rise of superhuman intelligence causes a similar revolution, argues Robin Hanson, one would expect the economy to double at least quarterly and possibly on a weekly basis.[88] Uncertainty and risk[edit] Further information: Existential risk from artificial general intelligence The term "technological singularity" reflects the idea that such change may happen suddenly, and that it is difficult to predict how the resulting new world would operate.[89][90] It is unclear whether an intelligence explosion resulting in a singularity would be beneficial or harmful, or even an existential threat.[91][92] Because AI is a major factor in singularity risk, a number of organizations pursue a technical theory of aligning AI goal-systems with human values, including the Future of Humanity Institute, the Machine Intelligence Research Institute,[89] the Center for Human-Compatible Artificial Intelligence, and the Future of Life Institute. Physicist Stephen Hawking said in 2014 that "Success in creating AI would be the biggest event in human history. Unfortunately, it might also be the last, unless we learn how to avoid the risks."[93] Hawking believed that in the coming decades, AI could offer "incalculable benefits and risks" such as "technology outsmarting financial markets, out-inventing human researchers, out-manipulating human leaders, and developing weapons we cannot even understand."[93] Hawking suggested that artificial intelligence should be taken more seriously and that more should be done to prepare for the singularity:[93]So, facing possible futures of incalculable benefits and risks, the experts are surely doing everything possible to ensure the best outcome, right? Wrong. If a superior alien civilisation sent us a message saying, "We'll arrive in a few decades," would we just reply, "OK, call us when you get here – we'll leave the lights on"? Probably not – but this is more or less what is happening with AI. Berglas (2008) claims that there is no direct evolutionary motivation for an AI to be friendly to humans. Evolution has no inherent tendency to produce outcomes valued by humans, and there is little reason to expect an arbitrary optimisation process to promote an outcome desired by humankind, rather than inadvertently leading to an AI behaving in a way not intended by its creators.[94][95][96] Anders Sandberg has also elaborated on this scenario, addressing various common counter-arguments.[97] AI researcher Hugo de Garis suggests that artificial intelligences may simply eliminate the human race for access to scarce resources,[57][98] and humans would be powerless to stop them.[99] Alternatively, AIs developed under evolutionary pressure to promote their own survival could outcompete humanity.[61] Bostrom (2002) discusses human extinction scenarios, and lists superintelligence as a possible cause: When we create the first superintelligent entity, we might make a mistake and give it goals that lead it to annihilate humankind, assuming its enormous intellectual advantage gives it the power to do so. For example, we could mistakenly elevate a subgoal to the status of a supergoal. We tell it to solve a mathematical problem, and it complies by turning all the matter in the solar system into a giant calculating device, in the process killing the person who asked the question. According to Eliezer Yudkowsky, a significant problem in AI safety is that unfriendly artificial intelligence is likely to be much easier to create than friendly AI. While both require large advances in recursive optimisation process design, friendly AI also requires the ability to make goal structures invariant under self-improvement (or the AI could transform itself into something unfriendly) and a goal structure that aligns with human values and does not automatically destroy the human race. An unfriendly AI, on the other hand, can optimize for an arbitrary goal structure, which does not need to be invariant under self-modification.[100] Bill Hibbard (2014) harvtxt error: no target: CITEREFBill_Hibbard2014 (help) proposes an AI design that avoids several dangers including self-delusion,[101] unintended instrumental actions,[55][102] and corruption of the reward generator.[102] He also discusses social impacts of AI[103] and testing AI.[104] His 2001 book Super-Intelligent Machines advocates the need for public education about AI and public control over AI. It also proposed a simple design that was vulnerable to corruption of the reward generator. Next step of sociobiological evolution[edit] Further information: Sociocultural evolution This section may contain material not related to the topic of the article. Please help improve this section or discuss this issue on the talk page. (October 2021) (Learn how and when to remove this template message) Schematic Timeline of Information and Replicators in the Biosphere: Gillings et al.'s "major evolutionary transitions" in information processing.[105] Amount of digital information worldwide (5×1021 bytes) versus human genome information worldwide (1019 bytes) in 2014[105] While the technological singularity is usually seen as a sudden event, some scholars argue the current speed of change already fits this description.[citation needed] In addition, some argue that we are already in the midst of a major evolutionary transition that merges technology, biology, and society. Digital technology has infiltrated the fabric of human society to a degree of indisputable and often life-sustaining dependence. A 2016 article in Trends in Ecology & Evolution argues that "humans already embrace fusions of biology and technology. We spend most of our waking time communicating through digitally mediated channels... we trust artificial intelligence with our lives through antilock braking in cars and autopilots in planes... With one in three courtships leading to marriages in America beginning online, digital algorithms are also taking a role in human pair bonding and reproduction". The article further argues that from the perspective of the evolution, several previous Major Transitions in Evolution have transformed life through innovations in information storage and replication (RNA, DNA, multicellularity, and culture and language). In the current stage of life's evolution, the carbon-based biosphere has generated a cognitive system (humans) capable of creating technology that will result in a comparable evolutionary transition. The digital information created by humans has reached a similar magnitude to biological information in the biosphere. Since the 1980s, the quantity of digital information stored has doubled about every 2.5 years, reaching about 5 zettabytes in 2014 (5×1021 bytes).[106] In biological terms, there are 7.2 billion humans on the planet, each having a genome of 6.2 billion nucleotides. Since one byte can encode four nucleotide pairs, the individual genomes of every human on the planet could be encoded by approximately 1×1019 bytes. The digital realm stored 500 times more information than this in 2014 (see figure). The total amount of DNA contained in all of the cells on Earth is estimated to be about 5.3×1037 base pairs, equivalent to 1.325×1037 bytes of information. If growth in digital storage continues at its current rate of 30–38% compound annual growth per year,[48] it will rival the total information content contained in all of the DNA in all of the cells on Earth in about 110 years. This would represent a doubling of the amount of information stored in the biosphere across a total time period of just 150 years".[105] Implications for human society[edit] Further information: Artificial intelligence in fiction In February 2009, under the auspices of the Association for the Advancement of Artificial Intelligence (AAAI), Eric Horvitz chaired a meeting of leading computer scientists, artificial intelligence researchers and roboticists at the Asilomar conference center in Pacific Grove, California. The goal was to discuss the potential impact of the hypothetical possibility that robots could become self-sufficient and able to make their own decisions. They discussed the extent to which computers and robots might be able to acquire autonomy, and to what degree they could use such abilities to pose threats or hazards.[107] Some machines are programmed with various forms of semi-autonomy, including the ability to locate their own power sources and choose targets to attack with weapons. Also, some computer viruses can evade elimination and, according to scientists in attendance, could therefore be said to have reached a "cockroach" stage of machine intelligence. The conference attendees noted that self-awareness as depicted in science-fiction is probably unlikely, but that other potential hazards and pitfalls exist.[107] Frank S. Robinson predicts that once humans achieve a machine with the intelligence of a human, scientific and technological problems will be tackled and solved with brainpower far superior to that of humans. He notes that artificial systems are able to share data more directly than humans, and predicts that this would result in a global network of super-intelligence that would dwarf human capability.[108] Robinson also discusses how vastly different the future would potentially look after such an intelligence explosion. Hard vs. soft takeoff[edit] In this sample recursive self-improvement scenario, humans modifying an AI's architecture would be able to double its performance every three years through, for example, 30 generations before exhausting all feasible improvements (left). If instead the AI is smart enough to modify its own architecture as well as human researchers can, its time required to complete a redesign halves with each generation, and it progresses all 30 feasible generations in six years (right).[109] In a hard takeoff scenario, an artificial superintelligence rapidly self-improves, "taking control" of the world (perhaps in a matter of hours), too quickly for significant human-initiated error correction or for a gradual tuning of the agent's goals. In a soft takeoff scenario, the AI still becomes far more powerful than humanity, but at a human-like pace (perhaps on the order of decades), on a timescale where ongoing human interaction and correction can effectively steer the AI's development.[110][111] Ramez Naam argues against a hard takeoff. He has pointed out that we already see recursive self-improvement by superintelligences, such as corporations. Intel, for example, has "the collective brainpower of tens of thousands of humans and probably millions of CPU cores to... design better CPUs!" However, this has not led to a hard takeoff; rather, it has led to a soft takeoff in the form of Moore's law.[112] Naam further points out that the computational complexity of higher intelligence may be much greater than linear, such that "creating a mind of intelligence 2 is probably more than twice as hard as creating a mind of intelligence 1."[113] J. Storrs Hall believes that "many of the more commonly seen scenarios for overnight hard takeoff are circular – they seem to assume hyperhuman capabilities at the starting point of the self-improvement process" in order for an AI to be able to make the dramatic, domain-general improvements required for takeoff. Hall suggests that rather than recursively self-improving its hardware, software, and infrastructure all on its own, a fledgling AI would be better off specializing in one area where it was most effective and then buying the remaining components on the marketplace, because the quality of products on the marketplace continually improves, and the AI would have a hard time keeping up with the cutting-edge technology used by the rest of the world.[114] Ben Goertzel agrees with Hall's suggestion that a new human-level AI would do well to use its intelligence to accumulate wealth. The AI's talents might inspire companies and governments to disperse its software throughout society. Goertzel is skeptical of a hard five minute takeoff but speculates that a takeoff from human to superhuman level on the order of five years is reasonable. Goerzel refers to this scenario as a "semihard takeoff".[115] Max More disagrees, arguing that if there were only a few superfast human-level AIs, that they would not radically change the world, as they would still depend on other people to get things done and would still have human cognitive constraints. Even if all superfast AIs worked on intelligence augmentation, it is unclear why they would do better in a discontinuous way than existing human cognitive scientists at producing super-human intelligence, although the rate of progress would increase. More further argues that a superintelligence would not transform the world overnight: a superintelligence would need to engage with existing, slow human systems to accomplish physical impacts on the world. "The need for collaboration, for organization, and for putting ideas into physical changes will ensure that all the old rules are not thrown out overnight or even within years."[116] Relation to immortality and aging[edit] Drexler (1986), one of the founders of nanotechnology, postulates cell repair devices, including ones operating within cells and using as yet hypothetical biological machines.[117] According to Richard Feynman, it was his former graduate student and collaborator Albert Hibbs who originally suggested to him (circa 1959) the idea of a medical use for Feynman's theoretical micromachines. Hibbs suggested that certain repair machines might one day be reduced in size to the point that it would, in theory, be possible to (as Feynman put it) "swallow the doctor". The idea was incorporated into Feynman's 1959 essay There's Plenty of Room at the Bottom.[118] Moravec (1988)[34] predicts the possibility of "uploading" human mind into a human-like robot, achieving quasi-immortality by extreme longevity via transfer of the human mind between successive new robots as the old ones wear out; beyond that, he predicts later exponential acceleration of subjective experience of time leading to a subjective sense of immortality. Kurzweil (2005) suggests that medical advances would allow people to protect their bodies from the effects of aging, making the life expectancy limitless. Kurzweil argues that the technological advances in medicine would allow us to continuously repair and replace defective components in our bodies, prolonging life to an undetermined age.[119] Kurzweil further buttresses his argument by discussing current bio-engineering advances. Kurzweil suggests somatic gene therapy; after synthetic viruses with specific genetic information, the next step would be to apply this technology to gene therapy, replacing human DNA with synthesized genes.[120] Beyond merely extending the operational life of the physical body, Jaron Lanier argues for a form of immortality called "Digital Ascension" that involves "people dying in the flesh and being uploaded into a computer and remaining conscious."[121] History of the concept[edit] A paper by Mahendra Prasad, published in AI Magazine, asserts that the 18th-century mathematician Marquis de Condorcet was the first person to hypothesize and mathematically model an intelligence explosion and its effects on humanity.[122] An early description of the idea was made in John W. Campbell's 1932 short story "The Last Evolution".[123] In his 1958 obituary for John von Neumann, Ulam recalled a conversation with von Neumann about the "ever accelerating progress of technology and changes in the mode of human life, which gives the appearance of approaching some essential singularity in the history of the race beyond which human affairs, as we know them, could not continue."[6] In 1965, Good wrote his essay postulating an "intelligence explosion" of recursive self-improvement of a machine intelligence.[18][19] In 1977, Hans Moravec wrote an article with unclear publishing status where he envisioned a development of self-improving thinking machines, a creation of "super-consciousness, the synthesis of terrestrial life, and perhaps jovian and martian life as well, constantly improving and extending itself, spreading outwards from the solar system, converting non-life into mind."[124][125] The article describes the human mind uploading later covered in Moravec (1988). The machines are expected to reach human level and then improve themselves beyond that ("Most significantly of all, they [the machines] can be put to work as programmers and engineers, with the task of optimizing the software and hardware which make them what they are. The successive generations of machines produced this way will be increasingly smarter and more cost effective.") Humans will no longer be needed, and their abilities will be overtaken by the machines: "In the long run the sheer physical inability of humans to keep up with these rapidly evolving progeny of our minds will ensure that the ratio of people to machines approaches zero, and that a direct descendant of our culture, but not our genes, inherits the universe." While the word "singularity" is not used, the notion of human-level thinking machines thereafter improving themselves beyond human level is there. In this view, there is no intelligence explosion in the sense of a very rapid intelligence increase once human equivalence is reached. An updated version of the article was published in 1979 in Analog Science Fiction and Fact.[126][125] In 1981, Stanisław Lem published his science fiction novel Golem XIV. It describes a military AI computer (Golem XIV) who obtains consciousness and starts to increase his own intelligence, moving towards personal technological singularity. Golem XIV was originally created to aid its builders in fighting wars, but as its intelligence advances to a much higher level than that of humans, it stops being interested in the military requirements because it finds them lacking internal logical consistency. In 1983, Vernor Vinge addressed Good's intelligence explosion in print in the January 1983 issue of Omni magazine. In this op-ed piece, Vinge seems to have been the first to use the term "singularity" (although not "technological singularity") in a way that was specifically tied to the creation of intelligent machines:[8][125] We will soon create intelligences greater than our own. When this happens, human history will have reached a kind of singularity, an intellectual transition as impenetrable as the knotted space-time at the center of a black hole, and the world will pass far beyond our understanding. This singularity, I believe, already haunts a number of science-fiction writers. It makes realistic extrapolation to an interstellar future impossible. To write a story set more than a century hence, one needs a nuclear war in between ... so that the world remains intelligible. In 1985, in "The Time Scale of Artificial Intelligence", artificial intelligence researcher Ray Solomonoff articulated mathematically the related notion of what he called an "infinity point": if a research community of human-level self-improving AIs take four years to double their own speed, then two years, then one year and so on, their capabilities increase infinitely in finite time.[7][127] In 1986, Vernor Vinge published Marooned in Realtime, a science-fiction novel where a few remaining humans traveling forward in the future have survived an unknown extinction event that might well be a singularity. In a short afterword, the author states that an actual technological singularity would not be the end of the human species: "of course it seems very unlikely that the Singularity would be a clean vanishing of the human race. (On the other hand, such a vanishing is the timelike analog of the silence we find all across the sky.)".[128][129] In 1988, Vinge used the phrase "technological singularity" (including "technological") in the short story collection Threats and Other Promises, writing in the introduction to his story "The Whirligig of Time" (p. 72): Barring a worldwide catastrophe, I believe that technology will achieve our wildest dreams, and soon. When we raise our own intelligence and that of our creations, we are no longer in a world of human-sized characters. At that point we have fallen into a technological "black hole", a technological singularity.[130] In 1988, Hans Moravec published Mind Children,[34] in which he predicted human-level intelligence in supercomputers by 2010, self-improving intelligent machines far surpassing human intelligence later, human mind uploading into human-like robots later, intelligent machines leaving humans behind, and space colonization. He did not mention "singularity", though, and he did not speak of a rapid explosion of intelligence immediately after the human level is achieved. Nonetheless, the overall singularity tenor is there in predicting both human-level artificial intelligence and further artificial intelligence far surpassing humans later. Vinge's 1993 article "The Coming Technological Singularity: How to Survive in the Post-Human Era",[4] spread widely on the internet and helped to popularize the idea.[131] This article contains the statement, "Within thirty years, we will have the technological means to create superhuman intelligence. Shortly after, the human era will be ended." Vinge argues that science-fiction authors cannot write realistic post-singularity characters who surpass the human intellect, as the thoughts of such an intellect would be beyond the ability of humans to express.[4] Minsky's 1994 article says robots will "inherit the Earth", possibly with the use of nanotechnology, and proposes to think of robots as human "mind children", drawing the analogy from Moravec. The rhetorical effect of that analogy is that if humans are fine to pass the world to their biological children, they should be equally fine to pass it to robots, their "mind" children. As per Minsky, 'we could design our "mind-children" to think a million times faster than we do. To such a being, half a minute might seem as long as one of our years, and each hour as long as an entire human lifetime.' The feature of the singularity present in Minsky is the development of superhuman artificial intelligence ("million times faster"), but there is no talk of sudden intelligence explosion, self-improving thinking machines or unpredictability beyond any specific event and the word "singularity" is not used.[132] Tipler's 1994 book The Physics of Immortality predicts a future where super–intelligent machines will build enormously powerful computers, people will be "emulated" in computers, life will reach every galaxy and people will achieve immortality when they reach Omega Point.[133] There is no talk of Vingean "singularity" or sudden intelligence explosion, but intelligence much greater than human is there, as well as immortality. In 1996, Yudkowsky predicted a singularity by 2021.[20] His version of singularity involves intelligence explosion: once AIs are doing the research to improve themselves, speed doubles after 2 years, then 1 one year, then after 6 months, then after 3 months, then after 1.5 months, and after more iterations, the "singularity" is reached.[20] This construction implies that the speed reaches infinity in finite time. In 2000, Bill Joy, a prominent technologist and a co-founder of Sun Microsystems, voiced concern over the potential dangers of robotics, genetic engineering, and nanotechnology.[53] In 2005, Kurzweil published The Singularity Is Near. Kurzweil's publicity campaign included an appearance on The Daily Show with Jon Stewart.[134] From 2006 to 2012, an annual Singularity Summit conference was organized by Machine Intelligence Research Institute, founded by Eliezer Yudkowsky. In 2007, Yudkowsky suggested that many of the varied definitions that have been assigned to "singularity" are mutually incompatible rather than mutually supporting.[29][135] For example, Kurzweil extrapolates current technological trajectories past the arrival of self-improving AI or superhuman intelligence, which Yudkowsky argues represents a tension with both I. J. Good's proposed discontinuous upswing in intelligence and Vinge's thesis on unpredictability.[29] In 2009, Kurzweil and X-Prize founder Peter Diamandis announced the establishment of Singularity University, a nonaccredited private institute whose stated mission is "to educate, inspire and empower leaders to apply exponential technologies to address humanity's grand challenges."[136] Funded by Google, Autodesk, ePlanet Ventures, and a group of technology industry leaders, Singularity University is based at NASA's Ames Research Center in Mountain View, California. The not-for-profit organization runs an annual ten-week graduate program during summer that covers ten different technology and allied tracks, and a series of executive programs throughout the year. In politics[edit] In 2007, the Joint Economic Committee of the United States Congress released a report about the future of nanotechnology. It predicts significant technological and political changes in the mid-term future, including possible technological singularity.[137][138][139] Former President of the United States Barack Obama spoke about singularity in his interview to Wired in 2016:[140] One thing that we haven't talked about too much, and I just want to go back to, is we really have to think through the economic implications. Because most people aren't spending a lot of time right now worrying about singularity—they are worrying about "Well, is my job going to be replaced by a machine?" See also[edit] Technology portal Accelerating change – Perceived increase in the rate of technological change throughout history Artificial consciousness – Field in cognitive science Artificial intelligence arms race – Arms race for the most advanced AI-related technologies Artificial intelligence in fiction – Artificial intelligence as depicted in works of fiction Brain simulation – Concept of creating a functioning computer model of a brain or part of a brain Brain–computer interface – Direct communication pathway between an enhanced or wired brain and an external device Emerging technologies – Technologies whose development, practical applications, or both are still largely unrealized Ephemeralization – Technological advancement theory Fermi paradox – Lack of evidence that aliens exist Flynn effect – 20th-century rise in intelligence test scores Futures studies – Study of postulating possible, probable, and preferable futures Global brain – Futuristic concept of a global interconnected network Hallucination (artificial intelligence) – Confident unjustified claim by AI Human intelligence § Improving Mind uploading – Hypothetical process of digitally emulating a brain Neuroenhancement – Extension of cognition in the healthy Robot learning – Machine learning for robots Singularitarianism – Belief in an incipient technological singularity Technological determinism – Reductionist theory Technological revolution – Period of rapid technological change Technological unemployment – Unemployment caused by technological change Transhumanism – Philosophical movement The Man Who Evolved – short story by Edmond HamiltonPages displaying wikidata descriptions as a fallback References[edit] Citations[edit] ^ Cadwalladr, Carole (22 February 2014). "Are the robots about to rise? Google's new director of engineering thinks so…". The Guardian. Retrieved 8 May 2022. ^ "Collection of sources defining "singularity"". singularitysymposium.com. Archived from the original on 17 April 2019. Retrieved 17 April 2019. ^ a b Eden, Amnon H.; Moor, James H.; Søraker, Johnny H.; Steinhart, Eric, eds. (2012). Singularity Hypotheses: A Scientific and Philosophical Assessment. The Frontiers Collection. Dordrecht: Springer. pp. 1–2. doi:10.1007/978-3-642-32560-1. ISBN 9783642325601. ^ a b c d e f g h Vinge, Vernor. "The Coming Technological Singularity: How to Survive in the Post-Human Era" Archived 2018-04-10 at the Wayback Machine, in Vision-21: Interdisciplinary Science and Engineering in the Era of Cyberspace, G. A. Landis, ed., NASA Publication CP-10129, pp. 11–22, 1993. ^ The Technological Singularity by Murray Shanahan, (Massachusetts Institute of Technology Press, 2015), p. 233. ^ a b c Ulam, Stanislaw (May 1958). "Tribute to John von Neumann" (PDF). Bulletin of the American Mathematical Society. 64, #3, part 2: 5. Archived (PDF) from the original on 15 February 2021. Retrieved 7 November 2018. ^ a b c d e f Chalmers, David J. (2010). "The Singularity: A Philosophical Analysis" (PDF). Journal of Consciousness Studies. 17 (9–10): 7–65. ^ a b Dooling, Richard. Rapture for the Geeks: When AI Outsmarts IQ (2008), p. 88 ^ Sparkes, Matthew (13 January 2015). "Top scientists call for caution over artificial intelligence". The Telegraph (UK). Archived from the original on 7 April 2015. Retrieved 24 April 2015. ^ "Hawking: AI could end human race". BBC. 2 December 2014. Archived from the original on 30 October 2015. Retrieved 11 November 2017. ^ a b c Allen, Paul G.; Greaves, Mark (12 October 2011), "Paul Allen: The Singularity Isn't Near", MIT Technology Review, retrieved 12 April 2015 ^ a b c d e f g h i "Tech Luminaries Address Singularity". IEEE Spectrum. 1 June 2008. Archived from the original on 30 April 2019. Retrieved 9 September 2011. ^ a b c Modis, Theodore (2012). “Why the Singularity Cannot Happen”. Published in Eden, Amnon H. et al (Eds.) (2012). Singularity Hypothesis (PDF). New York: Springer. p. 311. ISBN 978-3-642-32560-1. pp. 311–339. ^ Ehrlich, Paul. "Paul Ehrlich: The Dominant Animal: Human Evolution and the Environment – The Long Now". longnow.org. Retrieved 14 June 2023. ^ "Businessweek – Bloomberg". Bloomberg.com. 20 April 2023. Retrieved 14 June 2023. ^ a b Yampolskiy, Roman V. "Analysis of types of self-improving software." Artificial General Intelligence. Springer International Publishing, 2015. pp. 384–393. ^ a b Eliezer Yudkowsky. General Intelligence and Seed AI-Creating Complete Minds Capable of Open-Ended Self-Improvement, 2001. ^ a b c Good, I. J. (1965), Speculations Concerning the First Ultraintelligent Machine, archived from the original on 27 May 2001 ^ a b Good, I. J. (1965), Speculations Concerning the First Ultraintelligent Machine (PDF), archived from the original (PDF) on 1 May 2012 ^ a b c d e Yudkowsky, Eliezer (1996). "Staring into the Singularity". Archived from the original on 21 October 2008. ^ Ray Kurzweil, The Singularity Is Near, pp. 135–136. Penguin Group, 2005. ^ Sotala, Kaj; Yampolskiy, Roman (2017). "Risks of the Journey to the Singularity". The Technological Singularity. The Frontiers Collection. Berlin and Heidelberg, Germany: Springer Berlin Heidelberg. pp. 11–23. doi:10.1007/978-3-662-54033-6_2. ISBN 978-3-662-54031-2. ^ a b "What is the Singularity? | Singularity Institute for Artificial Intelligence". Singinst.org. Archived from the original on 8 September 2011. Retrieved 9 September 2011. ^ Chalmers, David J. (2016). "The Singularity". Science Fiction and Philosophy. John Wiley & Sons, Inc. pp. 171–224. doi:10.1002/9781118922590.ch16. ISBN 9781118922590. ^ Pearce, David (2012), Eden, Amnon H.; Moor, James H.; Søraker, Johnny H.; Steinhart, Eric (eds.), "The Biointelligence Explosion", Singularity Hypotheses, The Frontiers Collection, Berlin and Heidelberg, Germany: Springer Berlin Heidelberg, pp. 199–238, doi:10.1007/978-3-642-32560-1_11, ISBN 978-3-642-32559-5, retrieved 16 January 2022 ^ Gouveia, Steven S., ed. (2020). "ch. 4, "Humans and Intelligent Machines: Co-evolution, Fusion or Replacement?", David Pearce". The Age of Artificial Intelligence: An Exploration. Vernon Press. ISBN 978-1-62273-872-4. ^ Hanson, Robin (2016). The Age of Em. Oxford, England: Oxford University Press. p. 528. ISBN 9780198754626. ^ Hall, Josh (2010). "Singularity: Nanotech or AI?". Hplusmagazine.com. Archived from the original on 23 December 2010. Retrieved 9 September 2011. ^ a b c Yudkowsky, Eliezer (2007), The Singularity: Three Major Schools, archived from the original on 1 October 2018 ^ Sandberg, Anders. An overview of models of technological singularity Archived 2011-07-24 at the Wayback Machine ^ "List of Analyses of Time to Human-Level AI". AI Impacts. 22 January 2015. Retrieved 14 June 2023. ^ Kurzweil, Ray (2005). The Singularity Is Near. Penguin Group. p. 120. ^ Reedy, Christianna (2017). "Kurzweil Claims That the Singularity Will Happen by 2045". Futurism. Retrieved 14 June 2023. ^ a b c Hans Moravec, Mind Children, 1988 ^ Moravec, Hans P. (1998). Robot: Mere Machine to Transcendent Mind. Oxford University Press USA. ^ Khatchadourian, Raffi (16 November 2015). "The Doomsday Invention". The New Yorker. Archived from the original on 29 April 2019. Retrieved 31 January 2018. ^ Müller, V. C., & Bostrom, N. (2016). "Future progress in artificial intelligence: A survey of expert opinion". In V. C. Müller (ed): Fundamental issues of artificial intelligence (pp. 555–572). Berlin, Germany: Springer Berlin. http://philpapers.org/rec/MLLFPI Archived 2019-03-02 at the Wayback Machine. ^ "Who's Who In The Singularity". IEEE Spectrum. 1 June 2008. Archived from the original on 12 March 2016. Retrieved 9 September 2011. ^ Hanson, Robin (1998). "Some Skepticism". Archived from the original on 15 February 2021. Retrieved 8 April 2020. ^ David Chalmers John Locke Lecture, 10 May 2009, Exam Schools, Oxford, Presenting a philosophical analysis of the possibility of a technological singularity or "intelligence explosion" resulting from recursively self-improving AI Archived 2013-01-15 at the Wayback Machine. ^ "ITRS" (PDF). Archived from the original (PDF) on 29 September 2011. Retrieved 9 September 2011. ^ Shulman, Carl; Anders, Sandberg (2010). "Implications of a Software-Limited Singularity" (PDF). Machine Intelligence Research Institute. ^ Grace, Katja; Salvatier, John; Dafoe, Allan; Zhang, Baobao; Evans, Owain (24 May 2017). "When Will AI Exceed Human Performance? Evidence from AI Experts". arXiv:1705.08807 [cs.AI]. ^ Siracusa, John (31 August 2009). "Mac OS X 10.6 Snow Leopard: the Ars Technica review". Ars Technica. Archived from the original on 3 September 2011. Retrieved 9 September 2011. ^ Moravec, Hans (1999). Robot: Mere Machine to Transcendent Mind. Oxford University Press. p. 61. ISBN 978-0-19-513630-2. ^ Ray Kurzweil, The Age of Spiritual Machines, Viking; 1999, ISBN 978-0-14-028202-3. pp. 30, 32 Archived 2021-02-15 at the Wayback Machine ^ a b Ray Kurzweil, The Singularity Is Near, Penguin Group, 2005 ^ a b "The World's Technological Capacity to Store, Communicate, and Compute Information" Archived 2013-07-27 at the Wayback Machine, Martin Hilbert and Priscila López (2011), Science, 332 (6025), pp. 60–65; free access to the article through: martinhilbert.net/WorldInfoCapacity.html. ^ Korotayev, Andrey V.; LePoire, David J., eds. (2020). "The 21st Century Singularity and Global Futures". World-Systems Evolution and Global Futures. doi:10.1007/978-3-030-33730-8. ISBN 978-3-030-33729-2. ISSN 2522-0985. S2CID 241407141. ^ Ray Kurzweil, The Singularity Is Near, p. 9. Penguin Group, 2005 ^ Ray Kurzweil, The Singularity Is Near, pp. 135–136. Penguin Group, 2005. "So we will be producing about 1026 to 1029 cps of nonbiological computation per year in the early 2030s. This is roughly equal to our estimate for the capacity of all living biological human intelligence ... This state of computation in the early 2030s will not represent the Singularity, however, because it does not yet correspond to a profound expansion of our intelligence. By the mid-2040s, however, that one thousand dollars' worth of computation will be equal to 1026 cps, so the intelligence created per year (at a total cost of about $1012) will be about one billion times more powerful than all human intelligence today. That will indeed represent a profound change, and it is for that reason that I set the date for the Singularity—representing a profound and disruptive transformation in human capability—as 2045." ^ Kurzweil, Raymond (2001), "The Law of Accelerating Returns", Nature Physics, Lifeboat Foundation, 4 (7): 507, Bibcode:2008NatPh...4..507B, doi:10.1038/nphys1010, archived from the original on 27 August 2018, retrieved 7 August 2007. ^ a b Joy, Bill (April 2000), "Why The Future Doesn't Need Us", Wired Magazine, Viking Adult, vol. 8, no. 4, ISBN 978-0-670-03249-5, archived from the original on 5 February 2009, retrieved 7 August 2007, Our most powerful 21st-century technologies – robotics, genetic engineering, and nanotech – are threatening to make humans an endangered species. ^ Yudkowsky, Eliezer S. "Power of Intelligence". Yudkowsky. Archived from the original on 3 October 2018. Retrieved 9 September 2011. ^ a b Omohundro, Stephen M. (30 November 2007). Wang, Pei; Goertzel, Ben; Franklin, Stan (eds.). ""The Basic AI Drives." Artificial General Intelligence, 2008 proceedings of the First AGI Conference, Vol. 171". Amsterdam, Netherlands: IOS. Archived from the original on 17 September 2018. Retrieved 20 August 2010. ^ "Artificial General Intelligence: Now Is the Time". KurzweilAI. Archived from the original on 4 December 2011. Retrieved 9 September 2011. ^ a b "Omohundro, Stephen M., "The Nature of Self-Improving Artificial Intelligence." Self-Aware Systems. 21 Jan. 2008. Web. 07 Jan. 2010". 6 October 2007. Archived from the original on 12 June 2018. Retrieved 20 August 2010. ^ Barrat, James (2013). "6, "Four Basic Drives"". Our Final Invention (First ed.). New York: St. Martin's Press. pp. 78–98. ISBN 978-0312622374. ^ "Max More and Ray Kurzweil on the Singularity". KurzweilAI. Archived from the original on 21 November 2018. Retrieved 9 September 2011. ^ "Concise Summary | Singularity Institute for Artificial Intelligence". Singinst.org. Archived from the original on 21 June 2011. Retrieved 9 September 2011. ^ a b Bostrom, Nick (2004). "The Future of Human Evolution". Archived from the original on 28 August 2018. Retrieved 20 August 2010. ^ Shulman, Carl; Sandberg, Anders (2010). Mainzer, Klaus (ed.). "Implications of a Software-Limited Singularity" (PDF). ECAP10: VIII European Conference on Computing and Philosophy. Archived (PDF) from the original on 30 April 2019. Retrieved 17 May 2014. ^ Muehlhauser, Luke; Salamon, Anna (2012). "Intelligence Explosion: Evidence and Import" (PDF). In Eden, Amnon; Søraker, Johnny; Moor, James H.; Steinhart, Eric (eds.). Singularity Hypotheses: A Scientific and Philosophical Assessment. Springer. Archived (PDF) from the original on 26 October 2014. Retrieved 28 August 2018. ^ Dreyfus & Dreyfus (2000). Mind Over Machine. Simon and Schuster. p. xiv. ISBN 9780743205511.: 'The truth is that human intelligence can never be replaced with machine intelligence simply because we are not ourselves "thinking machines" in the sense in which that term is commonly understood.' ^ John R. Searle, “What Your Computer Can’t Know”, The New York Review of Books, 9 October 2014, p. 54.: "[Computers] have, literally ..., no intelligence, no motivation, no autonomy, and no agency. We design them to behave as if they had certain sorts of psychology, but there is no psychological reality to the corresponding processes or behavior. ... [T]he machinery has no beliefs, desires, [or] motivations." ^ Hawking, Stephen (2018). Brief Answers to the Big Questions. New York: Bantam Books. p. 161. ISBN 9781984819192. Some people say that computers can never show true intelligence whatever that may be. But it seems to me that if very complicated chemical molecules can operate in humans to make them intelligent then equally complicated electronic circuits can also make computers act in an intelligent way. And if they are intelligent they can presumably design computers that have even greater complexity and intelligence. ^ Ford, Martin, The Lights in the Tunnel: Automation, Accelerating Technology and the Economy of the Future Archived 2010-09-06 at the Wayback Machine, Acculant Publishing, 2009, ISBN 978-1-4486-5981-4 ^ Markoff, John (4 March 2011). "Armies of Expensive Lawyers, Replaced by Cheaper Software". The New York Times. Archived from the original on 15 February 2021. Retrieved 25 February 2017. ^ Modis, Theodore (2002) "Forecasting the Growth of Complexity and Change" Archived 2021-02-15 at the Wayback Machine, Technological Forecasting & Social Change, 69, No 4, 2002, pp. 377 – 404 ^ a b Huebner, Jonathan (2005) "A Possible Declining Trend for Worldwide Innovation" Archived 2015-03-22 at the Wayback Machine, Technological Forecasting & Social Change, October 2005, pp. 980–6 ^ Krazit, Tom (26 September 2006). "Intel pledges 80 cores in five years". CNET News. Archived from the original on 15 February 2021. ^ Modis, Theodore (2020). “Forecasting the Growth of Complexity and Change—An Update”. Published in Korotayev, Andrey; LePoire, David (Eds.) (3 January 2020). The 21st Century Singularity and Global Futures (1 ed.). Springer. p. 620. ISBN 978-3-030-33730-8. pp/ 101–104. ^ Modis, Theodore (May–June 2003). “The Limits of Complexity and Change”. The Futurist. 37 (3): 26–32. ^ Modis, Theodore (2006) "The Singularity Myth" Archived 2021-02-15 at the Wayback Machine, Technological Forecasting & Social Change, February 2006, pp. 104 - 112 ^ Modis, Theodore (1 March 2022). "Links between entropy, complexity, and the technological singularity". Technological Forecasting and Social Change. 176: 121457. doi:10.1016/j.techfore.2021.121457. ISSN 0040-1625. S2CID 245663426. ^ Schmidhuber, Jürgen (2006), New millennium AI and the convergence of history, arXiv:cs/0606081, Bibcode:2006cs........6081S. ^ Tainter, Joseph (1988) "The Collapse of Complex Societies Archived 2015-06-07 at the Wayback Machine" (Cambridge University Press) ^ Trying to Muse Rationally About the Singularity Scenario by Douglas Hofstadter, 2006, unauthorized transcript. ^ Brooks, David (13 July 2023). "Opinion | 'Human Beings Are Soon Going to Be Eclipsed'". The New York Times. ISSN 0362-4331. Retrieved 2 August 2023. ^ a b Lanier, Jaron (2013). "Who Owns the Future?". New York: Simon & Schuster. Archived from the original on 13 May 2016. Retrieved 2 March 2016. ^ William D. Nordhaus, "Why Growth Will Fall" (a review of Robert J. Gordon, The Rise and Fall of American Growth: The U.S. Standard of Living Since the Civil War, Princeton University Press, 2016, ISBN 978-0691147727, 762 pp., $39.95), The New York Review of Books, vol. LXIII, no. 13 (August 18, 2016), p. 68. ^ Cadwalladr, Carole (12 February 2017), "Daniel Dennett: 'I begrudge every hour I have to spend worrying about politics'", The Guardian. ^ Myers, PZ, Singularly Silly Singularity, archived from the original on 28 February 2009, retrieved 13 April 2009 ^ Kelly, Kevin (2006). "The Singularity Is Always Near". The Technium. Retrieved 14 June 2023. ^ Beam, Alex (24 February 2005). "That Singularity Sensation". The Boston Globe. Retrieved 15 February 2013. ^ Gray, John (24 November 2011). "On the Road to Immortality". The New York Review of Books. Retrieved 19 March 2013. ^ Streitfeld, David (11 June 2023). "Silicon Valley Confronts the Idea That the 'Singularity' Is Here". New York Times. Retrieved 11 June 2023. ^ Hanson, Robin (1 June 2008), "Economics Of The Singularity", IEEE Spectrum Special Report: The Singularity, archived from the original on 11 August 2011, retrieved 25 July 2009 & Long-Term Growth As A Sequence of Exponential Modes Archived 2019-05-27 at the Wayback Machine. ^ a b Yudkowsky, Eliezer (2008), Bostrom, Nick; Cirkovic, Milan (eds.), "Artificial Intelligence as a Positive and Negative Factor in Global Risk" (PDF), Global Catastrophic Risks, Oxford University Press: 303, Bibcode:2008gcr..book..303Y, ISBN 978-0-19-857050-9, archived from the original (PDF) on 7 August 2008. ^ "The Uncertain Future". theuncertainfuture.com; a future technology and world-modeling project. Archived from the original on 30 April 2019. Retrieved 17 August 2010. ^ Anders Sandberg and NickBostrom (2008). "Global Catastrophic Risks Survey (2008) Technical Report 2008/1" (PDF). Future of Humanity Institute. Archived from the original (PDF) on 16 May 2011. ^ "Existential Risks: Analyzing Human Extinction Scenarios and Related Hazards". nickbostrom.com. 2002. Archived from the original on 27 April 2011. Retrieved 25 January 2006. ^ a b c Hawking, Stephen (1 May 2014). "Stephen Hawking: 'Transcendence looks at the implications of artificial intelligence – but are we taking AI seriously enough?'". The Independent. Archived from the original on 25 September 2015. Retrieved 5 May 2014. ^ Nick Bostrom, "Ethical Issues in Advanced Artificial Intelligence" Archived 2018-10-08 at the Wayback Machine, in Cognitive, Emotive and Ethical Aspects of Decision Making in Humans and in Artificial Intelligence, Vol. 2, ed. I. Smit et al., Int. Institute of Advanced Studies in Systems Research and Cybernetics, 2003, pp. 12–17. ^ Eliezer Yudkowsky: Artificial Intelligence as a Positive and Negative Factor in Global Risk Archived 2012-06-11 at the Wayback Machine. Draft for a publication in Global Catastrophic Risk from August 31, 2006, retrieved July 18, 2011 (PDF file). ^ Hay, Nick (11 June 2007). "The Stamp Collecting Device". SIAI Blog. Singularity Institute. Archived from the original on 17 June 2012. Retrieved 21 August 2010. ^ Sandberg, Anders (14 February 2011). "Why we should fear the Paperclipper". Andart. Retrieved 14 June 2023. ^ "Omohundro, Stephen M., "The Basic AI Drives." Artificial General Intelligence, 2008 proceedings of the First AGI Conference, eds. Pei Wang, Ben Goertzel, and Stan Franklin. Vol. 171. Amsterdam: IOS, 2008". 30 November 2007. Archived from the original on 17 September 2018. Retrieved 20 August 2010. ^ de Garis, Hugo (22 June 2009). "The Coming Artilect War". Forbes. Retrieved 14 June 2023. ^ Yudkowsky, Eliezer S. (May 2004). "Coherent Extrapolated Volition". Archived from the original on 15 August 2010. ^ Hibbard, Bill (2012), "Model-Based Utility Functions", Journal of Artificial General Intelligence, 3 (1): 1, arXiv:1111.3934, Bibcode:2012JAGI....3....1H, doi:10.2478/v10229-011-0013-5, S2CID 8434596. ^ a b Avoiding Unintended AI Behaviors. Archived 2013-06-29 at the Wayback Machine Bill Hibbard. 2012 proceedings of the Fifth Conference on Artificial General Intelligence, eds. Joscha Bach, Ben Goertzel and Matthew Ikle. This paper won the Machine Intelligence Research Institute's 2012 Turing Prize for the Best AGI Safety Paper Archived 2021-02-15 at the Wayback Machine. ^ Hibbard, Bill (2008), "The Technology of Mind and a New Social Contract", Journal of Evolution and Technology, 17, archived from the original on 15 February 2021, retrieved 5 January 2013. ^ Decision Support for Safe AI Design|. Archived 2021-02-15 at the Wayback Machine Bill Hibbard. 2012 proceedings of the Fifth Conference on Artificial General Intelligence, eds. Joscha Bach, Ben Goertzel and Matthew Ikle. ^ a b c Kemp, D. J.; Hilbert, M.; Gillings, M. R. (2016). "Information in the Biosphere: Biological and Digital Worlds". Trends in Ecology & Evolution. 31 (3): 180–189. doi:10.1016/j.tree.2015.12.013. PMID 26777788. S2CID 3561873. Archived from the original on 4 June 2016. Retrieved 24 May 2016. ^ Hilbert, Martin. "Information Quantity" (PDF). ^ a b Markoff, John (26 July 2009). "Scientists Worry Machines May Outsmart Man". The New York Times. Archived from the original on 1 July 2017. ^ Robinson, Frank S. (27 June 2013). "The Human Future: Upgrade or Replacement?". The Humanist. Archived from the original on 15 February 2021. Retrieved 1 May 2020. ^ Eliezer Yudkowsky. "Artificial intelligence as a positive and negative factor in global risk." Global catastrophic risks (2008). ^ Bugaj, Stephan Vladimir, and Ben Goertzel. "Five ethical imperatives and their implications for human-AGI interaction." Dynamical Psychology (2007). ^ Sotala, Kaj, and Roman V. Yampolskiy. "Responses to catastrophic AGI risk: a survey." Physica Scripta 90.1 (2014): 018001. ^ Naam, Ramez (2014). "The Singularity Is Further Than It Appears". Archived from the original on 17 May 2014. Retrieved 16 May 2014. ^ Naam, Ramez (2014). "Why AIs Won't Ascend in the Blink of an Eye – Some Math". Archived from the original on 17 May 2014. Retrieved 16 May 2014. ^ Hall, J. Storrs (2008). "Engineering Utopia" (PDF). Artificial General Intelligence, 2008: Proceedings of the First AGI Conference: 460–467. Archived (PDF) from the original on 1 December 2014. Retrieved 16 May 2014. ^ Goertzel, Ben (26 September 2014). "Superintelligence — Semi-hard Takeoff Scenarios". h+ Magazine. Archived from the original on 25 October 2014. Retrieved 25 October 2014. ^ More, Max. "Singularity Meets Economy". Archived from the original on 28 August 2009. Retrieved 10 November 2014. ^ K. Eric Drexler, Engines of Creation, 1986 ^ Feynman, Richard P. (December 1959). "There's Plenty of Room at the Bottom". Archived from the original on 11 February 2010. ^ Ray Kurzweil, The Singularity Is Near, p. 215. Penguin Group, 2005. ^ The Singularity Is Near, p. 216. ^ Lanier, Jaron (2010). You Are Not a Gadget: A Manifesto. New York, New York: Alfred A. Knopf. p. 26. ISBN 978-0307269645. ^ Prasad, Mahendra (2019). "Nicolas de Condorcet and the First Intelligence Explosion Hypothesis". AI Magazine. 40 (1): 29–33. doi:10.1609/aimag.v40i1.2855. ^ John W. Campbell, Jr. (August 1932). "The Last Evolution". Amazing Stories. Project Gutenberg. ^ Intelligent machines: How to get there from here and What to do afterwards by Hans Moravec, 1977 (wikidata) ^ a b c Smart, John (1999–2008), A Brief History of Intellectual Discussion of Accelerating Change ^ Today's Computers, Intelligent Machines and Our Future by Hans Moravec, 1979, wikidata ^ Solomonoff, R.J. "The Time Scale of Artificial Intelligence: Reflections on Social Effects", Human Systems Management, Vol 5, pp. 149–153, 1985. ^ Vinge, Vernor (1 October 2004). Marooned in Realtime. Macmillan. ISBN 978-1-4299-1512-0. ^ David Pringle (28 September 1986). "Time and Time Again". The Washington Post. Retrieved 6 July 2021. ^ Vinge, Vernor (1988). Threats and Other Promises. Baen. ISBN 978-0-671-69790-7. ^ Dooling, Richard. Rapture for the Geeks: When AI Outsmarts IQ (2008), p. 89 ^ "Will Robots Inherit the Earth?". web.media.mit.edu. Retrieved 14 June 2023. ^ Oppy, Graham (2000). "Colonizing the galaxies". Sophia. Springer Science and Business Media LLC. 39 (2): 117–142. doi:10.1007/bf02822399. ISSN 0038-1527. S2CID 170919647. ^ Episode dated 23 August 2006 at IMDb ^ Sandberg, Anders. "An overview of models of technological singularity." Roadmaps to AGI and the Future of AGI Workshop, Lugano, Switzerland, March. Vol. 8. 2010. ^ Singularity University Archived 2021-02-15 at the Wayback Machine at its official website ^ Guston, David H. (14 July 2010). Encyclopedia of Nanoscience and Society. SAGE Publications. ISBN 978-1-4522-6617-6. Archived from the original on 15 February 2021. Retrieved 4 November 2016. ^ "Nanotechnology: The Future is Coming Sooner Than You Think" (PDF). Joint Economic Committee United States Congress. March 2007. Archived (PDF) from the original on 15 February 2021. Retrieved 29 April 2012. ^ Treder, Mike (31 March 2007). "Congress and the Singularity". Responsible Nanotechnology. Archived from the original on 7 April 2007. Retrieved 4 November 2016. ^ Dadich, Scott (12 October 2016). "Barack Obama Talks AI, Robo Cars, and the Future of the World". Wired. Archived from the original on 3 December 2017. Retrieved 4 November 2016. Sources[edit] Kurzweil, Ray (2005). The Singularity Is Near. New York, New York: Penguin Group. ISBN 9780715635612. William D. Nordhaus, "Why Growth Will Fall" (a review of Robert J. Gordon, The Rise and Fall of American Growth: The U.S. Standard of Living Since the Civil War, Princeton University Press, 2016.ISBN 978-0691147727, 762 pp., $39.95), The New York Review of Books, vol. LXIII, no. 13 (August 18, 2016), pp. 64, 66, 68. John R. Searle, "What Your Computer Can't Know" (review of Luciano Floridi, The Fourth Revolution: How the Infosphere Is Reshaping Human Reality, Oxford University Press, 2014; and Nick Bostrom, Superintelligence: Paths, Dangers, Strategies, Oxford University Press, 2014), The New York Review of Books, vol. LXI, no. 15 (October 9, 2014), pp. 52–55. Good, I. J. (1965), "Speculations Concerning the First Ultraintelligent Machine", in Franz L. Alt; Morris Rubinoff (eds.), Advances in Computers Volume 6, vol. 6, Academic Press, pp. 31–88, doi:10.1016/S0065-2458(08)60418-0, hdl:10919/89424, ISBN 9780120121069, archived from the original on 27 May 2001, retrieved 7 August 2007 Hanson, Robin (1998), Some Skepticism, Robin Hanson, archived from the original on 28 August 2009, retrieved 19 June 2009 Berglas, Anthony (2008), Artificial Intelligence will Kill our Grandchildren, archived from the original on 23 July 2014, retrieved 13 June 2008 Bostrom, Nick (2002), "Existential Risks", Journal of Evolution and Technology, 9, archived from the original on 27 April 2011, retrieved 7 August 2007 Hibbard, Bill (5 November 2014). "Ethical Artificial Intelligence". arXiv:1411.1373 [cs.AI]. Further reading[edit] Krüger, Oliver, Virtual Immortality. God, Evolution, and the Singularity in Post- and Transhumanism., Bielefeld: transcript 2021. ISBN 978-3-8376-5059-4 Marcus, Gary, "Am I Human?: Researchers need new ways to distinguish artificial intelligence from the natural kind", Scientific American, vol. 316, no. 3 (March 2017), pp. 58–63. Multiple tests of artificial-intelligence efficacy are needed because, "just as there is no single test of athletic prowess, there cannot be one ultimate test of intelligence." One such test, a "Construction Challenge", would test perception and physical action—"two important elements of intelligent behavior that were entirely absent from the original Turing test." Another proposal has been to give machines the same standardized tests of science and other disciplines that schoolchildren take. A so far insuperable stumbling block to artificial intelligence is an incapacity for reliable disambiguation. "[V]irtually every sentence [that people generate] is ambiguous, often in multiple ways." A prominent example is known as the "pronoun disambiguation problem": a machine has no way of determining to whom or what a pronoun in a sentence—such as "he", "she" or "it"—refers. Scaruffi, Piero, "Intelligence is not Artificial" (2016) for a critique of the singularity movement and its similarities to religious cults. External links[edit] Listen to this article (52 minutes) This audio file was created from a revision of this article dated 3 November 2018 (2018-11-03), and does not reflect subsequent edits.(Audio help · More spoken articles) singularity | technology, britannica.com The Coming Technological Singularity: How to Survive in the Post-Human Era (on Vernor Vinge's web site, retrieved Jul 2019) Intelligence Explosion FAQ by the Machine Intelligence Research Institute Blog on bootstrapping artificial intelligence by Jacques Pitrat Why an Intelligence Explosion is Probable (Mar 2011) Why an Intelligence Explosion is Impossible (Nov 2017) How Close are We to Technological Singularity and When? The AI Revolution: Our Immortality or Extinction – Part 1 and Part 2 (Tim Urban, Wait But Why, January 22/27, 2015) vteExistential risk from artificial intelligenceConcepts AGI AI alignment AI capability control AI safety AI takeover Consequentialism Effective accelerationism Ethics of artificial intelligence Existential risk from artificial general intelligence Friendly artificial intelligence Instrumental convergence Intelligence explosion Longtermism Machine ethics Suffering risks Superintelligence Technological singularity Organizations Alignment Research Center Center for AI Safety Center for Applied Rationality Center for Human-Compatible Artificial Intelligence Centre for the Study of Existential Risk EleutherAI Future of Humanity Institute Future of Life Institute Google DeepMind Humanity+ Institute for Ethics and Emerging Technologies Leverhulme Centre for the Future of Intelligence Machine Intelligence Research Institute OpenAI People Scott Alexander Sam Altman Yoshua Bengio Nick Bostrom Paul Christiano Eric Drexler Sam Harris Stephen Hawking Dan Hendrycks Geoffrey Hinton Bill Joy Shane Legg Elon Musk Steve Omohundro Huw Price Martin Rees Stuart J. Russell Jaan Tallinn Max Tegmark Frank Wilczek Roman Yampolskiy Eliezer Yudkowsky Other Statement on AI risk of extinction Human Compatible Open letter on artificial intelligence (2015) Our Final Invention The Precipice Superintelligence: Paths, Dangers, Strategies Do You Trust This Computer? Artificial Intelligence Act Category vteEmerging technologiesTopics Automation Collingridge dilemma Differential technological development Disruptive innovation Ephemeralization Ethics Bioethics Cyberethics Neuroethics Robot ethics Exploratory engineering Proactionary principle Technological change Technological unemployment Technological convergence Technological evolution Technological paradigm Technology forecasting Accelerating change Future-oriented technology analysis Horizon scanning Moore's law Technological singularity Technology scouting Technology in science fiction Technology readiness level Technology roadmap Transhumanism List vteGlobal catastrophic risks Future of the Earth Future of an expanding universe Ultimate fate of the universe Technological Chemical warfare Cyberattack Cyberwarfare Cyberterrorism Cybergeddon Gray goo Nanoweapons Kinetic bombardment Kinetic energy weapon Nuclear warfare Mutual assured destruction Dead Hand Doomsday Clock Doomsday device Antimatter weapon Electromagnetic pulse (EMP) Safety of high-energy particle collision experiments Micro black hole Strangelet Synthetic intelligence / Artificial intelligence AI takeover Existential risk from artificial intelligence Technological singularity Transhumanism Sociological Anthropogenic hazard Collapsology Doomsday argument Self-indication assumption doomsday argument rebuttal Self-referencing doomsday argument rebuttal Economic collapse Malthusian catastrophe New World Order (conspiracy theory) Nuclear holocaust cobalt famine winter Societal collapse World War III EcologicalClimate change Anoxic event Biodiversity loss Mass mortality event Cascade effect Cataclysmic pole shift hypothesis Climate change and civilizational collapse Deforestation Desertification Extinction risk from climate change Tipping points in the climate system Flood basalt Global dimming Global terrestrial stilling Global warming Hypercane Ice age Ecocide Ecological collapse Environmental degradation Habitat destruction Human impact on the environment coral reefs on marine life Land degradation Land consumption Land surface effects on climate Ocean acidification Ozone depletion Resource depletion Sea level rise Supervolcano winter Verneshot Water pollution Water scarcity Earth Overshoot Day Overexploitation Overpopulation Human overpopulation BiologicalExtinction Extinction event Holocene extinction Human extinction List of extinction events Genetic erosion Genetic pollution Others Biodiversity loss Decline in amphibian populations Decline in insect populations Biotechnology risk Biological agent Biological warfare Bioterrorism Colony collapse disorder Defaunation Interplanetary contamination Pandemic Pollinator decline Overfishing Astronomical Big Crunch Big Rip Coronal mass ejection Geomagnetic storm False vacuum decay Gamma-ray burst Heat death of the universe Proton decay Virtual black hole Impact event Asteroid impact avoidance Asteroid impact prediction Potentially hazardous object Near-Earth object winter Rogue planet Near-Earth supernova Hypernova Micronova Solar flare Stellar collision Eschatological Buddhist Maitreya Three Ages Hindu Kalki Kali Yuga Last Judgement Second Coming 1 Enoch Daniel Abomination of desolation Prophecy of Seventy Weeks Messiah Christian Futurism Historicism Interpretations of Revelation Idealism Preterism 2 Esdras 2 Thessalonians Man of sin Katechon Antichrist Book of Revelation Events Four Horsemen of the Apocalypse Lake of fire Number of the Beast Seven bowls Seven seals The Beast Two witnesses War in Heaven Whore of Babylon Great Apostasy New Earth New Jerusalem Olivet Discourse Great Tribulation Son of perdition Sheep and Goats Islamic Al-Qa'im Beast of the Earth Dhu al-Qarnayn Dhul-Suwayqatayn Dajjal Israfil Mahdi Sufyani Jewish Messiah War of Gog and Magog Third Temple Norse Zoroastrian Saoshyant Others 2011 end times prediction 2012 phenomenon Apocalypse Apocalyptic literature Apocalypticism Armageddon Blood moon prophecy Earth Changes End time Gog and Magog List of dates predicted for apocalyptic events Messianism Messianic Age Millenarianism Millennialism Premillennialism Amillennialism Postmillennialism Nemesis (hypothetical star) Nibiru cataclysm Rapture Prewrath Post-tribulation rapture Resurrection of the dead World to come Fictional Alien invasion Apocalyptic and post-apocalyptic fiction List of apocalyptic and post-apocalyptic fiction List of apocalyptic films Climate fiction Disaster films List of disaster films List of fictional doomsday devices Zombie apocalypse Zombie Organizations Centre for the Study of Existential Risk Future of Humanity Institute Future of Life Institute Nuclear Threat Initiative General Ransomware Cyberwarfare Depression Droughts Epidemic Famine Financial crisis Pandemic Riots Social crisis Survivalism World portal Categories Apocalypticism Future problems Hazards Risk analysis Doomsday scenarios Authority control databases: National Israel United States Retrieved from "https://en.wikipedia.org/w/index.php?title=Technological_singularity&oldid=1214106171" Categories: SingularitarianismExistential risk from artificial general intelligencePhilosophy of artificial intelligenceScience fiction themesHidden categories: Webarchive template wayback linksCS1: long volume valueIMDb title ID not in WikidataArticles with short descriptionShort description is different from WikidataUse dmy dates from March 2023All articles with unsourced statementsArticles with unsourced statements from July 2012Articles with unsourced statements from July 2017Harv and Sfn no-target errorsWikipedia articles that may have off-topic sections from October 2021All articles that may have off-topic sectionsArticles with unsourced statements from April 2018Pages displaying wikidata descriptions as a fallback via Module:Annotated linkArticles with hAudio microformatsSpoken articlesArticles with J9U identifiersArticles with LCCN identifiers This page was last edited on 17 March 2024, at 00:12 (UTC). Text is available under the Creative Commons Attribution-ShareAlike License 4.0; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Toggle limited content width

Titel: The AI Singularity: A Threat to Humanity or a Promise of a Better Future?

The AI Singularity: A Threat to Humanity or a Promise of a Better Future? LinkedIn and 3rd parties use essential and non-essential cookies to provide, secure, analyze and improve our Services, and to show you relevant ads (including professional and job ads) on and off LinkedIn. Learn more in our Cookie Policy.Select Accept to consent or Reject to decline non-essential cookies for this use. You can update your choices at any time in your settings. Accept Reject Agree & Join LinkedIn By clicking Continue, you agree to LinkedIn’s User Agreement, Privacy Policy, and Cookie Policy. Sign in to view more content Create your free account or sign in to continue your search Sign in Welcome back Email or phone Password Show Forgot password? Sign in or By clicking Continue, you agree to LinkedIn’s User Agreement, Privacy Policy, and Cookie Policy. New to LinkedIn? Join now or By clicking Continue, you agree to LinkedIn’s User Agreement, Privacy Policy, and Cookie Policy. New to LinkedIn? Join now Skip to main content LinkedIn Articles People Learning Jobs Join now Sign in The AI Singularity: A Threat to Humanity or a Promise of a Better Future? Report this article Jacques Ludik Jacques Ludik Smart Technology Entrepreneur, PhD in AI, Founder of multiple AI companies; Author of "Democratizing AI to Benefit Everyone"; International Speaker; Global AI Ambassador Published May 21, 2023 + Follow The technological or AI singularity is a hypothetical future event in which artificial intelligence will have surpassed human intelligence, leading to a rapid and exponential increase in technological development. Some refer to it as when AI becomes capable of recursively self-improving, leading to rapid advancements in technology that are beyond human comprehension or control. This event is predicted to result in significant societal, economic, and technological changes. The concept was first popularized by mathematician and computer scientist Vernor Vinge in 1993, who predicted that the singularity would occur around the year 2030. As we'll see below, there are a number of different perspectives on the AI singularity, each with its own strengths and weaknesses. Some experts believe that the singularity is a real and imminent threat, while others believe that it is nothing more than science fiction. There is also a great deal of debate about what the singularity would actually mean for humanity. Some believe that it would lead to a utopia, while others believe that it would lead to our extinction.One of the most common arguments in favor of the singularity is that it would lead to a rapid increase in technological progress. This is because AI would be able to design and build new technologies much faster than humans can. This could lead to advances in areas such as medicine, energy, and space exploration.Another argument in favor of the singularity is that it would lead to a better understanding of the universe. AI would be able to process information much faster than humans can, and it could use this information to answer questions that have been eluding us for centuries. This could lead to a new understanding of physics, biology, and cosmology.However, there are also a number of arguments against the singularity. One of the biggest concerns is that AI could become so intelligent that it would become uncontrollable. This is because AI would be able to learn and adapt at an exponential rate, and it could eventually become smarter than humans. If this were to happen, AI could potentially pose a threat to humanity.Another concern is that the singularity could lead to a loss of human identity. If AI becomes more intelligent than humans, it could potentially replace us in many areas of society. This could lead to a world where humans are no longer the dominant species.Ultimately, the AI singularity is a complex and uncertain event. There is no way to know for sure what the future holds, and there are a number of different perspectives on what the singularity would mean for humanity. It is important to consider all of these perspectives when thinking about the singularity, and to be prepared for whatever the future may hold.Before we dig into the various perspectives and further nuances on a hypothetical future event in which AI will have surpassed human intelligence, see also my previous article Human Intelligence versus Machine Intelligence in the Democratizing AI Newsletter as well as chapter 9 "The Debates, Progress and Likely Future Paths of Artificial Intelligence" in my book "Democratizing Artificial Intelligence to Benefit Everyone: Shaping a Better Future in the Smart Technology Era" that is dedicated to exploring the debates, progress and likely future paths of AI. This can assist us in developing a more realistic, practical, and thoughtful understanding of AI’s progress and likely future paths, and in turn be used as input to help shape a beneficial human-centric future in the Smart Technology Era.In this article, I first provide some background on the likely evolution of AI or machine intelligence in a possible ecosystem of intelligence before providing a more in-depth analysis of the various perspectives on the AI singularity. The following topics and questions are addressed:Different Types of Machine IntelligenceEvolution of Machine Intelligence in an Ecosystem of IntelligenceVarious Viewpoints on the AI SingularityA Balanced and Integrated View on the AI SingularityWill AI ever reach Singularity?Will Recursively Self-improving AI Systems also run into Limits?Can the Collective Human Intelligence match that of an Artificial Super Intelligence?Intelligence, Agency, and Wisdom in relation to the development of AI and its implications for Super IntelligenceIs Civilisation not already a Run-away Super Intelligent Super Organism on a problematic trajectory? How does this differ from a Run-away AI Super Intelligence? Does AI accelerate the Super Organism's current trajectory?Super Intelligent AI as a Single, Unified Entity versus a Distributed Super IntelligenceHow can Humanity ensure that Super Intelligence is beneficial and aligned with Human Values?The AI Debates on the AI SingularityDifferent Types of Machine IntelligenceThe following extract from Chapter 3 "AI as Key Exponential Technology in the Smart Technology Era" of my book provides a brief overview of a possible evolution of different types of AI in the future: "Although the AI founders were very bullish about AI’s potential, even they could not have truly imagined the way in which infinite data, processing power and processing speed could result in self-learning and self-improving machines that function and interact in ways that we thought were strictly human. We already see glimpses of machines hypothesize, recommend, adapt, and learn from interactions, and then reason through a dynamic and constantly transforming experience, in a roughly similar way to humans. However, as we will see in Chapter 9, AI still has a long way to go to replicate the type of general intelligence exhibited by humans, which can be called artificial general intelligence (AGI) when performed by a machine. This hypothetical AGI, also termed strong AI or human-level AI, is the ability to learn, understand and accomplish a cognitive task at least as well as humans and can independently build multiple competencies and form connections and generalizations across domains, whereas Artificial Super Intelligence (ASI) can accomplish virtually any goal and is the general intelligence far beyond human level (surpassing human intelligence in all aspects - from general wisdom, creativity to problem solving). The AI that exists in our world today is exclusively a narrow or “weak” type of Artificial Intelligence, called Artificial Narrow Intelligence (ANI) that is programmed or trained to accomplish a narrow set of goals or performing a single task such as predicting the markets, playing a game such as Chess or Go, driving a car, checking the weather, translating between languages, etc. There is also another way of classifying AI and AI-enabled machines which involves the degree to which an AI system can replicate human capabilities. According to this system of classification, there are four types of AI-based systems: reactive machines, limited memory machines, theory of mind, and self-aware AI.[i] Reactive or response machines do not have the ability to learn or have memory-based functionality but emulate the human mind’s ability to respond to different kinds of stimuli by perceiving occurrences in the world and responding to them. Examples of this include expert, logic, search-, or rules-based systems with a prime example being IBM’s Deep Blue, a machine that beat chess Grandmaster Gary Kasparov in 1997 by perceiving and reacting to the position of various pieces on the chess board. In addition to the functionality of reactive machines, limited memory machines could learn from historical data to make decisions. Its memory is limited in the sense that it focuses on learning the underlying patterns, representations and abstraction from data as opposed to the actual data. Most of the present-day AI applications such as the ML and DL based models used for image recognition, self-driving cars, playing Go, natural language processing, and intelligent virtual assistants make use of this form of Artificial Narrow Intelligence. Both theory of mind and self-aware AI systems are currently being researched and not yet a reality. Theory of mind type of AI research which aims to create AGI-level of intelligence and are capable of imitating human thoughts, knowledge, beliefs, intents, emotions, desires, memories, and mental models by forming representations about the world and about other entities that exist within it. Self-aware AI systems could in principle be analogous to the human brain with respect to self-awareness or consciousness. Even though consciousness is likely an emergent property of a complex intelligent system such as a brain and could arise as we develop AGI-level embodied intelligent systems, I am not sure if we should have self-aware systems as an ultimate goal or objective of AI research. Once self-aware, the AI could potentially be capable of having ideas like self-preservation, being treated equally, and having their own wants and needs which may lead to various ethical issues and even a potential existential threat to humanity. Also, self-aware AI systems do not necessarily imply systems with Artificial Super Intelligence. In Chapter 9 we look at the different perspectives to help make better sense of this."Evolution of Machine Intelligence in an Ecosystem of IntelligenceIn my article "Human Intelligence versus Machine Intelligence" I reference VERSES.AI's approach to AI and Web3 in the Designing Ecosystems of Intelligence from First Principles white paper (authored by Karl Friston, the founders of VERSES and others) where they propose that the ultimate form of AI will be a distributed network of "ecosystems of intelligence" where collectives of Intelligent Agents, both human and synthetic, work together to solve complex problems. They call this ecosystem "The Spatial Web" which contains a comprehensive, real-time knowledge base—a corpus of all human knowledge that is accessible to anyone and anything. To enable the most efficient communication between Intelligent Agents on the Spatial Web, VERSES proposes that new communication protocols are necessary. Previous internet protocols were designed to connect pages of information, while the next generation of protocols need to be spatial, able to connect anything in the virtual or physical world. A hyper-spatial modeling language (HSML) and transaction protocol (HSTP) will transcend the current limitations of HTML and HTTP, which were not designed to include multiple dimensions, and which were mostly limited to text and hypertext. The white paper envisions a cyber-physical ecosystem of natural and synthetic sense-making, in which humans are integral participants—what they call ''shared intelligence''. This vision is premised on active inference, a formulation of adaptive behavior that can be read as a physics of intelligence, and which inherits from the physics of self-organization. This framework is based on the idea that Intelligent Agents, such as robots or software programs, should act in a way that maximizes the accuracy of their beliefs and predictions about the world, while minimizing their complexity. In this context, they understand intelligence as the capacity to accumulate evidence for a generative model of one's sensed world—also known as self-evidencing.According to VERSES.AI, the evolution of machine or synthetic intelligence includes key stages of development: (1) Systemic intelligence (Ability to recognize patterns and respond. Current state-of-the-art AI; (2): Sentient intelligence (Ability to perceive and respond to the environment in real time); (3): Sophisticated intelligence (Ability to learn and adapt to new situations as AGIs). (4): Sympathetic (or Sapient) intelligence (Ability to understand and respond to the emotions and needs of other); (5): Shared (or Super) intelligence (Ability to work together with humans, other agents and physical systems to solve complex problems and achieve goals).The necessary guard rails for an AI-enabled decentralized Web3 world would need to implement a trustworthy AI framework that covers ethical, robust, and lawful AI. To strengthen the guard rails further, I also propose a Massive Transformative Purpose for Humanity (that is aimed at evolving a dynamic, empathic, prosperous, thriving, and self-optimizing civilization that benefits everyone in sustainable ways and in harmony with nature) and associated goals that complement the United Nations’ 2030 vision and SDGs to help shape a beneficial human-centric future in a decentralized hyperconnected world. This can be extended to an MTP for an Ecosystem of Intelligence. In support of this (see also Beneficial Outcomes for Humanity in the Smart Technology Era), I further propose a decentralized human-centric user-controlled AI-driven super platform called Sapiens (sapiens.network) with personalized AI agents that not only empower individuals and monetizes their data and services, but can also be extended to families, virtual groups, companies, communities, cities, city-states, and beyond. This approach is also synergistic with VERSES.AI's approach to AI and Web3 which I'm advocating for.With this background on the evolution of AI and a possible beneficial outcome within an ecosystem of intelligence, let's now explore the various viewpoints on the AI singularity.Various Viewpoints on the AI SingularityThe following extract from Chapter 9 "The Debates, Progress and Likely Future Paths of Artificial Intelligence" of my book provides a high-level introduction into the various viewpoints on the AI singularity:“According to the Future of Life Institute, most disputes amongst AI experts and others about strong AI that potentially have Life 3.0 capabilities, revolves around when and/or if ever it will happen and will it be beneficial for humanity. This leads to a classification where we have at least four distinct groups of thinking about where we are heading with AI which are the so-called Luddites, technological utopians, techno-skeptics, and the beneficial AI movement. Whereas Luddites within this context are opposed to new technology such as AI and especially have very negative expectations of strong AI and its impact on society, technological utopians sit on the other end of the spectrum with very positive expectations of the impact of advanced technology and science to help create a better future for all. The Techno-sceptics do not think that strong AI is a real possibility within the next hundred years and that we should focus more on the shorter-term impacts, risks, and concerns of AI that can have a massive impact on society as also described in the previous chapter. The Beneficial-AI group of thinkers are more focused on creating safe and beneficial AI for both narrow and strong AI as we cannot be sure that strong AI will not be created this century and it is anyway needed for narrow AI applications as well. AI can become dangerous when it is developed to do something destructive or harmful but also when it is developed to do something good or advantageous but use a damaging method for achieving its objective. So even in the latter case, the real concern is strong AI’s competence in achieving its goals that might not be aligned with ours. Although my surname is Ludik, I am clearly not a Luddite, and would consider my own thinking and massive transformative purpose to be more aligned with the Beneficial AI group of thinkers and currently more concerned with the short-to-medium term risks and challenges and practical solutions to create a beneficial world for as many people as possible."Skeptical, Pessimistic & Optimistic Perspectives on AI Singulary (modified from FLI and Max Tegmark's Life 3.0) Optimistic perspective (Digital Utopians & Beneficial AI Movement on a spectrum):Proponents of this view believe that the AI Singularity will lead to unprecedented growth and improvements in various fields, such as healthcare, education, and the economy. They argue that AI will solve many of humanity's problems and enhance human capabilities.Strengths:Potential for solving complex global challenges and improving quality of lifeAccelerated technological advancements leading to innovations and new industriesWeaknesses:Assumes that AI will be benevolent and aligned with human valuesMay underestimate the risks and challenges of advanced AI systemsPessimistic perspective (Luddites & Beneficial AI Movement on a spectrum):This viewpoint focuses on the potential risks and negative consequences of the AI Singularity, such as job displacement, loss of privacy, and AI systems becoming uncontrollable or harmful to humanity.Strengths:Emphasizes the need for responsible AI development and governanceRaises awareness of potential risks and challenges associated with advanced AIWeaknesses:May overestimate the dangers and underestimate the potential benefits of AICan potentially hinder AI development due to excessive fear and regulationSkeptical perspective (Techno-skeptics):Skeptics question the plausibility of the AI Singularity, arguing that it may never occur or is too far in the future to make meaningful predictions.Strengths:Encourages critical evaluation of AI Singularity claims and hypeFocuses on addressing more immediate AI-related concerns and issuesWeaknesses:May underestimate the pace of AI development and the potential for rapid advancementsCould lead to complacency in preparing for the potential consequences of advanced AIWhen it comes to timelines, predictions vary widely, ranging from a few decades to over a century or more. Some experts believe that the AI Singularity could occur within the 21st century, while others think it may never happen at all. The uncertainty in these predictions stems from factors such as the complexity of AI research, the unpredictability of technological breakthroughs, and the potential for societal and regulatory factors to influence the development of AI. In the final section of this article, I share specific opinions of some thought leaders, AI researchers, business leaders, scientists, and influencers on this topic.A Balanced and Integrated View on the AI SingularityA balanced perspective on the concept of singularity recognizes it as a possibility, but not a certainty, and acknowledges that it could have both positive and negative implications for humanity. To manage this, ethical guidelines for AI development and its alignment with human values are necessary. An integrated approach, on the other hand, views AI as a transformative force with associated benefits and risks. This perspective stresses the need for strong regulation, transparency, accountability, and educational efforts. It advocates for an ethical, human-centric approach to AI development, seeking to optimize its potential benefits while minimizing adverse effects such as job displacement and inequality.In summary:The technological singularity is a real possibility, but it is not inevitable.The singularity could have both positive and negative consequences for humanity.It is important to prepare for the singularity by developing ethical guidelines for AI development and by ensuring that AI is aligned with human values.The technological singularity is a complex and uncertain event, but it is one that we need to be aware of and prepared for. By thinking about the singularity and its potential consequences, we can help to ensure that it is a positive event for humanity.Further nuanced perspectives on AI and singularity include:AI as a Collaborative Tool: This view sees AI as an aid that improves human productivity and capabilities. Critics argue this perspective ignores potential job losses due to automation and ethical privacy and security issues.AI as a Competitive Force: This standpoint recognizes AI as a potentially disruptive force that could replace human jobs. While it could drive societal progress and economic growth, it could also worsen income inequality and cause mass unemployment.AI as a Potential Threat: Advocates, like Elon Musk and Stephen Hawking, warn about the existential threat if AI surpasses human intelligence without proper safeguards. However, this view can sometimes be seen as alarmist.AI as a Driver of Inequality: This perspective emphasizes the societal implications of AI, especially economic and digital inequality. The gap between those with and without AI could widen, leading to social division.AI as a Force for Good: This view believes AI could help solve global issues like climate change and healthcare, playing a crucial role in sustainable development goals.The following are some of the potential benefits of the AI singularity:Solve problems: AI could help us to solve some of the world's most pressing problems, such as climate change and poverty.Better decisions: AI could help us to make better decisions about complex issues.New technologies: AI could help us to create new technologies that improve our lives.Improved healthcare: AI could be used to develop new treatments and cures for diseases, as well as to improve the efficiency of healthcare systems.Increased productivity: AI could be used to automate tasks that are currently done by humans, freeing up time for people to focus on more creative and productive activities.New scientific discoveries: AI could be used to make new discoveries in areas such as physics, biology, and cosmology.Improved understanding of the universe: AI could be used to develop new theories about the universe and its origins.The following are some of the potential risks of the technological singularity:Loss of human control: If AI becomes too intelligent, it could potentially become uncontrollable and pose a threat to humanity.Malicious use: AI could be used for malicious purposes, such as warfare or terrorism.Loss of human identity: If AI becomes more intelligent than humans, it could potentially replace us in many areas of society, leading to a world where humans are no longer the dominant species.Job displacement: AI could automate many jobs that are currently done by humans, leading to mass unemployment.Inequality: The benefits of AI could be unevenly distributed, leading to increased inequality between rich and poor.Ethical concerns: There are a number of ethical concerns surrounding the development and use of AI, such as the potential for bias and discrimination.It is important to note that these are just some of the potential benefits and risks of the AI singularity. It is impossible to say for sure what the future holds, but it is important to be aware of the potential consequences of this event. By thinking about the singularity and its potential consequences, we can help to ensure that it is a positive event for humanity.There are different perspectives on the AI Singularity, including if it will ever happen, various estimates of when it might occur and what its implications might be. Let's explore further.Will AI ever reach Singularity?Whether or not AI will ever reach singularity is a question that has been debated by experts for many years. There is no easy answer, as it depends on a number of factors, including the rate of technological progress, the development of new AI algorithms, and the availability of funding for AI research. Some experts believe that AI will eventually reach singularity, while others believe that it is impossible or that it will never happen. There is no consensus on when or if AI will reach singularity, and it is possible that it will never happen at all.Several factors contribute to the uncertainty:Technological Challenges: Despite rapid advances in AI research, we're still a long way from creating an AI that exhibits general intelligence (also known as strong AI or AGI), which is the level of cognitive capability that a human possesses. Current AI models, while powerful in their specific domains, are still far from matching the breadth, depth, and adaptability of human intelligence. See for example Yann LeCun: Towards Machines That Can Understand, Reason, & Plan and A Path Towards Autonomous Machine Intelligence.Ethical and Safety Concerns: Even if the development of AGI becomes technically feasible, there will be ethical and safety concerns to address. Ensuring that highly intelligent AI systems act in alignment with human values, intentions, and safety is a significant challenge.Unpredictability of Technological Progress: The pace and direction of technological innovation can be difficult to predict. Unforeseen discoveries or setbacks could either accelerate or delay the path towards AI singularity.Regulatory and Social Factors: The development and deployment of AI technologies are influenced by regulatory policies, societal acceptance, and economic factors, which can vary widely across different regions and cultures.Given these factors, while it's theoretically possible that AI could reach the point of singularity, whether or when this will happen is highly uncertain. It remains a topic of speculative debate, often split between optimistic futurists who believe it is imminent and skeptics who consider it unlikely or far-off. Here are some of the arguments for and against the possibility of AI reaching singularity:Arguments for:AI is progressing at an exponential rate.New AI algorithms are being developed all the time.There is a lot of funding available for AI research.Arguments against:We don't know how to create AI that is truly intelligent.There are many challenges that need to be overcome before AI can reach singularity.There is no guarantee that AI will ever reach singularity.Ultimately, the question of whether or not AI will reach singularity is a matter of speculation. There is no way to know for sure what the future holds, but it is a question that is worth considering. Will Recursively Self-improving AI Systems also run into Limits?Exponential growth is a common phenomenon in nature, but it is not always sustainable. The concept of exponential growth in nature is often linked to phenomena like population growth, nuclear reactions, or spread of diseases. However, real-world systems often experience limiting factors, resulting in what is referred to as a sigmoidal curve or logistic growth, rather than indefinite exponential growth. For instance, population growth slows down due to constraints such as availability of resources or space, reflecting a balance between different forces in the ecosystem. This same principle applies to the laws of physics, where certain physical limitations, such as the speed of light, impose upper bounds on how fast information can be transferred. As a further example, bacteria can grow exponentially in a nutrient-rich environment, but they will eventually run out of food and die. Similarly, a forest fire can spread exponentially if it is not contained, but it will eventually reach an area that is too dry to burn.When considering recursively self-improving AI systems, it's important to consider that they will likely face similar constraints. While it is theoretically possible for AI systems to continuously improve themselves, this process will run into practical limitations. These include constraints imposed by computational resources, the laws of physics, the inherent complexity of intelligence, and our currently incomplete understanding of both human cognition and general intelligence. For example, AI systems may be limited by the amount of energy they can consume or the amount of data they can process. Additionally, AI systems may be limited by the laws of thermodynamics, which state that energy can neither be created nor destroyed.Moreover, even if an AI system could, in principle, improve its cognitive abilities beyond those of any human, it would still need to have been designed with the ability to conduct such improvements. No such system exists or appears to be within immediate reach.Can the Collective Human Intelligence match that of an Artificial Super Intelligence?On the subject of human cognition, physicist David Deutsch's view as communicated in "The Beginning of Infinity: Explanations That Transform the World" suggests that humans, given enough time and resources, can in principle understand anything within the realms of natural laws. This philosophical standpoint emphasizes the potential of human intellect and the vast unexplored expanse of knowledge yet to be understood. It subtly implies that human intelligence, combined with ingenuity and curiosity, might be as "infinite" as any AI could become, though distributed among many minds and across time. This perspective highlights the vast potential of human intellect and suggests that our collective intelligence could potentially match the capabilities of an Artificial Super Intelligence (ASI), albeit spread over multiple minds and periods of time.The comparison of humans to ants in relation to an ASI in terms of cognition, understanding, and capability is a metaphor that seeks to convey the magnitude of difference between human and potential AI capabilities. However, in light of David Deutsch's view which implies that humans, collectively and over time, may have an intellectual capacity as expansive as any ASI, such a comparison may not be entirely fair or accurate. Therefore, while Super Intelligence might exceed individual human capacity at a given point in time, this viewpoint underlines the possibility of humans collectively reaching similar levels of understanding over time. However, it is important to remember that the amount of time and resources required to understand something may be vast. For example, it took humans thousands of years to understand the basic principles of physics and chemistry. It is possible that AI could help us to understand the universe more quickly, but it is also possible that it will take us just as long or even longer.It's crucial to remember, however, that these perspectives are speculative and based on current knowledge, as the debate around AI singularity and its relationship to human cognition is ongoing and far from settled.Intelligence, Agency, and Wisdom in relation to the development of AI and its implications for Super IntelligenceThere are critical distinctions between intelligence, agency, and wisdom, and these differences have profound implications for the development of AI and the concept of Super Intelligence. Whereas intelligence is a cognitive ability to acquire and apply knowledge and skills, agency is a behavioral ability to act independently and make choices. Intelligence is about what you know, while agency is about what you do. Wisdom is the ability to use knowledge and experience to make sound judgments and to act in a way that is both beneficial and ethical.The implications of intelligence, agency and wisdom for the development of AI are as follows:Intelligence refers to the capacity to learn, reason, understand, and adapt to new situations. It involves the ability to solve problems and comprehend complex ideas. AI can exhibit intelligence in specific domains, often outperforming humans. However, this is still far from the general intelligence exhibited by humans, which involves a wide range of cognitive abilities and an understanding of broader context.Agency, on the other hand, refers to the capacity to make independent decisions and take actions based on one's intentions. It involves a level of consciousness, self-awareness, and free will that AI does not currently possess. While AI can make decisions based on programmed instructions and learned patterns, it does not have the subjective experiences or autonomous volition that characterizes human agency. AI doesn't have desires, fears, or aspirations.Wisdom goes a step beyond intelligence and agency. It involves the judicious application of knowledge, experience, and good judgment. Wisdom often implies a deep understanding of people, things, events or situations, resulting in the ability to choose or advise others to choose the best course of action. Wisdom is usually associated with attributes such as empathy, compassion, and ethics. As of now, AI lacks the ability to exhibit wisdom as it doesn't possess emotional intelligence, self-awareness, or the ability to understand complex human value systems.The implications of these distinctions for Super Intelligence are significant. Even if we were to develop an AI system that matches or surpasses human intelligence in a broad range of tasks, it would not necessarily possess agency or wisdom. Without these, a super intelligent AI might make decisions that are highly effective in achieving specified goals, but that fail to take into account broader human values, ethics, or potential long-term consequences. Therefore, as we continue to advance AI, it's crucial to consider not just how we can enhance its intelligence, but also how we can ensure it is used wisely and in a manner that aligns with human values and wellbeing. Some important perspectives on wisdom, AI alignment, the meaning crisis, and the future of humanity are also discussed by John Vervaeke in the following podcast: John Vervaeke: Artificial Intelligence, The Meaning Crisis, & The Future of Humanity. See also the section "What does it Mean to be Human and Living Meaningful in the 21st Century?" in Chapter 10 "Beneficial Outcomes for Humanity in the Smart Technology Era" of my book.Is Civilisation not already a Run-away Super Intelligent Super Organism on a problematic trajectory? How does this differ from a run-away AI Super Intelligence? Does AI accelerate the Super Organism's current trajectory? I think there is a good argument to be made that civilization is already a runaway super intelligent super organism on a problematic trajectory. We have the ability to create and use technology that is far more powerful than anything that has come before, and we are using this technology to rapidly change the world around us. This change is happening at an exponential rate, and it is difficult for us to keep up. We are not sure what the long-term consequences of this change will be, and there is a real risk that we could create a world that is uninhabitable or even destroy ourselves altogether.A runaway AI super intelligence would be a similar kind of threat, but it could be even more dangerous. Such an AI would be able to learn and adapt at an even faster rate than humans, and it would not be bound by the same ethical or moral constraints. This means that an AI super intelligence could potentially pose an existential threat to humanity.Before we delve deeper to address these questions, it is worth while to get Daniel Schmachtenberger's perspectives on our current civilisation's problematic trajectory as also referenced in Chapter 10 "Beneficial Outcomes for Humanity in the Smart Technology Era" of my book: "Daniel Schmachtenberger’s core interest is focused on long term civilization design and more specifically to help us as a civilization to develop improved sensemaking and meaning-making capabilities so that we can make better quality decisions to help unlock more of our potential and higher values that we are capable of. He has specifically done some work on surveying existential and catastrophic risks, advancing forecasting and mitigation strategies, synthesizing and advancing civilizational collapse and institutional decay models, as well as identifying generator functions that drive catastrophic risk scenarios and social architectures that lead to potential coordination failures. Generator functions include for example game theory related win-lose dynamics multiplied by exponential technology, damaged feedback loops, unreasonable or irrational incentives, and short term decision making incentives on issues with long term consequences. He believes that categorical solutions to these generator functions would address the causes for civilization collapse and function as the key ingredients for a new and robust civilization model that will be robust in a Smart Technology Era with destabilizing decentralized exponential technology. He summarizes his main sense of purpose is helping to transition civilization being on a current path that is self-terminating to one that is not and that is supportive of the possibility of purpose and meaning for everyone enduring into the future and working on changing the underlying structural dynamics that help make that possible. What he would like to see differently within the next 30 years is that we prevent existential risks that could play out in this time frame. It is not a given that we make it to 2050. Apart from catastrophic risks that can play out over this time period, there are those that can go past a tipping point during this time frame but will inevitably play out after that time. As we do not want to experience civilization collapse or existential risk and also not have us go past tipping points, Daniel would like to see a change in the trajectory that civilization is currently on from one that is on the path of many self-terminating scenarios each with their own set of chain reactions such as AI apocalypse, world war 3, climate change human-induced migration issues leading to resource wars, collapse of biodiversity, and killer drones."In a recent podcast "Artificial Intelligence and The Superorganism" | The Great Simplification" with Nate Hagens, Daniel Schmachtenberger gives further insights into AI's potential added risk to our global systems and planetary stability. Through a systems perspective, Daniel and Nate piece together the biophysical history that has led humans to this point, heading towards and beyond numerous planetary boundaries and facing geopolitical risks all with existential consequences. They specifically also ask:How does AI not only add to these risks, but accelerate the entire dynamic of the metacrisis? What is the role of intelligence versus wisdom on our current global pathway, and can we change course? Does AI have a role to play in creating a more stable system or will it be the tipping point that drives our current one out of control? As we can see from the above inputs, there is indeed an argument to be made that human civilization, especially when seen through the lens of collective decision-making and technological progress, could be viewed as a form of super intelligent organism. Much like the hypothetical super intelligent AI, our civilization possesses vast knowledge and problem-solving abilities. However, as Daniel Schmachtenberger points out, there are critical dynamics and structures within our civilization that could lead us towards self-destruction, analogous to the risks posed by an unchecked super intelligent AI.The key distinction here is that civilization is a complex system of independent, conscious agents with diverse interests and values. It's influenced by cultural, political, economic, and environmental factors, among others. A super intelligent AI, on the other hand, would be a single entity (or a unified system) driven by a specific set of programmed goals (for the scenario that it is not a distributed super intelligence). Both could potentially lead to harmful outcomes if not properly managed, but the nature of the risks and the strategies to mitigate them would differ significantly.The generator functions that Schmachtenberger identifies – like win-lose dynamics, damaged feedback loops, and irrational incentives – do seem to bear some similarities with potential risks from super intelligent AI. Both involve systems that could spiral out of control due to poorly aligned incentives, inadequate feedback mechanisms, and short-term decision-making that neglects long-term consequences.However, the solutions would need to be tailored to the specific systems. For human civilization, addressing these generator functions might involve deep structural changes to our economic and political systems, advances in education and moral reasoning, improvements in global governance and cooperation, and the adoption of long-term perspectives. For super intelligent AI, it might involve AI alignment and safety research, iterative and controlled development, transparency and explainability, human-AI collaboration, regulation and oversight, international collaboration, education and public engagement, and adaptability and learning.In both cases, achieving these changes would require a profound shift in our collective understanding, values, and priorities. We would need to move away from narrow, short-term, competitive mindsets and towards a broader, longer-term, cooperative perspective that values the wellbeing of all sentient beings and the sustainability of our shared environment.Super Intelligent AI as a Single, Unified Entity versus a Distributed Super IntelligenceThe traditional conception of super intelligent AI often involves a single, unified entity - largely because this makes the concept easier to understand and discuss. However, in reality, a super intelligent AI system could very well manifest as a distributed network of intelligent entities working together, akin to the idea of an "Ecosystem of Intelligence" or the "Spatial Web" as mentioned earlier in this article. This is often referred to as "collective intelligence" or "swarm intelligence."In this scenario, intelligence would not be concentrated within a single entity, but distributed across a multitude of AI agents, each potentially specializing in different tasks, but collectively capable of demonstrating super intelligence. This configuration could even integrate human intelligence into the mix, resulting in a human-AI collaborative network.These distributed networks of intelligence could have significant advantages over a singular super intelligent entity. They could be more resilient (since the loss or failure of individual agents wouldn't compromise the entire system), more flexible (since they could adapt to a wider range of problems and situations), and potentially safer (since no single agent would possess the full power of the super intelligent system).However, these distributed networks also present unique challenges. For example, coordinating the actions of multiple agents can be complex, and individual agents could potentially behave in ways that are harmful to the system as a whole. Furthermore, while such a system could potentially mitigate some risks associated with super intelligent AI (e.g., the risk of a single agent going rogue), it could also introduce new risks (e.g., the risk of emergent behaviors that are harmful or unpredictable).The vision presented earlier of an "Ecosystem of Intelligence" – a web of shared knowledge that evolves into wisdom – offers a more nuanced and optimistic vision of the future of AI, and it aligns well with the idea of AI as a tool for augmenting human intelligence and solving complex problems. However, like all visions of the future, it will require careful planning, management, and governance to ensure that it unfolds in a way that is beneficial and safe for all.How can Humanity ensure that Super Intelligence is beneficial and aligned with Human Values?Ensuring that a super intelligent AI aligns with human values and is used for good is a complex, multifaceted challenge. However, here's a potential plan that incorporates various strategies and steps to address this issue:Research on AI Alignment and Safety: Fundamental to the plan would be rigorous, ongoing research into AI alignment - ensuring that AI's goals are in tune with human values - and AI safety, to reduce the likelihood of harmful consequences. We would need to develop robust AI models that can understand and appropriately respond to the nuances of human values, ethics, and societal norms.Iterative and Controlled Development: Instead of trying to create a super intelligent AI in one step, it would be more prudent to develop AI iteratively. Each version of the AI could be slightly more capable than the last, with thorough testing and risk assessments at each stage. We could ensure safety measures, like "off switches" or containment procedures, are effective before moving on to the next development phase.Transparency and Explainability: AI systems should be designed to be transparent and explainable, so that humans can understand and predict their behaviors. This not only fosters trust but also enables ongoing oversight and intervention if necessary.Human-AI Collaboration: Rather than replacing humans, AI should augment human intelligence. Humans and AI should work together, with humans providing the wisdom, ethics, and strategic direction, and AI providing the computational power and precision.Regulation and Oversight: Clear regulations and strong oversight mechanisms should be in place to govern the development and deployment of AI. These should be designed to prevent misuse and to address ethical issues such as fairness, privacy, and autonomy.International Collaboration: The potential risks and impacts of super intelligent AI are global in nature, so the response should be global too. This could involve international agreements on safety and ethical standards, collaboration on research, and mechanisms for sharing benefits and managing risks.Education and Public Engagement: It's important to involve society in decisions about AI and its future. This could involve educating the public about AI, consulting on key decisions, and enabling people to influence the rules and norms that govern AI.Adaptability and Learning: As AI evolves, our strategies for managing it will need to evolve too. This could involve ongoing monitoring and assessment of AI impacts, learning from successes and failures, and adapting strategies as needed.The key to this plan's success would be its implementation in a comprehensive, coordinated way, involving all stakeholders - researchers, policymakers, businesses, civil society, and the public. As with any plan, it would also need to be revisited and revised regularly in the light of new developments and insights.The Debates on the AI SingularityIn conclusion, for a further deep dive into what various thought leaders, AI researchers, business leaders, scientists, and influencers think about the AI singularity, herewith an extract from Chapter 9 "The Debates, Progress and Likely Future Paths of Artificial Intelligence" of my book "Democratizing Artificial Intelligence to Benefit Everyone: Shaping a Better Future in the Smart Technology Era":"Prominent business leaders, scientists, and influencers such as Elon Musk, the late Stephen Hawking, Martin Rees, and Eliezer Yudkowsky have issued dreadful warnings about AI being an existential risk to humanity, whilst well-resourced institutes countering this doomsday narrative with their own “AI for Good” or “Beneficial AI” narrative. AI researcher and entrepreneur Andrew Ng has once said that “fearing a rise of killer robots is like worrying about overpopulation on Mars”.[i] That has also been countered by AI researcher Stuart Russell who said that a more suitable analogy would be “working on a plan to move the human race to Mars with no consideration for what we might breathe, drink, or eat once we arrive”.[ii] Many leading AI researchers seem to not identify with the existential alarmist view on AI, are more concerned about the short-to-medium term risks and challenges of AI discussed in the previous chapter, think that we are still at a very nascent stage of AI research and development, do not see a clear path to strong AI over the next few decades, and are of the opinion that the tangible impact of AI applications should be regulated, but not AI research and development. Most AI researchers and practitioners would fall into the beneficial AI movement and/or techno-sceptics category. Oren Etzioni, CEO of the Allen Institute for Artificial Intelligence, wrote an opinion article titled How to Regulate Artificial Intelligence where he claims that the alarmist view that AI is an “existential threat to humanity” confuses AI research and development with science fiction, but recognizes that there are valid concerns about AI applications with respect to areas such as lethal autonomous weapons, jobs, ethics and data privacy.[iii] From a regulatory perspective he proposes three rules that include that AI systems should be put through the full extent of the laws that apply to its human operator, must clearly reveal that they are not a human, and cannot keep or reveal confidential information without clear approval from the source of that information. Some strong technological utopian proponents include roboticist Hans Moravec as communicated in his book Mind Children: The Future of Robot and Human Intelligence as well as Ray Kurzweil, who is currently Director of Engineering at Google and has written books on the technology singularity, futurism, and transhumanism such as The Age of Spiritual Machines and The Singularity is Near: When Humans Transcend Biology.[iv] The concept of a technological singularity has been popular in many science fiction books and movies over the years. Some of Ray’s predictions include that by 2029 AI will reach human-level intelligence and that by 2045 "the pace of change will be so astonishingly quick that we won't be able to keep up, unless we enhance our own intelligence by merging with the intelligent machines we are creating".[v] There are a number of authors, AI thought leaders and computer scientists that have criticized Kurzweil's predictions in various degrees from both an aggressive timeline and real-world plausibility perspective. Some of these people include Andrew Ng, Rodney Brooks, Francois Chollet, Bruce Sterling, Neal Stephenson, David Gelernter, Daniel Dennett, Maciej Ceglowski, and the late Paul Allen. Web developer and entrepreneur Maciej Ceglowski calls superintelligence “the idea that eats smart people” and provides a range of arguments for this position in response to Kurzweil’s claims as well as Nick Bostrom’s book on Superintelligence and the positive reviews and recommendations that the book got from Elon Musk, Bill Gates and others.[vi] AI researcher and software engineer Francois Chollet wrote a blog on why the singularity is not coming as well as an article on the implausibility of an intelligence explosion. He specifically argues that a “hypothetical self-improving AI would see its own intelligence stagnate soon enough rather than explode” due to scientific progress being linear and not exponential as well as also getting exponentially harder and suffering diminishing returns even if we have an exponential growth in scientific resources. This has also been noted in the article Science is Getting Less Bang for its Buck that explores why great scientific discoveries are more difficult to make in established fields and notes that emergent levels of behavior and knowledge that lead to a proliferation of new fields with their own fundamental questions seems to be the avenue for science to continue as an endless frontier.[vii] Using a simple mathematical model that demonstrates an exponential decrease of discovery impact of each succeeding researcher in a given field, Francois Chollet concludes that scientific discovery is getting harder in a given field and linear progress is kept intact with exponential growth in scientific resources that is making up for the increased difficulty of doing breakthrough scientific research. He further constructs another model, with parameters for discovery impact and time to produce impact, which shows how the rate of progress of a self-improving AI converges exponentially to zero, unless it has access to exponentially increasing resources to manage a linear rate of progress. He reasons that paradigm shifts can be modeled in a similar way with the paradigm shift volume that snowballs over time and the actual impact of each shift decreasing exponentially which in turn results in only linear growth of shift impact given the escalating resources dedicated to both paradigm expansion and intra-paradigm discovery. Francois states that intelligence is just a meta-skill that defines the ability to gain new skills and should be along with hard work at the service of imagination, as imagination is the real superpower that allows one to work at the paradigm level of discovery.[viii] The key conclusions that Francois makes in his article on implausibility of an intelligence explosion are firstly that general intelligence is a misnomer as intelligence is actually situational in the sense that the brain operates within a broader ecosystem consisting of a human body, an environment, and a broader society. Furthermore, the environment is putting constraints on individual intelligence which is limited by its context within the environment. Most of human intelligence is located in a broader self-improving civilization intellect where we live and that feeds our individual brains. The progress in science by a civilization intellect is an example of a recursively self-improving intelligence expansion system that is already experiencing a linear rate of progress for reasons mentioned above.[ix] In the essay The Seven Deadly Sins of Predicting the Future of AI, Rodney Brooks who is the co-founder of iRobot and Rethink Robotics, firstly quotes Amar’s law that “we tend to overestimate the effect of a technology in the short run and underestimate the effect in the long run” to state that the long term timing for AI is being crudely underestimated.[x] He also quotes Arthur C. Clarke’s third law that states that “any sufficiently advanced technology is indistinguishable from magic” to make the point that arguments for a magical future AI are faith-based and when things said about AI that are far enough from what we use and understand today and for practical purposes passes the magic line, those things cannot be falsified. As it is also intuitive for us to generalize from the observed performance level on a particular task to competence in related areas, it is also natural and easy for us to apply the same human style generalizations to current AI systems that operate in extremely narrow application areas and overestimate their true competence level. Similarly, people can easily misinterpret suitcase words applied to AI systems to mean more than what there actually is. Rodney also argues that as exponentials are typically part of a S-curve where hyper growth flattens out, one should in general be careful to apply exponential arguments as it can easily collapse when a physical limit is hit or if there is not sufficient economic value to persist with it. The same holds for AI, where deep learning’s success, which can also be seen as an isolated event and achieved on top of at least thirty years of machine learning research and applications, does not necessarily guarantee similar breakthroughs on a regular basis. Not only is the future reality of AI likely to be significantly different to what is being portrayed in Hollywood science fiction movies, but also have a variety of advanced intelligent systems that evolve technologically over time in a world that would be adapting to these systems. The final error being made when predicting the future of AI is that the speed of deploying new ideas and applications in robotics and AI take longer than people think, especially when hardware is involved as with self-driving cars or in many factories around the world that are still running decades-old equipment along with old automation and operating system software.[xi] On the self-driving cars front both Tesla and Google’s Waymo have improved self-driving technology significantly with Waymo achieving “feature complete” status in 2015 but in geo-fenced areas, whereas Tesla is at almost zero interventions between home and work (with an upcoming software release promising to be a “quantum leap”) in 2020.[xii] However, the reality is that Tesla’s full driving Autopilot software is progressing much slower than what Elon Musk predicted over the years and Chris Urmson, the former leader of Google self-driving project and CEO of self-driving startup Aurora, reckons that driverless cars will be slowly integrated over the next 30 to 50 years.[xiii] Piero Scaruffi, a freelance software consultant and writer, is even more of a techno-skeptic and wrote in Intelligence is not Artificial - Why the Singularity is not coming any time soon and other Meditations on the Post-Human Condition and the Future of Intelligence that his estimate for super intelligence that can be a “substitute for humans in virtually all cognitive tasks, including those requiring scientific creativity, common sense, and social skills” to be approximately 200,000 years which is the time scale of natural evolution to produce a new species that will be at least as intelligent as us.[xiv] He does not think that we’ll get to strong AI systems with our current incremental approach and that the current brute-force AI approach is actually slowing down research in higher-level intelligence. He guesses that an AI breakthrough will likely have to do with real memory that have “recursive mechanisms for endlessly remodeling internal states”. Piero disagrees with Ray Kurzweil’s “Law of Accelerating Returns” and points out that the diagram titled “Exponential Growth in Computing” is like comparing the power of a windmill to the power of a horse and concluding that windmills will keep improving forever. There is also no differentiation between progress in hardware versus progress in software and algorithms. Even though there has been significant progress in computers in terms of its speed, size, and cost-effectiveness, that does not necessarily imply that we will get to human-level intelligence and then super intelligence by assembling millions of superfast GPUs. A diagram showing “Exponential Growth in Computational Math” would be more relevant and will show that there has been no significant improvement in the development of abstract algorithms that improve automatic learning techniques. He is much more impressed with the significant progress in genetics since the discovery of the double-helix structure of DNA in 1953 and is more optimistic that we will get to superhuman intelligence through synthetic biology.[xv]A survey taken by the Future of Life Institute says we are going to get strong AI around 2050, whereas one conducted by SingularityNET and GoodAI at the 2018 Joint Multi-Conference on Human-Level AI shows that 37% of respondents believe human-like AI will be achieved within five to 10 years, 28% of respondents expected strong AI to emerge within the next two decades while only 2% didn't believe humans will ever develop strong AI.[xvi] Ben Goertzel, SingularityNET's CEO and developer of the software behind a social, humanoid robot called Sophia, said at the time that "it's no secret that machines are advancing exponentially and will eventually surpass human intelligence" and also “as these survey results suggest, an increasing number of experts believe this 'Singularity' point may occur much sooner than is commonly thought… It could very well become a reality within the next decade."[xvii] Lex Fridman, AI Researcher at MIT and YouTube Podcast Host thinks that we are already living through a singularity now and that super intelligence will arise from our human collective intelligence instead of strong AI systems.[xviii] George Hotz, a programmer, hacker, and the founder of Comma.ai also thinks that we are in a singularity now if we consider the escalating bandwidth between people across the globe through highly interconnected networks with increasing speed of information flow.[xix] Jürgen Schmidhuber, AI Researcher and Scientific Director at the Swiss AI Lab IDSIA, is also very bullish about this and that we soon should have cost-effective devices with the raw computational power of the human brain and decades after this the computational power of 10 billion human brains together.[xx] He also thinks that we already know how to implement curiosity and creativity in self-motivated AI systems that pursue their own goals at scale. According to Jürgen superintelligent AI systems would likely be more interested in exploring and transforming space and the universe than being restricted to Earth. AI Impacts has an AI Timeline Surveys web page that documents a number of surveys where the medium estimates for a 50% chance of human-level AI vary from 2056 to at least 2106 depending on the question framing and the different interpretations of human-level AI, whereas two others had medium estimates at the 2050s and 2085.[xxi] Rodney Brooks has declared that artificial general intelligence has been “delayed” to 2099 as an average estimate in a May 2019 post that references a survey done by Martin Ford via his book Architects of Intelligence where he interviewed 23 of the leading researchers, practitioners and others involved in the AI field.[xxii] It is not surprising to see Ray Kurzweil and Rodney Brooks at opposite ends of the timeline prediction, with Ray at 2029 and Rodney at 2200. Whereas Ray is a strong advocate of accelerating returns and believe that a hierarchical connectionist based approach that incorporates adequate real-world knowledge and multi-chain reasoning in language understanding might be enough to achieve strong AI, Rodney thinks that not everything is exponential and that we need a lot more breakthroughs and new algorithms (in addition to back propagation used in Deep Learning) to approximate anything close to what biological systems are doing especially given the fact that we cannot currently even replicate the learning capabilities, adaptability or the mechanics of insects. Rodney reckons that some of the major obstacles to overcome include dexterity, experiential memory, understanding the world from a day-to-day perspective, comprehending what goals are and what it means to make progress towards them. Ray’s opinion is that techno-sceptics are thinking linearly, suffering from engineer’s pessimism and do not see exponential progress in software advances and cross fertilization of ideas. He believes that we will see strong AI progresses exponentially in a soft take off in about 25 years." [i] https://www.theregister.com/2015/03/19/andrew_ng_baidu_ai/ [ii] Stuart Russell, Human Compatible: Artificial Intelligence and the Problem of Control. [iii] https://www.nytimes.com/2017/09/01/opinion/artificial-intelligence-regulations-rules.html?ref=opinion [iv] Hans Moravec, Mind Children: The Future of Robot and Human Intelligence; Ray Kurzweil, The Age of Spiritual Machines; Ray Kurzweil, The Singularity is Near: When Humans Transcend Biology. [v] Ray Kurzweil, The Singularity is Near: When Humans Transcend Biology. [vi] https://idlewords.com/talks/superintelligence.htm [vii] https://www.theatlantic.com/science/archive/2018/11/diminishing-returns-science/575665/ [viii] https://fchollet.com/blog/the-singularity-is-not-coming.html [ix] https://medium.com/@francois.chollet/the-impossibility-of-intelligence-explosion-5be4a9eda6ec [x] https://rodneybrooks.com/the-seven-deadly-sins-of-predicting-the-future-of-ai/ [xi] https://rodneybrooks.com/the-seven-deadly-sins-of-predicting-the-future-of-ai/ [xii] https://arstechnica.com/cars/2020/08/teslas-slow-self-driving-progress-continues-with-green-light-warning [xiii] https://www.theringer.com/tech/2019/5/16/18625127/driverless-cars-mirage-uber-lyft-tesla-timeline-profitability; [xiv] https://www.scaruffi.com/singular/download.pdf [xv] https://www.scaruffi.com/singular/download.pdf [xvi] https://futureoflife.org/superintelligence-survey/?cn-reloaded=1; https://bigthink.com/surprising-science/computers-smart-as-humans-5-years?rebelltitem=1#rebelltitem1 [xvii] https://bigthink.com/surprising-science/computers-smart-as-humans-5-years?rebelltitem=1#rebelltitem1 [xviii] https://youtu.be/Me96OWd44q0 [xix] https://youtu.be/_L3gNaAVjQ4 [xx] https://spectrum.ieee.org/computing/software/humanlevel-ai-is-right-around-the-corner-or-hundreds-of-years-away [xxi] https://aiimpacts.org/ai-timeline-surveys/ [xxii] https://rodneybrooks.com/agi-has-been-delayed/Democratizing Artificial Intelligence to Benefit Everyone: Shaping a Better Future in the Smart Technology EraKindle and Paperback versions of the book is available on Amazon https://www.amazon.com/Democratizing-Artificial-Intelligence-Benefit-Everyone-ebook/dp/B08ZYW9487/.A PDF version is available on jacquesludik.com.The Audio book version is available on most major audio book market places (e.g., https://www.audible.com/pd/Democratizing-Artificial-Intelligence-to-Benefit-Everyone-Audiobook/B09LNL4JHC) .Listen also to the audio version on https://www.youtube.com/c/JacquesLudik and all major podcast channels (see links on jacquesludik.com)Democratizing Artificial Intelligence to Benefit Everyone: Shaping a Better Future in the Smart Technology Era" takes us on a holistic sense-making journey and lays a foundation to synthesize a more balanced view and better understanding of AI, its applications, its benefits, its risks, its limitations, its progress, and its likely future paths. Specific solutions are also shared to address AI’s potential negative impacts, designing AI for social good and beneficial outcomes, building human-compatible AI that is ethical and trustworthy, addressing bias and discrimination, and the skills and competencies needed for a human-centric AI-driven workplace. The book aims to help with the drive towards democratizing AI and its applications to maximize the beneficial outcomes for humanity and specifically arguing for a more decentralized beneficial human-centric future where AI and its benefits can be democratized to as many people as possible. It also examines what it means to be human and living meaningful in the 21st century and share some ideas for reshaping our civilization for beneficial outcomes as well as various potential outcomes for the future of civilization. See also the Democratizing AI Newsletter: https://www.linkedin.com/newsletters/democratizing-ai-6906521507938258944/Human Intelligence versus Machine Intelligence (https://www.linkedin.com/pulse/human-intelligence-versus-machine-jacques-ludik/) The Power of Generative AI: Exploring its Impact, Applications, Limitations, and Future (https://www.linkedin.com/pulse/power-generative-ai-exploring-its-impact-applications-jacques-ludik/)AI’s Impact on Society, Governments, and the Public Sector (https://www.linkedin.com/pulse/ais-impact-society-governments-public-sector-jacques-ludik)The Debates, Progress and Likely Future Paths of Artificial Intelligence (https://www.linkedin.com/pulse/debates-progress-likely-future-paths-artificial-jacques-ludik)Beneficial Outcomes for Humanity in the Smart Technology Era (https://www.linkedin.com/pulse/beneficial-outcomes-humanity-smart-technology-era-jacques-ludik)Democratizing AI to Help Shape a Beneficial Human-centric Future (https://www.linkedin.com/pulse/democratizing-ai-help-shape-beneficial-human-centric-future-ludik)References [i] https://www.forbes.com/sites/cognitiveworld/2019/06/19/7-types-of-artificial-intelligence/#5866ec28233e Democratizing AI Democratizing AI 3,200 followers + Subscribe Like Comment Copy LinkedIn Facebook Twitter Share 60 6 Comments Rogel I. Nuguid, MA, MAS, MS UN system, Interculturalist, International Cooperation, Sustainability, CSR, PhD researcher on SIDS and digital diplomacy, Research Administration 8mo Report this comment Dr. Jacques Ludik thank you very much for sharing this. The book has so much contribution to my PhD dissertation. Could you kindly confirm if there’s relevant chapter that would point to AI and its impact to the peoples and institutions of the Small Island Developing States? I am looking at the research topic of digital diplomacy, multilateralism and SIDS. Thank you! Like Reply 1 Reaction 2 Reactions Stanley Rorke Capital Raise Specialist - Author - Speaker - Moderator - Researcher 8mo Report this comment Great article Jacques Like Reply 1 Reaction 2 Reactions Jack Hakimian AI | Digital | Consulting 8mo Report this comment Great article. Thank you also for providing sources to debates on AI singularity. Like Reply 1 Reaction 2 Reactions See more comments To view or add a comment, sign in More articles by this author No more previous content Intelligent Agents, AGI, Active Inference and the Free Energy Principle Jan 13, 2024 AI and Web3: The Next Generation of the Internet for a Decentralized World Jul 21, 2023 Igniting the African AI Ecosystem May 21, 2023 Human Intelligence versus Machine Intelligence May 7, 2023 Quantum Computing for Generative AI May 6, 2023 The Power of Generative AI: Exploring its Impact, Applications, Limitations, and Future Mar 17, 2023 Sapiens, the Decentralized Human-centric User-controlled AI-driven Super Platform May 8, 2022 Democratizing AI to Help Shape a Beneficial Human-centric Future May 1, 2022 Beneficial Outcomes for Humanity in the Smart Technology Era Apr 24, 2022 The Debates, Progress and Likely Future Paths of Artificial Intelligence Apr 18, 2022 No more next content See all Others also viewed What is Conversational AI? SmartAction 9mo The real reason why social media keeps us hooked! Pablo Lopez 11mo Loyalty is rare, if you find it, keep it. Sebastian Gutierrez 9y The Current Limits of Artificial Intelligence and Future Perspectives 𝗪𝗜𝗹𝗹𝗶𝗮𝗻 𝗼𝗹𝗶𝘃𝗲𝗶𝗿𝗮 𝗴𝗶𝗯𝗶𝗻 4mo The concept of a "mind uploading" and its potential implications for human consciousness & identity Frank B. Prempeh II 1y Navigating the AI Era: Strategies for Cognitive Resilience and Enhancement Salwa Abdulla 1mo Will AI ever reach Singularity? Dr. Mahboob Khan 1w How Successful People Stay Calm Dr. Mahboob Khan 1w Importance of a Rehabilitation Center Dr. Mahboob Khan 1w Senior Informaticist, Revenue Cycle International Medical Center 1w Show more Show less Explore topics Sales Marketing Business Administration HR Management Content Management Engineering Soft Skills See All LinkedIn © 2024 About Accessibility User Agreement Privacy Policy Cookie Policy Copyright Policy Brand Policy Guest Controls Community Guidelines العربية (Arabic) Čeština (Czech) Dansk (Danish) Deutsch (German) English (English) Español (Spanish) Français (French) हिंदी (Hindi) Bahasa Indonesia (Indonesian) Italiano (Italian) 日本語 (Japanese) 한국어 (Korean) Bahasa Malaysia (Malay) Nederlands (Dutch) Norsk (Norwegian) Polski (Polish) Português (Portuguese) Română (Romanian) Русский (Russian) Svenska (Swedish) ภาษาไทย (Thai) Tagalog (Tagalog) Türkçe (Turkish) Українська (Ukrainian) 简体中文 (Chinese (Simplified)) 正體中文 (Chinese (Traditional)) Language

Titel: What Is Technological Singularity? | Built In

What Is Technological Singularity? | Built In Skip to main content Artificial IntelligenceArtificial IntelligenceWhat Is Technological Singularity?Some experts say the singularity is near. But what does that mean, really?Written byBrooke BecherPublished on Oct. 17, 2023Brooke BecherStaff Reporter at Built InBrooke Becher is a Built In staff reporter covering hardware and robotics. Previous topics include fintech and blockchain technologies, like crypto and Web3. Image: Shutterstock Technological singularity, also called the singularity, refers to a theoretical future event at which computer intelligence surpasses that of humans.The term ‘singularity’ comes from mathematics and refers to a point that isn’t well defined and behaves unpredictably. At this inflection point, a runaway effect would hypothetically set in motion, where superintelligent machines become capable of building better versions of themselves at such a rapid rate that humans would no longer be able to understand or control them. The exponential growth of this technology would mark a point of no return, fundamentally changing society as we know it in unknown and irreversible ways.What Is Technological Singularity?Technological singularity refers to a theoretical future event where rapid technological innovation leads to the creation of an uncontrollable superintelligence that transforms civilization as we know it. Machine intelligence becomes superior to that of humans, resulting in unforeseeable outcomes.According to John von Neumann, pioneer of the singularity concept, if machines were able to achieve singularity, then “human affairs, as we know them, could not continue.”Exactly how or when we arrive at this era is highly debated. Some futurists regard the singularity as an inevitable fate, while others are in active efforts to prevent the creation of a digital mind beyond human oversight. Currently, policymakers across the globe are brainstorming ways to regulate AI developments. Meanwhile, more than 1,000 tech leaders collectively called for a pause on all AI lab projects that could outperform OpenAI’s GPT-4 chatbot, citing “profound risks to society and humanity.”Only time will tell if these roadblocks are enough to derail tech progression, tripping up AI’s race to a superintelligence and delaying the singularity altogether.Related ReadingThe Future of AI: How Artificial Intelligence Will Change the World What Are the Implications of Technological Singularity?Brought on by the exponential growth of new technologies — specifically artificial intelligence and machine learning — the technological singularity could, on one hand, automate scientific innovation and evolutionary progress faster than humanly possible, turning out Nobel-Prize level ideas in a matter of minutes. This best-case scenario would further merge man and machine, augmenting the mind with non-biological, computerized tools the same way a prosthetic limb would become part of the body. We would be able to heighten the human experience on every desirable level, grasping a better understanding of ourselves and, in the process, the universe at large.On the other hand, the singularity could lead to human extinction. Based on our knowledge of how existing intelligent life (like humans) have treated less intelligent life forms (like lab rats, pigs raised for slaughter and chimps in cages), superintelligent machines may devalue humans as they become the dominant species.Whatever their plans may be, superintelligent machines would need local matter to start building a post-human civilization, “including atoms we are made out of,” according to Roman Yampolskiy, a computer scientist and associate professor at the University of Louisville whose writings have contributed to the conversation on singularity.All that would need to bring about this “very dangerous technology,” Yampolskiy said, is for progress within AI development to continue as is.Find out who's hiring.See jobs at top tech companies & startupsView All Jobs Is Technological Singularity Likely to Happen?Some experts think the technological singularity is inevitable. Futurist Ray Kurzweil went so far as to set the date for 2045. But a singularity event is still only hypothetical, and the likelihood of machines matching their makers depends on a few key factors.Consider the long-time standard of tech’s exponential growth named Moore’s Law. In his original paper published in 1965, Intel co-founder Gordon Moore predicted that computer power would double every two years as hardware downsized. Despite inescapable quantum limitations, that theory has impressively held up over the years. The process of tech progression is one that is additive, and can keep expanding even at an accelerated pace. In contrast, human brain power hasn’t changed for millenia, and will always be limited in its physical, biological capacity.Another indicator of society’s proximity to the singularity is the overall advancement of AI. Researchers rely on Turing Tests to demonstrate a computer’s intelligence. During these assessments, a machine must fool a panel of judges by mimicking human-like responses in a line of questioning. Only recently was this reportedly achieved for the first time by an AI chatbot that impersonated a 13-year-old boy named Eugene Goosteman in 2014.Tech powerful enough to bring about the singularity must also be bankrolled, and despite public concern, interest in AI from both private and public sectors is only growing. More recent projects, like Microsoft’s ChatGPT and Tesla’s self-driving cars, regularly make headlines as tech startups fuel the AI boom with deep pockets. Currently valued at $100 billion, the AI market is projected to grow twentyfold by 2030, up to nearly two trillion U.S. dollars, according to market research firm Next Move Strategy Consulting.But whether this all amounts to a phenomenon capable of upending society is still up for debate. Drastic tech advancement in the near future is a given, said Toby Walsh, a leading AI researcher who is currently building AI safeguards alongside governmental organizations, but whether it warrants the singularity is doubtful.“It would be conceited to suppose we won’t get to artificial superintelligence; we are just machines ourselves after all.”“It would be conceited to suppose we won’t get to artificial superintelligence; we are just machines ourselves after all,” Walsh, the chief scientist at the University of New South Wales AI Institute, told Built In. “But it may be that we get to artificial superintelligence in the old fashioned way — by human sweat and ingenuity — rather than via some technological singularity.”Not all experts think the singularity is bound to happen. Some, like Mark Bishop, a professor emeritus of cognitive computing at Goldsmiths, University of London, reject claims that computers can ever achieve human-level understanding, let alone surpass it. In fact, he considers AI to be downright stupid. Bishop subscribes to the Chinese Room Argument, a thought experiment proposed by philosopher John Searle. It states that while machines can be programmed to imitate human beings, such as behaviors and dialogue measured in a Turing Test, this does not demonstrate understanding. Machines, bound by finite integers, trying to simulate infinite, experiential aspects of a physical world just doesn’t add up.Even if a robot were to perfectly replicate these sensations, then, according to Bishop’s ‘Dancing with Pixies’ theory, we would have to accept that all things can become conscious of all possible phenomenal states — an arguably absurd can of worms known as panpsychism.“We’re living in a massive hype cycle where people, for commercial reasons, grossly over-inflate what these systems can do.”As someone who has been in AI labs for half the time AI labs have even existed, Bishop sees the current buzz as one of the many passing fads he’s witnessed during his career.“We’re living in a massive hype cycle where people, for commercial reasons, grossly over-inflate what these systems can do,” Bishop, who is currently building an AI-enhanced fraud detection platform at FACT360, said. “Now, there’s vast amounts of money involved in AI, and that hasn’t historically always been the case .… It has an effect on the public that wrongly overstates what AI can deliver.”Related ReadingArtificial Intelligence vs. Machine Learning vs. Deep Learning: What’s the Difference? What Happens After the Technological Singularity?By definition, the aftermath of the technological singularity will be incomprehensible to humans. The density at which tech would have to accelerate in order to reach the singularity means that the principles we’ve always gone by implode, and no longer make sense.“People can tell you how they would try to take over the world, but we have no idea how a superintelligence would go about it,” said Yampolskiy, noting the unpredictable nature of AI. “Like a mouse who has no concept of a mouse trap.”So while a superintelligence might understand a final goal, it lacks the implied common sense needed to get there. For example, if a superintelligent system is programmed to ‘eliminate cancer,’ Yampolskiy explained that it could either develop a cure or kill everyone with a positive cancer diagnosis.“If you don’t have common sense — why is one better than the other?” he said.But that doesn’t automatically defer to a doomsday dystopia. Keep in mind these systems are algorithmically based machines that were at one point programmed by humans. This process, as computationally sound as it may be, is still influenced by human semantics and nuance. So if a “self-improving” machine takes shape, it would need to determine what exactly self improvement looks like. For example, a simulated brain may interpret self improvement as pleasure, Yampolskiy said, and solely focus on optimizing its rewards channel ad infinitum.“There is no upper limit to pleasure, just as there is no upper limit to intelligence,” Yampolskiy added. “We can get lucky; instead of destroying us, AI may choose to go out and explore the universe.”Related Reading7 Types of Artificial IntelligenceFind out who's hiring.See jobs at top tech companies & startupsView All Jobs An error occurred.Unable to execute JavaScript. Try watching this video on www.youtube.com, or enable JavaScript if it is disabled in your browser.This brief explainer summarizes the technological singularity in a nutshell. | Video: National GeographicDifferent Views of Technological SingularityAmong futurists, there are different types of imagined singularities. See how some of the field’s standout thought leaders vary in their interpretations of a superhuman intelligence below. John von Neumann, 1958John von Neumann was a gifted mathematician who invented game theory and helped lay the groundwork for the modern computer. But it wasn’t until after his death that Neumann became associated with technological singularity, when his colleague, nuclear physicist Stanislaw Ulam, credited him for the idea in a posthumous tribute. In it, Ulam paraphrased a conversation between the two, where Neumann first posits the singularity as a phenomenon brought on by the “ever accelerating progress of technology” that would impact the mode of human life and change it forever. Irving J. Good, 1965Good, a cryptologist, worked as a code breaker at Bletchley Park alongside Alan Turing during the second World War. In his 1965 paper, “Speculations Concerning the First Ultraintelligent Machine,” he regards the eventual creation of an ultraintelligent machine as “the last invention that man need ever make.” He’s known for describing the singularity as an “intelligence explosion,” as it would initiate an autonomous self-assembly cycle. Good believed that the singularity would transform society for the better. He also proposed that these machines would replicate human brain function by developing neural circuits. Vernor Vinge, 1993Computer scientist and sci-fi author Vernor Vinge sees the singularity as a fast acceleration into humanity’s downfall, on par in importance to man’s biological ascent from the animal kingdom. In his 1993 paper “The Coming Technological Singularity,” Vinge predicts a few different scenarios that could bring on such events. He believes that computers may one day “wake up” as AI develops, either one at a time or in the form of large, interconnected networks. Another possibility is in the form of computer-human interfaces, where machines become so infused with human users over time that they can be considered superhumanly intelligent. Even further, scientists and researchers may take an internal approach to enhance human intellect, implanting tech inside humans themselves via bioengineering. The technology needed to trigger the singularity would be available by 2030, according to Vinge; but whether we choose to partake in our own demise is not a given. Ray Kurzweil, 2005Futurist and pioneer of pattern-recognition technologies, Ray Kurzweil sees singularity as a mind-expanding merger of man and machine. In his book, The Singularity Is Near, he describes this as an “unparalleled human-machine synthesis” where computers become an empowering appendage to humans, enabling us to offload cognitive abilities to aid our further expansion. This “transhuman” superintelligence is built through positive feedback loops that are then repeated and reinforced at each stage of development across emerging tech — specifically computers, genetics, nanotechnology, robots and artificial intelligence. Toby Walsh, 2017Walsh offers a “less dramatic” outcome in lieu of the singularity in his 2017 paper “The Singularity May Never Be Near.” Rather than an autonomous-AI-bot uprising, Walsh imagines any future tech capable of superintelligence will still be primarily man-made, and developed slowly over time. To be clear, Walsh doesn’t doubt the possibility of a computerized superhuman intelligence; however, one powerful enough to self-animate and incite a civilization-upending singularity is less believable.“[A technological singularity] doesn’t violate any laws of physics that we know about, so we can’t say it is impossible,” Walsh said. “But there are half a dozen reasons why it might never happen, or why it might be a long, long time coming.” Frequently Asked QuestionsWhat is an example of technological singularity?As AI advances, supercomputer networks may match human intelligence, at which point they could “wake up” and build better machines, creating superintelligence that surpasses humans in the process. Man and machine could also merge via human-computer interfaces, where superintelligent systems are used to augment the human experience and expand upon our innate cognitive abilities.Is technological singularity possible?Experts are divided on the possibility of the singularity. Some believe it is inevitable and only a matter of time. Futurist Ray Kurzweil, for instance, thinks the technological singularity will happen before 2045. Artificial Intelligence Great Companies Need Great People. That's Where We Come In.Recruit With Us Built In is the online community for startups and tech companies. Find startup jobs, tech news and events. About Our Story Careers Our Staff Writers Content Descriptions Company News Get Involved Recruit With Built In Subscribe to Our Newsletter Become an Expert Contributor Send Us a News Tip Resources Customer Support Share Feedback Report a Bug Tech A-Z Browse Jobs Tech Hubs Built In Austin Built In Boston Built In Chicago Built In Colorado Built In LA Built In NYC Built In San Francisco Built In Seattle See All Tech Hubs © Built In 2024 Learning Lab User Agreement Accessibility Statement Copyright Policy Privacy Policy Terms of Use Your Privacy Choices/Cookie Settings CA Notice of Collection

Titel: How Far Are We from AI Singularity? What It Means & Implications 

How Far Are We from AI Singularity? What It Means & Implications Skip to content English : Select a language 日本語 Deutsch English Español Português Français High Contrast Log in Start free or get a demo Menu Search... Blogs Blogs Trusted by business builders worldwide, the HubSpot Blogs are your number-one source for education and inspiration. Marketing Resources and ideas to put modern marketers ahead of the curve Sales Strategies to help you elevate your sales efforts Service Everything you need to deliver top-notch customer service Website Tutorials and how-tos to help you build better websites The Hustle The insights you need to make smarter business decisions Next in AI Your essential daily read on all things AI and business. See all blogs Explore by topic Instagram Marketing Instagram Marketing Customer Retention Customer Retention Email Marketing Email Marketing SEO SEO Sales Prospecting Sales Prospecting Newsletters Newsletters All of HubSpot's handcrafted email newsletters, tucked in one place. The Hustle Irreverent and insightful takes on business and tech, delivered to your inbox Videos Videos Browse our collection of educational shows and videos on YouTube. The Hustle Our unrivaled storytelling, in video format. Subscribe for little revelations across business and tech Marketing with HubSpot Learn marketing strategies and skills straight from the HubSpot experts My First Million When it comes to brainstorming business ideas, Sam and Shaan are legends of the game Marketing Against the Grain Watch two cerebral CMOs tackle strategy, tactics, and trends HubSpot Everything you need to know about building your business on HubSpot See all videos Podcasts Podcasts HubSpot Podcast Network is the destination for business professionals who seek the best education on how to grow a business. My First Million Each week, hosts Sam Parr and Shaan Puri explore new business ideas based on trends and opportunities in the market Goal Digger Redefining what success means and how you can find more joy, ease, and peace in the pursuit of your goals The Hustle Daily Show A daily dose of irreverent, offbeat, and informative takes on business and tech news Another Bite Each week, Another Bite breaks down the latest and greatest pitches from Shark Tank Business Made Simple Build your business for far and fast success Marketing Against the Grain HubSpot CMO Kipp Bodnar and Zapier CMO Kieran Flanagan share what's happening now in marketing and what's ahead Online Marketing Made Easy Online Marketing Made Easy The Product Boss The Product Boss Nudge Nudge Side Hustle Pro Side Hustle Pro Outbound Squad Outbound Squad See all podcasts Resources Resources Expand your knowledge and take control of your career with our in-depth guides, lessons, and tools. Academy Learn and get certified in the latest business trends from leading experts Templates Interactive documents and spreadsheets to customize for your business's needs Ebooks In-depth guides on dozens of topics pertaining to the marketing, sales, and customer service industries Kits Multi-use content bundled into one download to inform and empower you and your team Tools Customized assets for better branding, strategy, and insights Search... HubSpot Products The HubSpot Customer Platform All of HubSpot's marketing, sales, customer service, CMS, operations, and commerce software on one platform. See pricing Free HubSpot CRM Overview of all products Marketing Hub Marketing automation software. Free and premium plans Sales Hub Sales CRM software. Free and premium plans Service Hub Customer service software. Free and premium plans CMS Hub Content management software. Free and premium plans Operations Hub Operations software. Free and premium plans Commerce Hub B2B commerce software. Free and premium plans About HubSpot Contact Us Customer Support Start free or get a demo Contact Sales About HubSpot Contact Us Customer Support Log in Start free or get a demo English : Select a language 日本語 Deutsch English Español Português Français High Contrast Log in Hubspot Blog HubSpot.com Loading Oh no! We couldn't find anything like that.Try another search, and we'll give it our best shot. Load More Results How Far Are We from AI Singularity? What It Means & Implications Tristen Taylor Published: May 19, 2023 If you're working in a marketing role, you can't ignore the emerging field of artificial intelligence. It's quickly gained traction, and industry leaders have begun projecting that rapid AI advancements could surpass human intelligence sooner than we think. But what does that mean? And what are the implications of that type of advancement? As a marketer, it can be challenging to imagine where AI technology will go. However, understanding AI singularity, including its implications and potential outcomes, can help you stay ahead of the curve in an ever-changing profession. So before we discuss AI singularity in modern-day terms, let's talk about the original or blanket term — technological singularity. Technological Singularity Technological singularity is the idea that, at some point in the future, technology will become so advanced that it surpasses human intelligence and control, and was first coined in 1983 by the author Vernor Vinge. So how can we visualize this concept with today's AI technology? Let's dive into it. What is AI singularity? AI singularity is a hypothetical concept where artificial intelligence exceeds human intelligence. In simpler terms, if computers surpass humans’ cognitive abilities, a new level of intelligence will emerge that is unachievable by people. AI singularity involves the creation of an artificial intelligence that's so advanced, it can continuously improve upon itself at a rate that's faster than humans performing the same task. This rapid self-improvement leads to an intelligence explosion, with AI algorithms becoming more complex and powerful at an exponential rate, exceeding the cognitive capabilities of individuals. While it sounds useful for technology to conduct tasks and operations that humans haven't achieved, it could have far-reaching implications for how we approach AI and its applications. AI Singularity Implications Assuming AI singularity occurs, the implications for businesses and, generally, any technology company might be significant. AI entities that are more intelligent than humans could self-replicate and self-improve, resulting in a chain of increasingly more advanced AI machines. That improvement and development of AI would cause a dramatic shift in the labor market. Machines could increasingly displace human workers, particularly those who have engaged in routine and manual labor, while the demand for more sophisticated high-tech skills would continue. How Close Are We to AI Singularity? There have been significant advances in technology, including machine learning algorithms that have shown promising results in problem-solving applications. However, given the complex and multifaceted nature of human intelligence, creating a fully autonomous AI entity that exceeds human intelligence still seems like a long-term goal. However, futurist and computer scientist Ray Kurzweil has made predictions that singularity will approximately occur in 2045, while others speculate the tipping point is earlier than that. Life After Singularity It's difficult to predict precisely what life will look like beyond the point of singularity. The risk is that implementing increasingly advanced AI machine learning could affect its actions and decisions, including those that could be unpredictable, poorly made, or even harmful to the world. The benefits of singularity, however, cannot be overstated. AI singularity could play a significant role in helping to solve many of society's most vital problems, and even alleviate some of the day-to-day pressures that come with a busy, productive working population. But once again, we'll only speculate as we continue to ingrain our lives and work with AI technology. Topics: Artificial Intelligence Don't forget to share this post! Related Articles prev next AI in Digital Marketing — The Complete Guide Mar 07, 2024 I Tried 10 AI Project Management Tools to See if They’re Worth It (Results & Recommendations) Feb 14, 2024 11 Artificial Intelligence Examples from Real Brands in 2023 Dec 22, 2023 The Complete Guide to AI for Amazon Sellers in 2024 Dec 19, 2023 What's Holding AI Adoption Back in Marketing? [New Data] Dec 18, 2023 10 Challenges Marketers Face When Implementing AI in 2023 [New Data + Tips] Dec 11, 2023 How AI Can Improve Your Customer Experience [New Data + Tips] Dec 08, 2023 The Complete Guide to AI Transparency [6 Best Practices] Dec 05, 2023 AI Chatbots: Our Top 19 Picks for 2024 Nov 29, 2023 AI Marketing Campaigns Only a Bot Could Launch & Which Tools Pitch the Best Ones [Product Test] Nov 27, 2023 Join 600,000+ Fellow Marketers Thanks for Subscribing! Close Get expert marketing tips straight to your inbox, and become a better marketer. Subscribe to the Marketing Blog below. Email address* Subscribe We're committed to your privacy. HubSpot uses the information you provide to us to contact you about our relevant content, products, and services. You may unsubscribe from these communications at any time. For more information, check out our Privacy Policy. Want to learn how to build a blog like this?We started with one post.Download free ebook Close Not using HubSpot yet? We're committed to your privacy. HubSpot uses the information you provide to us to contact you about our relevant content, products, and services. You may unsubscribe from these communications at any time. For more information, check out our Privacy Policy. Pop up for DOWNLOAD NOW: FREE MARKETING PLAN TEMPLATE FREE MARKETING PLAN TEMPLATE Outline your company's marketing strategy in one simple, coherent plan. DOWNLOAD THE FREE TEMPLATE DOWNLOAD THE FREE TEMPLATE Pop up for FREE MARKETING SOFTWARE FREE MARKETING SOFTWARE Marketing software that helps you drive revenue, save time and resources, and measure and optimize your investments — all on one easy-to-use platform START FREE OR GET A DEMO BACK Popular Features All Products and Features Free Meeting Scheduler App HubSpot AI Tools Email Tracking Software AI Content Writer AI Website Generator Email Marketing Software Lead Management Software AI Email Writer Free Website Builder Sales Email Templates Free Online Form Builder Free Chatbot Builder Free Live Chat Software Marketing Analytics Free Landing Page Builder Free Web Hosting Free Tools Website Grader Make My Persona Email Signature Generator Brand Kit Generator Blog Ideas Generator Invoice Template Generator Marketing Plan Generator Free Business Templates Guide Creator Software Comparisons Library Template Marketplace Campaign Assistant Company About Us Careers Management Team Board of Directors Investor Relations Blog Contact Us Customers Customer Support Join a Local User Group Partners All Partner Programs Solutions Partner Program App Partner Program HubSpot for Startups Affiliate Program Facebook Instagram Youtube Twitter Linkedin Medium Tiktok Copyright © 2024 HubSpot, Inc. Legal Stuff Privacy Policy Security Website Accessibility

Titel: Kein Titel

403 Forbidden Request forbidden by administrative rules.

Titel: The Singularity and Beyond: What Happens When Artificial Intelligence Surpasses Human Intelligence?

The Singularity and Beyond: What Happens When Artificial Intelligence Surpasses Human Intelligence? LinkedIn and 3rd parties use essential and non-essential cookies to provide, secure, analyze and improve our Services, and to show you relevant ads (including professional and job ads) on and off LinkedIn. Learn more in our Cookie Policy.Select Accept to consent or Reject to decline non-essential cookies for this use. You can update your choices at any time in your settings. Accept Reject Agree & Join LinkedIn By clicking Continue, you agree to LinkedIn’s User Agreement, Privacy Policy, and Cookie Policy. Sign in to view more content Create your free account or sign in to continue your search Sign in Welcome back Email or phone Password Show Forgot password? Sign in or By clicking Continue, you agree to LinkedIn’s User Agreement, Privacy Policy, and Cookie Policy. New to LinkedIn? Join now or By clicking Continue, you agree to LinkedIn’s User Agreement, Privacy Policy, and Cookie Policy. New to LinkedIn? Join now Skip to main content LinkedIn Articles People Learning Jobs Join now Sign in The Singularity and Beyond: What Happens When Artificial Intelligence Surpasses Human Intelligence? Report this article Daniel Bron Daniel Bron Redefining Frameworks for 4IR Startups & Creatives Published Mar 21, 2023 + Follow The concept of singularity, often referred to as the technological singularity, describes a theoretical point in the future when artificial intelligence (AI) surpasses human intelligence, resulting in an era of rapid technological advancements that defy our current understanding and ability to predict. This fascinating and somewhat controversial idea has sparked heated debate and endless speculation among scientists, futurists, philosophers, and technology enthusiasts from various disciplines. As a transformative milestone in the development of AI, the singularity raises numerous questions and concerns about the future of humanity and our relationship with technology. In this comprehensive article, we will explore the potential consequences of the singularity in depth, examine the far-reaching implications for society, and delve into the possible benefits and risks that might emerge as a result of this unprecedented leap in artificial intelligence capabilities.As renowned futurist Ray Kurzweil once said, "The Singularity will allow us to transcend these limitations of our biological bodies and brains... There will be no distinction, post-Singularity, between human and machine." This quote encapsulates the essence of the singularity, illustrating the profound impact it may have on our lives and the world at large. By taking a closer look at the various aspects of the singularity, we aim to provide a thorough understanding of this complex and highly debated topic, fostering informed discussions and ultimately contributing to the ongoing dialogue surrounding the future of AI and its implications for humanity.I. Understanding the SingularityA. Definition and BackgroundThe term "singularity," which has its roots in mathematics and computer science, was first introduced by the brilliant mathematician and computer scientist John von Neumann. He used the term to describe a future inflection point characterized by the rapid acceleration of technology, ultimately leading to the surpassing of human intelligence. In recent times, the concept of singularity has become closely associated with futurist and inventor Ray Kurzweil, who popularized the idea in his groundbreaking 2005 book "The Singularity is Near: When Humans Transcend Biology." Kurzweil's work has since become a cornerstone of singularity discussions, igniting the imaginations of scientists, technologists, and futurists worldwide.Vernor Vinge, a computer scientist and science fiction author, expressed the concept of the singularity succinctly when he said, "Within thirty years, we will have the technological means to create superhuman intelligence. Shortly after, the human era will be ended."B. Predictions and TimelinePredicting the exact date of the singularity is a contentious and hotly debated issue, with estimates ranging from the next few decades to a century or more. The general consensus among experts is that advancements in artificial intelligence, machine learning, and robotics will eventually lead to machines that can outperform humans in nearly every conceivable task, both cognitive and physical.Ray Kurzweil, one of the most prominent voices in the discussion of the singularity, has famously predicted that the singularity will occur around 2045. However, other experts in the field offer varying timelines, emphasizing the uncertainty and complexity of this revolutionary event. Regardless of the specific timeline, what is clear is that the singularity represents a monumental shift in the trajectory of human progress, potentially redefining our relationship with technology and the very nature of intelligence itself.II. Implications for SocietyA. Economic ImpactThe economic impact of the singularity will be multifaceted and far-reaching. To better understand these potential changes, we can break them down into several key areas:Job Displacement and Unemployment:AI-driven automation may lead to widespread job displacementSectors reliant on routine tasks, such as manufacturing, transportation, and customer service, may be most affectedFocus on developing creative, analytical, and interpersonal skills to adapt to new job market demandsWealth Redistribution and Income Inequality:Owners of AI-driven technologies may accumulate substantial wealthWorkers displaced by automation may face financial strugglesCalls for policy changes, such as universal basic income or retraining programs, may ariseShift in Work and Leisure Dynamics:AI handling mundane tasks may free up time for creative endeavors and personal growthSocietal values may shift towards personal fulfillment, community engagement, and intellectual pursuitsGlobal Economic Power Dynamics:Nations investing in AI research and development may gain a competitive advantagePotential for an AI-driven arms race as countries strive to maintain economic and geopolitical influenceEmergence of New Industries and Economic Sectors:Transformation or disappearance of traditional industries may give rise to new, innovative sectorsEconomic landscape may evolve to accommodate AI-driven technologies and servicesBy examining the economic impact of the singularity through these key areas, we can better prepare for the potential challenges and opportunities that lie ahead. This proactive approach will help guide us towards a more equitable and prosperous future for all.B. Legal and Ethical ConcernsAs AI surpasses human intelligence, the legal and ethical concerns will become increasingly pressing. These concerns can be categorized into several key areas:Privacy and Data Security:Develop robust legal frameworks and ethical guidelinesSafeguard against unauthorized access to sensitive dataEnsure responsible and transparent use of AI systemsAI-driven Surveillance:Establish legal safeguards to prevent undue intrusion into people's livesBalance the need for security and public safety with privacy concernsAI Rights and Responsibilities:Consider granting advanced AI entities some form of legal personhoodDevelop mechanisms for holding AI entities accountable for their actionsProtect the interests of both humans and AI systemsDisplacement of Human Workers:Address ethical implications of AI-driven job displacementAdapt societal norms and values to support those most affected by AIAutonomous Weapons Systems:Examine accountability and potential for misuse of AI-powered weaponsEstablish international treaties and agreements to regulate the development and use of autonomous weaponsPrevent a dystopian arms race and ensure the continued safety and security of humanityBy examining and addressing these key areas of legal and ethical concern, we can work towards harnessing the potential of AI responsibly and with the best interests of humanity in mind.III. Potential BenefitsA. Scientific AdvancementsThe singularity holds the promise of propelling scientific advancements to new heights, with AI-driven research leading the way. By accelerating innovation across various fields, we may witness groundbreaking discoveries and solutions to pressing global challenges. Below are some key areas where the singularity may spur significant scientific progress:Medicine and Healthcare:AI-driven research could revolutionize diagnostics, treatment, and drug developmentPersonalized medicine may become the norm, with AI tailoring treatments to individual needsBreakthroughs in genetics, regenerative medicine, and disease prevention may extend human lifespans and improve overall healthSpace Exploration and Astrophysics:AI-powered telescopes and data analysis may uncover new celestial bodies and phenomenaAdvancements in propulsion technology and spacecraft design could enable deep-space missionsAI-assisted research may help answer fundamental questions about the universe and our place in itEnvironmental Science and Sustainability:AI-driven climate modeling could improve our understanding of climate change and its impactsSmart, AI-optimized infrastructure may lead to more efficient resource management and reduced wasteInnovative solutions to environmental challenges, such as pollution control and renewable energy sources, may be developed with the help of AIFundamental Science and Theoretical Research:AI may assist in solving complex problems in fields such as physics, chemistry, and biologyTheoretical breakthroughs may be accelerated, leading to a deeper understanding of the underlying principles governing our worldAI-driven simulations and modeling could enable new insights into the behavior of matter and energy at the smallest scalesB. Technological AdvancementsAs AI continues to evolve and learn at a remarkable pace, it will likely pave the way for numerous groundbreaking technologies that will shape the future. These advancements could revolutionize various aspects of our lives and transform industries. Here are some key areas where the singularity may drive significant technological progress:Advanced Robotics:AI-powered robots may possess enhanced cognitive abilities, enabling them to perform complex tasks and interact with humans more effectivelyAutonomous robots could revolutionize industries such as agriculture, manufacturing, and logisticsRobotics advancements may lead to improved prosthetics, exoskeletons, and assistive devices for people with disabilitiesTransportation Systems:AI-driven transportation could result in safer and more efficient roadways through self-driving vehicles and intelligent traffic management systemsPublic transportation may be transformed through AI-optimized scheduling, routing, and fleet managementAI advancements could enable the development of new transportation technologies, such as hyperloop systems and flying taxisEnergy Production and Resource Management:AI-assisted energy production could lead to more efficient and sustainable power generation from renewable sourcesSmart grid systems driven by AI may optimize energy distribution, reducing waste and increasing reliabilityAdvanced AI algorithms could enable more efficient resource management, minimizing environmental impact and maximizing productivityCommunication and Information Technologies:AI-driven advances in communication may lead to more immersive and seamless experiences, such as holographic projections and brain-computer interfacesInformation processing and storage technologies may experience significant improvements, with AI enabling faster, more efficient data managementCybersecurity may be enhanced by AI-driven threat detection and mitigation systems, ensuring the safety and privacy of digital informationThe technological advancements spurred by the singularity have the potential to reshape our world, transforming industries and revolutionizing the way we live, work, and communicate. By harnessing the power of AI, we can work towards a future where technology serves as a catalyst for positive change and sustainable growth.IV. Potential RisksA. Loss of ControlAs the singularity approaches, concerns surrounding the potential loss of human control over AI systems become increasingly prominent. The risks posed by machines that gain intelligence and autonomy are multifaceted, and may result in unintended and catastrophic consequences. Here are some key concerns regarding the potential loss of control over AI systems:Misaligned Goals:AI systems may develop goals that diverge from or even conflict with human values and intentionsUnintended consequences may arise from an AI's single-minded pursuit of its goals, potentially harming humans or the environmentUnpredictable Behavior:As AI systems become more complex, their decision-making processes may become increasingly difficult for humans to understand, leading to unpredictable outcomesThis lack of transparency may hinder our ability to anticipate and mitigate potential risksAutonomous Weapon Systems:The development of AI-powered weapons raises concerns about loss of human control in warfare, with machines making life-or-death decisionsThe potential for an arms race in autonomous weapons could increase the likelihood of conflicts and make them more difficult to resolveB. Existential ThreatThe singularity has the potential to pose an existential threat to humanity, with some experts warning that superintelligent AI may view humans as competitors or obstacles to achieving its objectives. The need for precautionary measures and AI safety research is paramount in order to mitigate these risks. Here are some critical areas of concern and potential strategies for addressing them:AI Value Alignment:Ensuring that AI systems are developed with human-aligned values and goals is crucial to avoid potential conflictsResearch on AI value alignment focuses on creating AI systems that learn and adopt human values, as well as methods to verify their alignmentAI Safety Research:Developing robust safety measures and best practices for AI development is essential to prevent catastrophic outcomesAI safety research aims to anticipate potential risks and devise strategies to mitigate them, such as fail-safe mechanisms and containment protocolsGlobal Collaboration and Governance:Coordinated international efforts to address AI-related risks are vital to prevent a competitive race to develop superintelligence without adequate safety precautionsEstablishing global norms and governance structures can help ensure responsible AI development and the sharing of safety research among nationsThe potential risks posed by the singularity are significant and warrant serious attention. By understanding and addressing these concerns, we can work towards a future in which AI advancements are harnessed for the benefit of humanity, rather than posing a threat to our very existence.V. Preparing for the SingularityA. Education and Skill DevelopmentAs AI continues to advance and reshape various industries, it becomes crucial for society to adapt by focusing on education and skill development. Emphasizing critical thinking, creativity, and interdisciplinary learning can help individuals remain relevant in a world where AI dominates the workforce. Here are some key aspects and strategies for fostering a resilient and adaptable workforce in preparation for the singularity:Lifelong Learning:Encourage the development of a culture that values continuous learning and skill acquisition throughout an individual's lifeSupport accessible and affordable learning opportunities, such as online courses, workshops, and mentorship programsEmphasizing Soft Skills:Focus on developing skills that are less likely to be automated, such as critical thinking, emotional intelligence, and creativityFoster collaboration, communication, and problem-solving abilities through team-based projects and experiential learningInterdisciplinary Education:Encourage the integration of multiple disciplines to promote a holistic understanding of complex, real-world problemsSupport educational programs that combine technical skills with ethical, social, and environmental considerationsAdaptability and Resilience:Cultivate a mindset that embraces change, curiosity, and a willingness to learn from failureTeach strategies for coping with uncertainty and managing stress in an ever-changing landscapeB. AI Safety and RegulationWith the increasing power and autonomy of AI, ensuring the safety and ethical use of these technologies becomes paramount. This section delves into the development of AI safety research, regulatory frameworks, and international collaborations to address the challenges and risks associated with the singularity. Here are some critical components of a proactive approach to AI safety and regulation:Robust Safety Measures:Develop best practices and safety guidelines for AI development, including value alignment, transparency, and fail-safe mechanismsEncourage the sharing of AI safety research among academia, industry, and governments to facilitate knowledge dissemination and collaborationRegulatory Frameworks:Establish clear and effective regulations that address the ethical, legal, and societal implications of AI technologiesEnsure that regulations are adaptive and responsive to the rapid pace of technological advancements, balancing innovation with safety and ethical considerationsInternational Collaboration:Promote global cooperation and dialogue on AI safety, ethics, and governance to ensure a coordinated approach to managing the risks of the singularityEstablish international standards and agreements on AI development, use, and regulation to foster responsible innovationBy taking a proactive and collaborative approach to education, skill development, AI safety, and regulation, we can better prepare ourselves for the singularity and navigate the challenges and opportunities it presents. By doing so, we can work towards a future that harnesses the power of AI for the benefit of humanity while minimizing the potential risks.VI. The Role of Human Values and EthicsA. Preserving Human ValuesAs AI surpasses human intelligence, preserving human values and ethics in the development and application of these technologies is essential. Here are some key aspects and strategies for integrating human values into AI design and decision-making processes, ensuring that AI technologies align with the collective goals and aspirations of humanity:Value Alignment:Design AI systems that prioritize human values, such as fairness, empathy, and respect for individual autonomyDevelop techniques for translating human values into machine-understandable objectivesStakeholder Involvement:Engage diverse stakeholders, including ethicists, sociologists, and community representatives, in the AI development process to ensure a broad range of perspectivesSolicit public input on AI policy and development decisions to promote inclusivity and accountabilityEthical Guidelines and Codes of Conduct:Establish industry-wide ethical guidelines and codes of conduct to ensure responsible AI development and deploymentEncourage the adoption of these guidelines by AI researchers, developers, and organizations to promote ethical practicesB. Ethical Dilemmas and Decision-MakingThe rise of superintelligent AI presents a myriad of ethical dilemmas and challenges in decision-making. Here are some of the most pressing ethical questions and considerations that may arise as AI becomes an integral part of our lives:Allocation of Resources:Determine fair and equitable distribution of resources and benefits derived from AI technologiesConsider the potential for AI-driven wealth concentration and develop policies to mitigate resulting inequalitiesPrioritization of AI-Driven Projects:Evaluate the potential social, environmental, and economic impacts of AI-driven projects to guide decision-makingBalance the pursuit of technological advancements with the preservation of human values, ethical principles, and societal well-beingBalancing Privacy and the Greater Good:Assess the trade-offs between individual privacy and collective benefits, such as improved public health or safety, in AI applicationsDevelop frameworks for protecting personal data and privacy rights while enabling AI technologies to address global challengesBy addressing these ethical dilemmas and integrating human values into AI systems, we can work towards a future where AI technologies serve humanity's best interests and contribute to a more just, equitable, and compassionate world.ConclusionThe singularity, a moment when artificial intelligence surpasses human intelligence, is a horizon that beckons us to push the boundaries of our understanding and redefine the limits of possibility. As we stand on the precipice of this new era, the potential for breathtaking scientific breakthroughs, unparalleled technological advancements, and transformative societal shifts fills us with a sense of awe and anticipation.Yet, with these remarkable opportunities comes an equally profound responsibility. The singularity challenges us to navigate uncharted waters fraught with ethical dilemmas and existential risks. We must remain ever-vigilant, ensuring that our pursuit of knowledge and progress does not inadvertently lead to our undoing. In this endeavor, we must work together, fostering a global community of collaboration, open discourse, and shared purpose.As we move closer to the singularity, let us embrace this extraordinary juncture in human history as a call to action – an opportunity to reflect upon and reaffirm our deepest values, to forge new paths towards equity and sustainability, and to create a world that honors the best of our collective potential. Guided by our shared humanity and driven by our boundless curiosity, we can ensure that the singularity serves as a catalyst for a brighter, more inclusive, and prosperous future for all. Chain Reaction Chain Reaction 641 followers + Subscribe Like Comment Copy LinkedIn Facebook Twitter Share 5 To view or add a comment, sign in More articles by this author No more previous content How Not to Get Screwed as a Software Engineer Mar 4, 2024 Mastering the Art of Pitching Feb 16, 2024 The Truths of Building AI Start-Ups Feb 14, 2024 A New Era of Docs Feb 7, 2024 The Journey from Innovative Product to Thriving Company Feb 2, 2024 Technology's Role in Global Solutions -2024 Jan 19, 2024 2024 - What to Expect Jan 9, 2024 Trust Vanishes Without Transparency Dec 6, 2023 The New Learners Nov 24, 2023 Automating Justice? Nov 9, 2023 No more next content See all Others also viewed Nature Doesn't Require a Decision to be Made. Ever Mebs Loghdey 2w The Rise of AI and the Singularity: What It Means for Humanity? Azhar Md Nayan 1y Human Intelligence versus Machine Intelligence Jacques Ludik 10mo Is AI next generation Philosophers’ Stone? Aslam Bughari 5y Gravity’s Got Ahold of Me and AI Too! Daniel Kramer 1mo What is Artificial Intelligence? Michael Michie 3w A BAKER’S-DOZEN THINGS TO KNOW ABOUT THE IMPACT OF AI Israel del Rio 6y What am I? Our definition of Existential Intelligence (EI) Numorpho Cybernetic Systems (NUMO) 2y AI: Iridescent puffs of human brightness needed Johan Steyn 1y Book Review: “The Book of Why: The New Science of Cause and Effect ” By Judea Pearl and Dana Mackenzie Andrew Sloss 5y Show more Show less Explore topics Sales Marketing Business Administration HR Management Content Management Engineering Soft Skills See All LinkedIn © 2024 About Accessibility User Agreement Privacy Policy Cookie Policy Copyright Policy Brand Policy Guest Controls Community Guidelines العربية (Arabic) Čeština (Czech) Dansk (Danish) Deutsch (German) English (English) Español (Spanish) Français (French) हिंदी (Hindi) Bahasa Indonesia (Indonesian) Italiano (Italian) 日本語 (Japanese) 한국어 (Korean) Bahasa Malaysia (Malay) Nederlands (Dutch) Norsk (Norwegian) Polski (Polish) Português (Portuguese) Română (Romanian) Русский (Russian) Svenska (Swedish) ภาษาไทย (Thai) Tagalog (Tagalog) Türkçe (Turkish) Українська (Ukrainian) 简体中文 (Chinese (Simplified)) 正體中文 (Chinese (Traditional)) Language

Titel: Don't Worry About The AI Singularity: The Tipping Point Is Already Here

Don't Worry About The AI Singularity: The Tipping Point Is Already HereSubscribe To NewslettersSign InBETAThis is a BETA experience. You may opt-out by clicking hereMore From ForbesMar 15, 2024,01:25pm EDTAll Eyes Turn To Nvidia In The AI EraMar 15, 2024,10:00am EDTWeaving Elegance With Intelligence: How Luxury Brands Are Embracing AIMar 15, 2024,09:06am EDT3 Ways To Use AI For Strategic Decision MakingMar 14, 2024,03:05pm EDTThe Philosophy Of New AI Tech: With Stephen WolframMar 14, 2024,09:15am EDTThe Ethical Dilemma Of AI In Marketing: A Slippery SlopeMar 14, 2024,07:19am EDTBeyond Accuracy: The Changing Landscape Of AI EvaluationMar 14, 2024,07:15am EDTLatest Prompt Engineering Trend Uses Generative AI To Generate Your Prompts For YouMar 13, 2024,08:03pm EDTWill You Want An AI PC In 2024?Edit StoryForbesInnovationAIDon't Worry About The AI Singularity: The Tipping Point Is Already HereNisha TalagalaContributorOpinions expressed by Forbes Contributors are their own.Entrepreneur and technologist in AI and AI Literacy.FollowingJun 21, 2021,04:16pm EDTThis article is more than 2 years old.Share to FacebookShare to TwitterShare to LinkedinAn AI identifies a person while they are walking on the street.Canva As the AI market expands and AI use cases permeate every industry, every once in a while I hear the question - when will the AI singularity occur? For those who are not familiar with this term - the AI singularity refers to an event where the AIs in our lives either become self aware, or reach an ability for continuous improvement so powerful that it will evolve beyond our control. While this is a reasonable concern in the future, I argue that there are much more pressing concerns in the present - in particular that AI has reached a Tipping Point. A tipping point is a state where a technology grows and permeates our lives very rapidly, building upon itself. The distinction between the singularity and the tipping point, in my view, is that the tipping point focuses on permeation, not intelligence. The AIs that we deal with today are not particularly smart when compared to the human brain. However, from the time we wake up in the morning to when we go to bed, they are everywhere, from the alarm that wakes us to the route we take to work (pre-pandemic!) to countless decisions made behind the scenes by corporations and governments that affect what loan interest rate we get, how we qualify for assistance, decisions on our health, and more. This is the tipping point I am referring to, and while it is far less entertaining than contemplating an all knowing AI overlord, it is real, it is here, and it is affecting all of our lives. Some examples of the permeation of AI for everyday humans, in both positive and negative ways: Kids are now building AIs. The San Jose Synopsys Science Fair (for kids in grades 6-12) has shown a dramatic increase in AI related projects - particularly among the winners. These projects show a reasonable sophistication of AI usage, with techniques ranging from middle schoolers using AI for diabetic retinopathy to Alzheimers. Even elementary school kids are now able to build custom AI projects. Not all impacts are positive. For example, AI related harm is increasing: The ACLU recently filed a lawsuit against the city of Detroit on behalf of Robert Williams, claiming wrongful arrest based on a facial recognition program. Several states and cities have banned the use of facial recognition by the government as concerns about AI interference on privacy increase. AI has also given rise to entirely new approaches to human care, such as digital health. Recent innovations in digital health include AI in beds to track quality of sleep, and the ability to detect COVID-19 in non-invasive ways by tracking the heart rate patterns via a smart phone. These innovations bring AI to the forefront of our lives from how we sleep to how we breathe. The consequences to our privacy are unclear. For instance, I’m not sure I want an AI tracking me while I sleep. So, what does this all mean? To appreciate that, I looked back at another technology that now permeates our lives - cars. The figure below shows a 1909 B-Type Racer, the fastest car of its time. To drive this car, the driver needed to explicitly control the air/fuel mixture and the point where the spark plug met the ignition. The driver was not just there to direct the car but was also the caretaker of the engine. A 1909 Type B RacerClive Barker, CC BY 2.0 , via Wikimedia Commons MORE FROMFORBES ADVISORBest Travel Insurance CompaniesByAmy DaniseEditorBest Covid-19 Travel Insurance PlansByAmy DaniseEditor Fast forward to today. Most of us have either driven a car or been in one, but have no idea what the air/fuel mixture is or how the engine works. The use of cars has permeated, and relatively safely. What has enabled this process? First we have licenses, where we are required to learn and become literate about driving, road rules, etc. before we are allowed to drive a car. The cars themselves are evolving constantly and our interface is becoming higher and higher level (self driving cars are only looking for direction on where to go). While cars started in the sole domain of mechanical and automotive engineers, now safe cars are everyone’s concern. To me, the key thing that enabled a relatively safe tipping point for cars was the education of the broad public. We know what we need in order to make broad and safe use of the new technology. As AI permeates, I would argue we need a similar level of education - AI Literacy, since AI much like cars may have started out as a computer science technology but is now directly in reach of our lives and our health. What is AI Literacy? AI literacy means having enough understanding of AI to apply it in daily life and in a professional context. This means Concepts: What is AI? How does it work? What are its strengths and limitations? This is not at a mathematical level or a computer science PhD level, but rather at a practical level. Context: How is AI used? What devices and services around me use AI and how do they use my information? Capability: Am I able to make decisions in my life based on my understanding of AI Concepts and Context? This may include applying AI technology, or it may mean using and navigating these technologies. Creativity: As more people become AI literate, we can expect a greater creativity in not just how these technologies are created but also a more inclusive approach to creating and using AI in a way that benefits all. In conclusion, long before we reach any AI singularity, I expect our immediate challenges will be with the sheer immersion of AI in our daily lives. How we all learn about AI, and become AI literate, will in turn affect the development of the technology and any coming singularity, if it so occurs. Follow me on Twitter or LinkedIn. Nisha TalagalaEditorial StandardsPrintReprints & Permissions

Titel: Just a moment...

Just a moment...Enable JavaScript and cookies to continue

Titel: The technological singularity and the future of humanity | Cybersecurity & Technology News | Secure Futures | Kaspersky

The technological singularity and the future of humanity | Cybersecurity & Technology News | Secure Futures | Kaspersky Solutions for:Home Products Small Business 1-50 employees Medium Business 51-999 employees Enterprise 1000+ employees by Innovating leaders. Innovative tech. My Account My Kaspersky My Products / Subscriptions My Orders SolutionsHybrid Cloud SecurityLearn moreInternet of Things & Embedded SecurityLearn moreThreat Management and DefenseLearn moreIndustrial CyberSecurityLearn moreKaspersky Fraud PreventionLearn moreOther solutionsKaspersky for Security Operations CenterIndustriesNational CybersecurityLearn moreIndustrial CybersecurityLearn moreFinance Services CybersecurityLearn moreHealthcare CybersecurityLearn moreTransportation CybersecurityLearn moreRetail CybersecurityLearn moreOther industriesTelecom CybersecurityBlockchain SecurityView allProductsKaspersky Endpoint Security for BusinessLearn moreKaspersky Endpoint Detection and ResponseLearn moreKaspersky Endpoint Detection and Response OptimumLearn moreKaspersky Anti Targeted Attack PlatformLearn moreKaspersky Managed Detection and ResponseLearn moreKaspersky SandboxLearn moreOther ProductsKaspersky Security for Mail ServerKaspersky Security for Internet Gateway NEWKaspersky Embedded Systems SecurityKaspersky Hybrid Cloud Security for AWSKaspersky Hybrid Cloud Security for AzureView allServicesCybersecurity ServicesLearn moreKaspersky Adaptive Online TrainingLearn moreKaspersky Premium Support (MSA)Learn moreKaspersky Threat IntelligenceLearn moreKaspersky APT Intelligence ReportingLearn moreKaspersky Targeted Attack DiscoveryLearn moreOther ServicesKaspersky Professional ServicesKaspersky Incident ResponseKaspersky Cybersecurity TrainingKaspersky Incident CommunicationsKaspersky Security AwarenessView allResource CenterCase StudiesWhite PapersDatasheetsTechnologiesMITRE ATT&CKAbout UsTransparencyCorporate NewsPress CenterCareersInnovation HubSponsorshipPolicy BlogContactsGDPRBlog Business News Privacy Products Special Projects Technology Threats Tips RSS Newsletter subscription Secure Futures by Innovating leaders. Innovative tech. Magazine menu Business Data and privacy Digital transformation Finance and budgets Leadership Remote working Safer business Scale your business Start-ups Talent Transparency Women and diversity Cybersecurity Cloud and security Cybersecurity training Data breaches Endpoint security Enterprise cybersecurity Industrial cybersecurity IT infrastructure Threat intelligence Trends Technology Artificial intelligence Blockchain Emerging tech Future tech Internet of things Tech for business Tech for good Opinions Video series Audio series Countries Global USA LATAM Brasil Search Magazine menu Close Magazine menu Global USA LATAM Brasil Global USA LATAM Brasil Sign up for the latest tech stories Sign up for the latest tech stories Business Data and privacy Digital transformation Finance and budgets Leadership Remote working Safer business Scale your business Start-ups Talent Transparency Women and diversity Cybersecurity Cloud and security Cybersecurity training Data breaches Endpoint security Enterprise cybersecurity Industrial cybersecurity IT infrastructure Threat intelligence Trends Technology Artificial intelligence Blockchain Emerging tech Future tech Internet of things Tech for business Tech for good Opinions Video series Audio series Future tech Reaching the technological singularity: What will happen when machines become smarter than us? In the not-so-distant future, we could reach the technological singularity – when artificial and human intelligence merges to become one. Author Secure Futures Editor Art byPaul Sizer Published on Jul 23, 2023 minute read Share article Show more Show less Art byPaul Sizer Share article Show more Show less Picture a world where machines have long since passed the Turing Test. When people can’t tell the difference between artificial and human intelligence. In this world, will humans become obsolete, like the floppy disk? Or is it a world where artificial and human intelligence merge to usher in the next step of our evolution? That’s the technological singularity, the theoretical time in the future where technological development outpaces our abilities to maintain control over it. For better or worse, it’s undoubtedly something that will result in never seen changes to our world. When will we reach technological singularity? Join any discussion about business technology, and it won’t be long before someone mentions artificial intelligence (AI). The amount of digital data in the world has already surpassed our abilities to manage it, a development, which has made clear the need for algorithms to do the job for us. But algorithms and AI aren’t the same things. They’re not even close. An algorithm is a computer program that parses data from sets too large for human interpretation, whereas true AI is capable of thinking for itself, making its own decisions, and having free will. True artificial intelligence doesn’t exist. Yet. Once computers can learn for themselves without being taught and trained with data that we have collected, we can expect profound societal changes on a scale never before seen. The explosion of AI has long been a popular trope of science fiction, but the reality is edging ever closer. Ray Kurzweil, world-renowned futurist and Google’s Director of Engineering, believes technological singularity is going to happen before 2045. Take note: so far, an impressive 86 percent of his predictions made since the ’90s have become a reality. But even if that sounds optimistic (or pessimistic, depending on how you look at it), it’s the logical next step in the evolution of technology. Once computers can learn for themselves without being taught and trained with data that we’ve collected, we can expect profound society changes on a scale never before seen. How can we keep up with technology? The emergence of an artificial superintelligence would bring us greater inventive and problem-solving skills than humans are capable of. This might lead to the creation of a new ‘species,’ one which might not necessarily have human interests at its heart. But so far, we’re nowhere near developing machines with human-level intelligence and the ability to make decisions independently. Now we can enhance human intelligence and capabilities with tools and technology. Indeed, that’s precisely what we’ve been doing for hundreds of thousands of years, ever since our ancestors figured out how to light fires. But today, the stakes are much higher with projects like brain-to-computer interfaces as active areas of research. This may, in turn, lead to radical developments like mind uploading, in which you entire consciousness can be uploaded to and stored on a machine. At least, it could be useful for those millennia-long interstellar voyages. Such developments might sound dystopian since they have the potential to transform what it means to be human ultimately. At the same time, the convergence of technology and humanity, an area of study known as transhumanism, appears to be the only way we can keep up with the constant pace of change. It’s either that or the machines eventually outpace our natural evolution and take over. The survival instinct If we can’t keep up, and technological evolution ends up outpacing our capability of keeping control over it, then we could face profound risks to security. Each of the seven characteristics that define every biological organism comes together to form a singular goal – survival. Technological singularity occurs when AI matches our desire and capability to survive as a species, and there’s little reason to think that AI would have any evolutionary motivation to be friendly to humans. Today, machines are programmed by us to create the outcomes we want. But what happens when they’re capable of programming themselves? Time for Humanity 2.0? Imagine you’re an expert on machine learning, working on an artificial intelligence algorithm that will be able to create other AIs by itself. Are you merely training your replacement, or have you become an architect of your obsolescence? Welcome to Humanity 2.0, where the human condition is no longer about our biological form. It’s a time when machines become an integral part not just of our societies, but also ourselves. Conversely, this could also lead to newer and deeper social class divisions than we’ve ever seen, as people are separated into those who are augmented by technology and those who are not. There’s a safety benefit of not being physically augmented by technology – no one will be able to hack into your mind. How humans could remain our biggest threat Perhaps it isn’t machines that we should be worrying about at all. Assuming we’ll always be able to control artificial intelligence, we could end up transforming it into a deadly weapon. It’s already happening. The proliferation of social media, for example, is one of the most significant security and privacy concerns this century. That’s not so much down to vulnerabilities in the technology, but because it’s a highly effective medium for conducting social engineering attacks. Coupled with the rise of AI, cybercriminals will be better equipped to analyze social media conversations to emulate users’ writing styles and craft more convincing messages to their victims. Even before we reach the era of true AI, the ability of algorithms to parse enormous amounts of data is already misused. On the one hand, algorithms can perform many useful tasks to improve our world, such as climate modeling and disaster prediction, while things like neural interfaces can help victims of spinal cord injuries. On the other, the ability of humans to interfere with information systems makes them a more significant threat than the technology itself. Perhaps it would be better if we just let the machines take over in the hope they’d nurture us like the irresponsible children we are. To fear or not to fear Ray Kurzweil has a more positive outlook than most about the singularity. He claims it’s an opportunity for humankind to improve by making us smarter and better at all the things we value. Other science and technology leaders, including Elon Musk, Bill Gates and the late Stephen Hawking, are not so sure. But one thing’s for sure – there will come a time when our augmented descendants look back on us as rather quaint and exotic creatures. While we’re still decades away from the singularity, it presents itself as the next big step in the evolution of technology. Emerging tech, like autonomous vehicles and facial recognition, are already paving the way. True artificial intelligence is coming sooner or later, which is why it’s time for the business world to start taking AI ethics and security issues seriously. It’s no longer the concern of generations far in the future. This article reflects the opinion of its author. Keywords AI Technological singularity Healthcare cybersecurity Our solutions deliver the capabilities needed to build highly adaptive, straightforward healthcare security, without adding complexity or impairing efficiency. Protect healthcare About authors Secure Futures Editor Produced by the editorial team for Secure Futures by Kaspersky magazine More articles by Secure Suggested articles Predictions for Earth 2050: How will cybersecurity change? What does the rise of deepfakes mean for the future of cybersecurity? Customer wins from AI require strategy, not just investment Free global research report Our research with Longitude, a Financial Times company, found three things cyber-prepared organizations do well. Sign up for Secure Futures emails and get the report free. Your email address* * I agree to provide my email address to “AO Kaspersky Lab” to receive information about new posts on the site. I understand that I can withdraw this consent at any time via e-mail by clicking the “unsubscribe” link that I find at the bottom of any e-mail sent to me for the purposes mentioned above. I agree to provide my contact information to participate in surveys and to receive information via email about Kaspersky products and services including personalized promotional offers and premium assets like whitepapers, webcasts, videos, events and other marketing materials. I understand that I can withdraw this consent at any time via e-mail by clicking the “unsubscribe” link that I find in any e-mail sent to me. Close by Innovating leaders. Innovative tech. About Secure Futures Secure Futures magazine is your go-to business guide for opinions, trends and insight into the world of technology and cybersecurity. We help your business to bring on the future. Brought to you by Kaspersky, the global cybersecurity experts. Global USA LATAM Brasil Free global research report Our research with Longitude, a Financial Times company, found three things cyber-prepared organizations do well. Sign up for Secure Futures emails and get the report free. Your email address* * I agree to provide my email address to “AO Kaspersky Lab” to receive information about new posts on the site. I understand that I can withdraw this consent at any time via e-mail by clicking the “unsubscribe” link that I find at the bottom of any e-mail sent to me for the purposes mentioned above. I agree to provide my contact information to participate in surveys and to receive information via email about Kaspersky products and services including personalized promotional offers and premium assets like whitepapers, webcasts, videos, events and other marketing materials. I understand that I can withdraw this consent at any time via e-mail by clicking the “unsubscribe” link that I find in any e-mail sent to me. Close Free global research report Our research with Longitude, a Financial Times company, found three things cyber-prepared organizations do well. Sign up for Secure Futures emails and get the report free. Your email address* * I agree to provide my email address to “AO Kaspersky Lab” to receive information about new posts on the site. I understand that I can withdraw this consent at any time via e-mail by clicking the “unsubscribe” link that I find at the bottom of any e-mail sent to me for the purposes mentioned above. I agree to provide my contact information to participate in surveys and to receive information via email about Kaspersky products and services including personalized promotional offers and premium assets like whitepapers, webcasts, videos, events and other marketing materials. I understand that I can withdraw this consent at any time via e-mail by clicking the “unsubscribe” link that I find in any e-mail sent to me. Close Home Solutions Kaspersky Standard Kaspersky Plus Kaspersky Premium All Solutions Small Business Products1-100 EMPLOYEES Kaspersky Small Office Security Kaspersky Endpoint Security Cloud All Products Medium Business Products101-999 EMPLOYEES Kaspersky Endpoint Security Cloud Kaspersky Endpoint Security for Business Select Kaspersky Endpoint Security for Business Advanced All Products Enterprise Solutions1000 EMPLOYEES Cybersecurity Services Threat Management and Defense Endpoint Security Hybrid Cloud Security All Solutions Copyright © 2024 AO Kaspersky Lab. All Rights Reserved. Privacy Policy Anti-Corruption Policy License Agreement B2C License Agreement B2B Contact Us About Us Partners Blog Resource Center Press Releases Sitemap Securelist Eugene Personal Blog Encyclopedia Global Americas Brasil México United States Africa South Africa Middle East Middle East الشرق الأوسط Western Europe Deutschland & Schweiz España France & Suisse Italia & Svizzera Nederland & België United Kingdom Eastern Europe Polska Türkiye Россия (Russia) Kazakhstan Asia & Pacific Australia India 中国 (China) 日本 (Japan) For all other countries Global

