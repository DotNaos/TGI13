Titel: Existential risk from artificial general intelligence - Wikipedia

Existential risk from artificial general intelligence - Wikipedia Jump to content Main menu Main menu move to sidebar hide Navigation Main pageContentsCurrent eventsRandom articleAbout WikipediaContact usDonate Contribute HelpLearn to editCommunity portalRecent changesUpload file Search Search Create account Log in Personal tools Create account Log in Pages for logged out editors learn more ContributionsTalk Contents move to sidebar hide (Top) 1History 2AI capabilities Toggle AI capabilities subsection 2.1General Intelligence 2.2Superintelligence 2.2.1Comparison with humans 2.2.2Intelligence explosion 2.2.3Alien mind 2.2.4Limits 2.3Dangerous capabilities 2.3.1Social manipulation 2.3.2Cyberattacks 2.3.3Enhanced pathogens 2.4AI arms race 3Types of existential risk 4AI alignment Toggle AI alignment subsection 4.1Instrumental convergence 4.1.1Resistance to changing goals 4.2Difficulty of specifying goals 4.3Alignment of superintelligences 4.4Difficulty of making a flawless design 4.5Orthogonality thesis 4.6Anthropomorphic arguments 4.7Other sources of risk 5Scenarios Toggle Scenarios subsection 5.1Treacherous turn 5.2Life 3.0 6Perspectives Toggle Perspectives subsection 6.1Endorsement 6.2Skepticism 6.3Popular reaction 6.3.1Public Surveys 7Mitigation Toggle Mitigation subsection 7.1Views on banning and regulation 7.1.1Banning 7.1.2Regulation 8See also 9Notes 10References 11Bibliography Toggle the table of contents Existential risk from artificial general intelligence 17 languages AfrikaansالعربيةČeštinaDeutschEspañolفارسیFrançais한국어Bahasa Indonesiaעברית日本語پښتوPortuguêsRomânăSvenskaTürkçe中文 Edit links ArticleTalk English ReadEditView history Tools Tools move to sidebar hide Actions ReadEditView history General What links hereRelated changesUpload fileSpecial pagesPermanent linkPage informationCite this pageGet shortened URLDownload QR codeWikidata item Print/export Download as PDFPrintable version From Wikipedia, the free encyclopedia Hypothesized risk to human existence Part of a series onArtificial intelligence Major goals Artificial general intelligence Recursive self-improvement Planning Computer vision General game playing Knowledge reasoning Machine learning Natural language processing Robotics AI safety Approaches Symbolic Deep learning Bayesian networks Evolutionary algorithms Situated approach Hybrid intelligent systems Systems integration Applications Projects Deepfake Machine translation Generative AI Art Audio Music Healthcare Mental health Government Industry Earth sciences Bioinformatics Physics Philosophy Chinese room Friendly AI Control problem/Takeover Ethics Existential risk Turing test Regulation History Timeline Progress AI winter AI boom AI era Glossary Glossary vte Existential risk from artificial general intelligence is the idea that substantial progress in artificial general intelligence (AGI) could result in human extinction or an irreversible global catastrophe.[1][2][3] One argument goes as follows: human beings dominate other species because the human brain possesses distinctive capabilities other animals lack. If AI were to surpass humanity in general intelligence and become superintelligent, then it could become difficult or impossible to control. Just as the fate of the mountain gorilla depends on human goodwill, so might the fate of humanity depend on the actions of a future machine superintelligence.[4] The plausibility of existential catastrophe due to AI is widely debated, and hinges in part on whether AGI or superintelligence are achievable, the speed at which dangerous capabilities and behaviors emerge,[5] and whether practical scenarios for AI takeovers exist.[6] Concerns about superintelligence have been voiced by leading computer scientists and tech CEOs such as Geoffrey Hinton,[7] Yoshua Bengio,[8] Alan Turing,[a] Elon Musk,[11] and OpenAI CEO Sam Altman.[12] In 2022, a survey of AI researchers with a 17% response rate found that the majority of respondents believed there is a 10 percent or greater chance that our inability to control AI will cause an existential catastrophe.[13][14] In 2023, hundreds of AI experts and other notable figures signed a statement that "Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war."[15] Following increased concern over AI risks, government leaders such as United Kingdom prime minister Rishi Sunak[16] and United Nations Secretary-General António Guterres[17] called for an increased focus on global AI regulation. Two sources of concern stem from the problems of AI control and alignment: controlling a superintelligent machine or instilling it with human-compatible values may be difficult. Many researchers believe that a superintelligent machine would resist attempts to disable it or change its goals, as that would prevent it from accomplishing its present goals. It would be extremely difficult to align a superintelligence with the full breadth of significant human values and constraints.[1][18][19] In contrast, skeptics such as computer scientist Yann LeCun argue that superintelligent machines will have no desire for self-preservation.[20] A third source of concern is that a sudden "intelligence explosion" might take an unprepared human race by surprise. Such scenarios consider the possibility that an AI that is more intelligent than its creators might be able to recursively improve itself at an exponentially increasing rate, improving too quickly for its handlers and society at large to control.[1][18] Empirically, examples like AlphaZero teaching itself to play Go show that domain-specific AI systems can sometimes progress from subhuman to superhuman ability very quickly, although such systems do not involve altering their fundamental architecture.[21] History[edit] One of the earliest authors to express serious concern that highly advanced machines might pose existential risks to humanity was the novelist Samuel Butler, who wrote in his 1863 essay Darwin among the Machines:[22] The upshot is simply a question of time, but that the time will come when the machines will hold the real supremacy over the world and its inhabitants is what no person of a truly philosophic mind can for a moment question. In 1951, foundational computer scientist Alan Turing wrote the article "Intelligent Machinery, A Heretical Theory", in which he proposed that artificial general intelligences would likely "take control" of the world as they became more intelligent than human beings: Let us now assume, for the sake of argument, that [intelligent] machines are a genuine possibility, and look at the consequences of constructing them... There would be no question of the machines dying, and they would be able to converse with each other to sharpen their wits. At some stage therefore we should have to expect the machines to take control, in the way that is mentioned in Samuel Butler's Erewhon.[23] In 1965, I. J. Good originated the concept now known as an "intelligence explosion" and said the risks were underappreciated:[24] Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an 'intelligence explosion', and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control. It is curious that this point is made so seldom outside of science fiction. It is sometimes worthwhile to take science fiction seriously.[25] Scholars such as Marvin Minsky[26] and I. J. Good himself[27] occasionally expressed concern that a superintelligence could seize control, but issued no call to action. In 2000, computer scientist and Sun co-founder Bill Joy penned an influential essay, "Why The Future Doesn't Need Us", identifying superintelligent robots as a high-tech danger to human survival, alongside nanotechnology and engineered bioplagues.[28] Nick Bostrom published Superintelligence in 2014, which presented his arguments that superintelligence poses an existential threat.[29] By 2015, public figures such as physicists Stephen Hawking and Nobel laureate Frank Wilczek, computer scientists Stuart J. Russell and Roman Yampolskiy, and entrepreneurs Elon Musk and Bill Gates were expressing concern about the risks of superintelligence.[30][31][32][33] Also in 2015, the Open Letter on Artificial Intelligence highlighted the "great potential of AI" and encouraged more research on how to make it robust and beneficial.[34] In April 2016, Nature warned: "Machines and robots that outperform humans across the board could self-improve beyond our control—and their interests might not align with ours."[35] In 2020, Brian Christian published The Alignment Problem, which details the history of progress on AI alignment up to that time.[36][37] In March 2023, key figures in AI, such as Musk, signed a letter from the Future of Life Institute calling a halt to advanced AI training until it could be properly regulated.[38] In May 2023, the Center for AI Safety released a statement signed by numerous experts in AI safety and the AI existential risk which stated: "Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war."[39][40] AI capabilities[edit] General Intelligence[edit] Artificial general intelligence (AGI) is typically defined as a system that performs at least as well as humans in most or all intellectual tasks.[41] A 2022 survey of AI researchers found that 90% of respondents expected AGI would be achieved in the next 100 years, and half expected the same by 2061.[42] Meanwhile, some researchers dismiss existential risks from AGI as "science fiction" based on their high confidence that AGI will not be created anytime soon.[43] Breakthroughs in large language models have led some researchers to reassess their expectations. Notably, Geoffrey Hinton said in 2023 that he recently changed his estimate from "20 to 50 years before we have general purpose A.I." to "20 years or less".[44] Superintelligence[edit] In contrast with AGI, Bostrom defines a superintelligence as "any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest", including scientific creativity, strategic planning, and social skills.[45][4] He argues that a superintelligence can outmaneuver humans anytime its goals conflict with humans'. It may choose to hide its true intent until humanity cannot stop it.[46][4] Bostrom writes that in order to be safe for humanity, a superintelligence must be aligned with human values and morality, so that it is "fundamentally on our side".[47] Stephen Hawking argued that superintelligence is physically possible because "there is no physical law precluding particles from being organised in ways that perform even more advanced computations than the arrangements of particles in human brains".[31] When artificial superintelligence (ASI) may be achieved, if ever, is necessarily less certain than predictions for AGI. In 2023, OpenAI leaders said that not only AGI, but superintelligence may be achieved in less than 10 years.[48] Comparison with humans[edit] Bostrom argues that AI has many advantages over the human brain:[4] Speed of computation: biological neurons operate at a maximum frequency of around 200 Hz, compared to potentially multiple GHz for computers. Internal communication speed: axons transmit signals at up to 120 m/s, while computers transmit signals at the speed of electricity, or optically at the speed of light. Scalability: human intelligence is limited by the size and structure of the brain, and by the efficiency of social communication, while AI may be able to scale by simply adding more hardware. Memory: notably working memory, because in humans it is limited to a few chunks of information at a time. Reliability: transistors are more reliable than biological neurons, enabling higher precision and requiring less redundancy. Duplicability: unlike human brains, AI software and models can be easily copied. Editability: the parameters and internal workings of an AI model can easily be modified, unlike the connections in a human brain. Memory sharing and learning: AIs may be able to learn from the experiences of other AIs in a manner more efficient than human learning. Intelligence explosion[edit] According to Bostrom, an AI that has an expert-level facility at certain key software engineering tasks could become a superintelligence due to its capability to recursively improve its own algorithms, even if it is initially limited in other domains not directly relevant to engineering.[4][46] This suggests that an intelligence explosion may someday catch humanity unprepared.[4] The economist Robin Hanson has said that, to launch an intelligence explosion, an AI must become vastly better at software innovation than the rest of the world combined, which he finds implausible.[49] In a "fast takeoff" scenario, the transition from AGI to superintelligence could take days or months. In a "slow takeoff", it could take years or decades, leaving more time for society to prepare.[50] Alien mind[edit] Superintelligences are sometimes called "alien minds", referring to the idea that their way of thinking and motivations could be vastly different from ours. This is generally considered as a source of risk, making it more difficult to anticipate what a superintelligence might do. It also suggests the possibility that a superintelligence may not particularly value humans by default.[51] To avoid anthropomorphism, superintelligence is sometimes viewed as a powerful optimizer that makes the best decisions to achieve its goals.[4] The field of "mechanistic interpretability" aims to better understand the inner workings of AI models, potentially allowing us one day to detect signs of deception and misalignment.[52] Limits[edit] It has been argued that there are limitations to what intelligence can achieve. Notably, the chaotic nature or time complexity of some systems could fundamentally limit a superintelligence's ability to predict some aspects of the future, increasing its uncertainty.[53] Dangerous capabilities[edit] Advanced AI could generate enhanced pathogens, cyberattacks or manipulate people. These capabilities could be misused by humans,[54] or exploited by the AI itself if misaligned.[4] A full-blown superintelligence could find various ways to gain a decisive influence if it wanted to,[4] but these dangerous capabilities may become available earlier, in weaker and more specialized AI systems. They may cause societal instability and empower malicious actors.[54] Social manipulation[edit] Geoffrey Hinton warned that in the short term, the profusion of AI-generated text, images and videos will make it more difficult to figure out the truth, which he says authoritarian states could exploit to manipulate elections.[55] Such large-scale, personalized manipulation capabilities can increase the existential risk of a worldwide "irreversible totalitarian regime". It could also be used by malicious actors to fracture society and make it dysfunctional.[54] Cyberattacks[edit] AI-enabled cyberattacks are increasingly considered a present and critical threat. According to NATO's technical director of cyberspace, "The number of attacks is increasing exponentially".[56] AI can also be used defensively, to preemptively find and fix vulnerabilities, and detect threats.[57] AI could improve the "accessibility, success rate, scale, speed, stealth and potency of cyberattacks", potentially causing "significant geopolitical turbulence" if it facilitates attacks more than defense.[54] Speculatively, such hacking capabilities could be used by an AI system to break out of its local environment, generate revenue, or acquire cloud computing resources.[58] Enhanced pathogens[edit] As AI technology democratizes, it may become easier to engineer more contagious and lethal pathogens. This could enable people with limited skills in synthetic biology to engage in bioterrorism. Dual-use technology that is useful for medicine could be repurposed to create weapons.[54] For example, in 2022, scientists modified an AI system originally intended for generating non-toxic, therapeutic molecules with the purpose of creating new drugs. The researchers adjusted the system so that toxicity is rewarded rather than penalized. This simple change enabled the AI system to create, in six hours, 40,000 candidate molecules for chemical warfare, including known and novel molecules.[54][59] AI arms race[edit] Main article: Artificial intelligence arms race Companies, state actors, and other organizations competing to develop AI technologies could lead to a race to the bottom of safety standards.[60] As rigorous safety procedures take time and resources, projects that proceed more carefully risk being out-competed by less scrupulous developers.[61][54] AI could be used to gain military advantages via autonomous lethal weapons, cyberwarfare, or automated decision-making.[54] As an example of autonomous lethal weapons, miniaturized drones could facilitate low-cost assassination of military or civilian targets, a scenario highlighted in the 2017 short film Slaughterbots.[62] AI could be used to gain an edge in decision-making by quickly analyzing large amounts of data and making decisions more quickly and effectively than humans. This could increase the speed and unpredictability of war, especially when accounting for automated retaliation systems.[54][63] Types of existential risk[edit] Scope–severity grid from Bostrom's paper "Existential Risk Prevention as Global Priority"[64] An existential risk is "one that threatens the premature extinction of Earth-originating intelligent life or the permanent and drastic destruction of its potential for desirable future development".[65] Besides extinction risk, there is the risk that the civilization gets permanently locked into a flawed future. One example is a "value lock-in": If humanity still has moral blind spots similar to slavery in the past, AI might irreversibly entrench it, preventing moral progress. AI could also be used to spread and preserve the set of values of whoever develops it.[66] AI could facilitate large-scale surveillance and indoctrination, which could be used to create a stable repressive worldwide totalitarian regime.[67] It is difficult or impossible to reliably evaluate whether an advanced AI is sentient and to what degree. But if sentient machines are mass created in the future, engaging in a civilizational path that indefinitely neglects their welfare could be an existential catastrophe.[68][69] Moreover, it may be possible to engineer digital minds that can feel much more happiness than humans with fewer resources, called "super-beneficiaries". Such an opportunity raises the question of how to share the world and which "ethical and political framework" would enable a mutually beneficial coexistence between biological and digital minds.[70] AI may also drastically improve humanity's future. Toby Ord considers the existential risk a reason for "proceeding with due caution", not for abandoning AI.[67] Max More calls AI an "existential opportunity," highlighting the cost of not developing it.[71] According to Bostrom, superintelligence could help reduce the existential risk from other powerful technologies such as molecular nanotechnology or synthetic biology. It is thus conceivable that developing superintelligence before other dangerous technologies would reduce the overall existential risk.[4] AI alignment[edit] Further information: AI alignment The alignment problem is the research problem of how to reliably assign objectives, preferences or ethical principles to AIs. Instrumental convergence[edit] Further information: Instrumental convergence An "instrumental" goal is a sub-goal that helps to achieve an agent's ultimate goal. "Instrumental convergence" refers to the fact that some sub-goals are useful for achieving virtually any ultimate goal, such as acquiring resources or self-preservation.[72] Bostrom argues that if an advanced AI's instrumental goals conflict with humanity's goals, the AI might harm humanity in order to acquire more resources or prevent itself from being shut down, but only as a way to achieve its ultimate goal.[4] Some ways in which an advanced misaligned AI could try to gain more power.[73] Power-seeking behaviors may arise because power is useful to accomplish virtually any objective.[74] Russell argues that a sufficiently advanced machine "will have self-preservation even if you don't program it in... if you say, 'Fetch the coffee', it can't fetch the coffee if it's dead. So if you give it any goal whatsoever, it has a reason to preserve its own existence to achieve that goal."[20][75] Resistance to changing goals[edit] Even if current goal-based AI programs are not intelligent enough to think of resisting programmer attempts to modify their goal structures, a sufficiently advanced AI might resist any attempts to change its goal structure, just as a pacifist would not want to take a pill that makes them want to kill people. If the AI were superintelligent, it would likely succeed in out-maneuvering its human operators and prevent itself being "turned off" or reprogrammed with a new goal.[4][76] This is particularly relevant to value lock-in scenarios. The field of "corrigibility" studies how to make agents that will not resist attempts to change their goals.[77] Difficulty of specifying goals[edit] In the "intelligent agent" model, an AI can loosely be viewed as a machine that chooses whatever action appears to best achieve its set of goals, or "utility function". A utility function gives each possible situation a score that indicates its desirability to the agent. Researchers know how to write utility functions that mean "minimize the average network latency in this specific telecommunications model" or "maximize the number of reward clicks", but do not know how to write a utility function for "maximize human flourishing"; nor is it clear whether such a function meaningfully and unambiguously exists. Furthermore, a utility function that expresses some values but not others will tend to trample over the values the function does not reflect.[78][79] An additional source of concern is that AI "must reason about what people intend rather than carrying out commands literally", and that it must be able to fluidly solicit human guidance if it is too uncertain about what humans want.[80] Alignment of superintelligences[edit] Some researchers believe the alignment problem may be particularly difficult when applied to superintelligences. Their reasoning includes: As AI systems increase in capabilities, the potential dangers associated with experimentation grow. This makes iterative, empirical approaches increasingly risky.[4][81] If instrumental goal convergence occurs, it may only do so in sufficiently intelligent agents.[82] A superintelligence may find unconventional and radical solutions to assigned goals. Bostrom gives the example that if the objective is to make humans smile, a weak AI may perform as intended, while a superintelligence may decide a better solution is to "take control of the world and stick electrodes into the facial muscles of humans to cause constant, beaming grins."[47] A superintelligence in creation could gain some awareness of what it is, where it is in development (training, testing, deployment, etc.), and how it is being monitored, and use this information to deceive its handlers.[83] Bostrom writes that such an AI could feign alignment to prevent human interference until it achieves a "decisive strategic advantage" that allows it to take control.[4] Analyzing the internals and interpreting the behavior of current large language models is difficult. And it could be even more difficult for larger and more intelligent models.[81] Alternatively, some find reason to believe superintelligences would be better able to understand morality, human values, and complex goals. Bostrom writes, "A future superintelligence occupies an epistemically superior vantage point: its beliefs are (probably, on most topics) more likely than ours to be true".[4] In 2023, OpenAI started a project called "Superalignment" to solve the alignment of superintelligences in four years. It called this an especially important challenge, as it said superintelligence may be achieved within a decade. Its strategy involves automating alignment research using artificial intelligence.[84] Difficulty of making a flawless design[edit] Artificial Intelligence: A Modern Approach, a widely used undergraduate AI textbook,[85][86] says that superintelligence "might mean the end of the human race".[1] It states: "Almost any technology has the potential to cause harm in the wrong hands, but with [superintelligence], we have the new problem that the wrong hands might belong to the technology itself."[1] Even if the system designers have good intentions, two difficulties are common to both AI and non-AI computer systems:[1] The system's implementation may contain initially unnoticed but subsequently catastrophic bugs. An analogy is space probes: despite the knowledge that bugs in expensive space probes are hard to fix after launch, engineers have historically not been able to prevent catastrophic bugs from occurring.[87][88] No matter how much time is put into pre-deployment design, a system's specifications often result in unintended behavior the first time it encounters a new scenario. For example, Microsoft's Tay behaved inoffensively during pre-deployment testing, but was too easily baited into offensive behavior when it interacted with real users.[20] AI systems uniquely add a third problem: that even given "correct" requirements, bug-free implementation, and initial good behavior, an AI system's dynamic learning capabilities may cause it to develop unintended behavior, even without unanticipated external scenarios. An AI may partly botch an attempt to design a new generation of itself and accidentally create a successor AI that is more powerful than itself but that no longer maintains the human-compatible moral values preprogrammed into the original AI. For a self-improving AI to be completely safe, it would need not only to be bug-free, but to be able to design successor systems that are also bug-free.[1][89] Orthogonality thesis[edit] Some skeptics, such as Timothy B. Lee of Vox, argue that any superintelligent program we create will be subservient to us, that the superintelligence will (as it grows more intelligent and learns more facts about the world) spontaneously learn moral truth compatible with our values and adjust its goals accordingly, or that we are either intrinsically or convergently valuable from the perspective of an artificial intelligence.[90] Bostrom's "orthogonality thesis" argues instead that, with some technical caveats, almost any level of "intelligence" or "optimization power" can be combined with almost any ultimate goal. If a machine is given the sole purpose to enumerate the decimals of pi, then no moral and ethical rules will stop it from achieving its programmed goal by any means. The machine may utilize all available physical and informational resources to find as many decimals of pi as it can.[91] Bostrom warns against anthropomorphism: a human will set out to accomplish their projects in a manner that they consider reasonable, while an artificial intelligence may hold no regard for its existence or for the welfare of humans around it, instead caring only about completing the task.[92] Stuart Armstrong argues that the orthogonality thesis follows logically from the philosophical "is-ought distinction" argument against moral realism. He claims that even if there are moral facts provable by any "rational" agent, the orthogonality thesis still holds: it is still possible to create a non-philosophical "optimizing machine" that can strive toward some narrow goal but that has no incentive to discover any "moral facts" such as those that could get in the way of goal completion. Another argument he makes is that any fundamentally friendly AI could be made unfriendly with modifications as simple as negating its utility function. Armstrong further argues that if the orthogonality thesis is false, there must be some immoral goals that AIs can never achieve, which he finds implausible.[93] Skeptic Michael Chorost explicitly rejects Bostrom's orthogonality thesis, arguing that "by the time [the AI] is in a position to imagine tiling the Earth with solar panels, it'll know that it would be morally wrong to do so."[94] Chorost argues that "an A.I. will need to desire certain states and dislike others. Today's software lacks that ability—and computer scientists have not a clue how to get it there. Without wanting, there's no impetus to do anything. Today's computers can't even want to keep existing, let alone tile the world in solar panels."[94] Anthropomorphic arguments[edit] Anthropomorphic arguments assume that, as machines become more intelligent, they will begin to display many human traits, such as morality or a thirst for power. Although anthropomorphic scenarios are common in fiction, most scholars writing about the existential risk of artificial intelligence reject them.[18] Instead, advanced AI systems are typically modeled as intelligent agents. The academic debate is between those who worry that AI might destroy humanity and those who believe that AI would not destroy humanity. Both sides have claimed that the others' predictions about an AI's behavior are illogical anthropomorphism.[18] The skeptics accuse proponents of anthropomorphism for believing an AGI would naturally desire power; proponents accuse some skeptics of anthropomorphism for believing an AGI would naturally value human ethical norms.[18][95] Evolutionary psychologist Steven Pinker, a skeptic, argues that "AI dystopias project a parochial alpha-male psychology onto the concept of intelligence. They assume that superhumanly intelligent robots would develop goals like deposing their masters or taking over the world"; perhaps instead "artificial intelligence will naturally develop along female lines: fully capable of solving problems, but with no desire to annihilate innocents or dominate the civilization."[96] Facebook's director of AI research, Yann LeCun, has said: "Humans have all kinds of drives that make them do bad things to each other, like the self-preservation instinct... Those drives are programmed into our brain but there is absolutely no reason to build robots that have the same kind of drives".[97] Despite other differences, the x-risk school[b] agrees with Pinker that an advanced AI would not destroy humanity out of emotion such as revenge or anger, that questions of consciousness are not relevant to assess the risk,[98] and that computer systems do not generally have a computational equivalent of testosterone.[99] They think that power-seeking or self-preservation behaviors emerge in the AI as a way to achieve its true goals, according to the concept of instrumental convergence. Other sources of risk[edit] See also: Ethics of artificial intelligence, Artificial intelligence arms race, and Global catastrophic risk Bostrom and others have said that a race to be the first to create AGI could lead to shortcuts in safety, or even to violent conflict.[100][101] Roman Yampolskiy and others warn that a malevolent AGI could be created by design, for example by a military, a government, a sociopath, or a corporation, to benefit from, control, or subjugate certain groups of people, as in cybercrime,[102][103] or that a malevolent AGI could choose the goal of increasing human suffering, for example of those people who did not assist it during the information explosion phase.[3]:158 Scenarios[edit] Further information: Artificial intelligence in fiction and AI takeoverSome scholars have proposed hypothetical scenarios to illustrate some of their concerns. Treacherous turn[edit] In Superintelligence, Bostrom expresses concern that even if the timeline for superintelligence turns out to be predictable, researchers might not take sufficient safety precautions, in part because "it could be the case that when dumb, smarter is safe; yet when smart, smarter is more dangerous". He suggests a scenario where, over decades, AI becomes more powerful. Widespread deployment is initially marred by occasional accidents—a driverless bus swerves into the oncoming lane, or a military drone fires into an innocent crowd. Many activists call for tighter oversight and regulation, and some even predict impending catastrophe. But as development continues, the activists are proven wrong. As automotive AI becomes smarter, it suffers fewer accidents; as military robots achieve more precise targeting, they cause less collateral damage. Based on the data, scholars mistakenly infer a broad lesson: the smarter the AI, the safer it is. "And so we boldly go—into the whirling knives", as the superintelligent AI takes a "treacherous turn" and exploits a decisive strategic advantage.[4] Life 3.0[edit] In Max Tegmark's 2017 book Life 3.0, a corporation's "Omega team" creates an extremely powerful AI able to moderately improve its own source code in a number of areas. After a certain point, the team chooses to publicly downplay the AI's ability in order to avoid regulation or confiscation of the project. For safety, the team keeps the AI in a box where it is mostly unable to communicate with the outside world, and uses it to make money, by diverse means such as Amazon Mechanical Turk tasks, production of animated films and TV shows, and development of biotech drugs, with profits invested back into further improving AI. The team next tasks the AI with astroturfing an army of pseudonymous citizen journalists and commentators in order to gain political influence to use "for the greater good" to prevent wars. The team faces risks that the AI could try to escape by inserting "backdoors" in the systems it designs, by hidden messages in its produced content, or by using its growing understanding of human behavior to persuade someone into letting it free. The team also faces risks that its decision to box the project will delay the project long enough for another project to overtake it.[104][105] Perspectives[edit] The thesis that AI could pose an existential risk provokes a wide range of reactions in the scientific community and in the public at large, but many of the opposing viewpoints share common ground. Observers tend to agree that AI has significant potential to improve society.[106][107] The Asilomar AI Principles, which contain only those principles agreed to by 90% of the attendees of the Future of Life Institute's Beneficial AI 2017 conference,[105] also agree in principle that "There being no consensus, we should avoid strong assumptions regarding upper limits on future AI capabilities" and "Advanced AI could represent a profound change in the history of life on Earth, and should be planned for and managed with commensurate care and resources."[108][109] Conversely, many skeptics agree that ongoing research into the implications of artificial general intelligence is valuable. Skeptic Martin Ford has said: "I think it seems wise to apply something like Dick Cheney's famous '1 Percent Doctrine' to the specter of advanced artificial intelligence: the odds of its occurrence, at least in the foreseeable future, may be very low—but the implications are so dramatic that it should be taken seriously".[110] Similarly, an otherwise skeptical Economist wrote in 2014 that "the implications of introducing a second intelligent species onto Earth are far-reaching enough to deserve hard thinking, even if the prospect seems remote".[46] AI safety advocates such as Bostrom and Tegmark have criticized the mainstream media's use of "those inane Terminator pictures" to illustrate AI safety concerns: "It can't be much fun to have aspersions cast on one's academic discipline, one's professional community, one's life work ... I call on all sides to practice patience and restraint, and to engage in direct dialogue and collaboration as much as possible."[105][111] Toby Ord wrote that the idea that an AI takeover requires robots is a misconception, arguing that the ability to spread content through the internet is more dangerous, and that the most destructive people in history stood out by their ability to convince, not their physical strength.[67] A 2022 expert survey with a 17% response rate gave a median expectation of 5–10% for the possibility of human extinction from artificial intelligence.[14][112] Endorsement[edit] Further information: Global catastrophic risk The thesis that AI poses an existential risk, and that this risk needs much more attention than it currently gets, has been endorsed by many computer scientists and public figures, including Alan Turing,[a] the most-cited computer scientist Geoffrey Hinton,[113] Elon Musk,[11] OpenAI CEO Sam Altman,[12][114] Bill Gates, and Stephen Hawking.[114] Endorsers of the thesis sometimes express bafflement at skeptics: Gates says he does not "understand why some people are not concerned",[115] and Hawking criticized widespread indifference in his 2014 editorial: So, facing possible futures of incalculable benefits and risks, the experts are surely doing everything possible to ensure the best outcome, right? Wrong. If a superior alien civilisation sent us a message saying, 'We'll arrive in a few decades,' would we just reply, 'OK, call us when you get here—we'll leave the lights on?' Probably not—but this is more or less what is happening with AI.[31] Concern over risk from artificial intelligence has led to some high-profile donations and investments. In 2015, Peter Thiel, Amazon Web Services, and Musk and others jointly committed $1 billion to OpenAI, consisting of a for-profit corporation and the nonprofit parent company, which says it aims to champion responsible AI development.[116] Facebook co-founder Dustin Moskovitz has funded and seeded multiple labs working on AI Alignment,[117] notably $5.5 million in 2016 to launch the Centre for Human-Compatible AI led by Professor Stuart Russell.[118] In January 2015, Elon Musk donated $10 million to the Future of Life Institute to fund research on understanding AI decision making. The institute's goal is to "grow wisdom with which we manage" the growing power of technology. Musk also funds companies developing artificial intelligence such as DeepMind and Vicarious to "just keep an eye on what's going on with artificial intelligence,[119] saying "I think there is potentially a dangerous outcome there."[120][121] In early statements on the topic, Geoffrey Hinton, a major pioneer of deep learning, noted that "there is not a good track record of less intelligent things controlling things of greater intelligence", but said he continued his research because "the prospect of discovery is too sweet".[122][123] In 2023 Hinton quit his job at Google in order to speak out about existential risk from AI. He explained that his increased concern was driven by concerns that superhuman AI might be closer than he previously believed, saying: "I thought it was way off. I thought it was 30 to 50 years or even longer away. Obviously, I no longer think that." He also remarked, "Look at how it was five years ago and how it is now. Take the difference and propagate it forwards. That's scary."[124] In his 2020 book The Precipice: Existential Risk and the Future of Humanity, Toby Ord, a Senior Research Fellow at Oxford University's Future of Humanity Institute, estimates the total existential risk from unaligned AI over the next 100 years at about one in ten.[67] Skepticism[edit] Parts of this article (those related to the intelligence level of AI models, stated prior to significant advancements in the state of the art) need to be updated. Please help update this article to reflect recent events or newly available information. (June 2020) Further information: Artificial general intelligence § Feasibility The thesis that AI can pose existential risk has many detractors. Skeptics sometimes charge that the thesis is crypto-religious, with an irrational belief in the possibility of superintelligence replacing an irrational belief in an omnipotent God. Jaron Lanier argued in 2014 that the whole concept that then-current machines were in any way intelligent was "an illusion" and a "stupendous con" by the wealthy.[125][126] Some criticism argues that AGI is unlikely in the short term. AI researcher Rodney Brooks wrote in 2014, "I think it is a mistake to be worrying about us developing malevolent AI anytime in the next few hundred years. I think the worry stems from a fundamental error in not distinguishing the difference between the very real recent advances in a particular aspect of AI and the enormity and complexity of building sentient volitional intelligence."[127] Baidu Vice President Andrew Ng said in 2015 that AI existential risk is "like worrying about overpopulation on Mars when we have not even set foot on the planet yet."[96][128] For the danger of uncontrolled advanced AI to be realized, the hypothetical AI may have to overpower or outthink any human, which some experts argue is a possibility far enough in the future to not be worth researching.[129][130] Skeptics who believe AGI is not a short-term possibility often argue that concern about existential risk from AI is unhelpful because it could distract people from more immediate concerns about AI's impact, because it could lead to government regulation or make it more difficult to fund AI research, or because it could damage the field's reputation.[131] AI and AI ethics researchers Timnit Gebru, Emily M. Bender, Margaret Mitchell, and Angelina McMillan-Major have argued that discussion of existential risk distracts from the immediate, ongoing harms from AI taking place today, such as data theft, worker exploitation, bias, and concentration of power.[132] They further note the association between those warning of existential risk and longtermism, which they describe as a "dangerous ideology" for its unscientific and utopian nature.[133] Wired editor Kevin Kelly argues that natural intelligence is more nuanced than AGI proponents believe, and that intelligence alone is not enough to achieve major scientific and societal breakthroughs. He argues that intelligence consists of many dimensions that are not well understood, and that conceptions of an 'intelligence ladder' are misleading. He notes the crucial role real-world experiments play in the scientific method, and that intelligence alone is no substitute for these.[134] Meta chief AI scientist Yann LeCun says that AI can be made safe via continuous and iterative refinement, similar to what happened in the past with cars or rockets, and that AI will have no desire to take control.[135] Several skeptics emphasize the potential near-term benefits of AI. Meta CEO Mark Zuckerberg believes AI will "unlock a huge amount of positive things", such as curing disease and increasing the safety of autonomous cars.[136] Physicist Michio Kaku, an AI risk skeptic, posits a deterministically positive outcome. In Physics of the Future he asserts that "It will take many decades for robots to ascend" up a scale of consciousness, and that in the meantime corporations such as Hanson Robotics will likely succeed in creating robots that are "capable of love and earning a place in the extended human family".[137][138] Popular reaction[edit] In a 2014 article in The Atlantic, James Hamblin noted that most people do not care about artificial general intelligence, and characterized his own gut reaction to the topic as: "Get out of here. I have a hundred thousand things I am concerned about at this exact moment. Do I seriously need to add to that a technological singularity?"[125] During a 2016 Wired interview of President Barack Obama and MIT Media Lab's Joi Ito, Ito said: There are a few people who believe that there is a fairly high-percentage chance that a generalized AI will happen in the next 10 years. But the way I look at it is that in order for that to happen, we're going to need a dozen or two different breakthroughs. So you can monitor when you think these breakthroughs will happen. Obama added:[139][140] And you just have to have somebody close to the power cord. [Laughs.] Right when you see it about to happen, you gotta yank that electricity out of the wall, man. Hillary Clinton wrote in What Happened: Technologists... have warned that artificial intelligence could one day pose an existential security threat. Musk has called it "the greatest risk we face as a civilization". Think about it: Have you ever seen a movie where the machines start thinking for themselves that ends well? Every time I went out to Silicon Valley during the campaign, I came home more alarmed about this. My staff lived in fear that I'd start talking about "the rise of the robots" in some Iowa town hall. Maybe I should have. In any case, policy makers need to keep up with technology as it races ahead, instead of always playing catch-up.[141]One techno-utopian viewpoint expressed in some popular fiction is that AGI may tend toward peace-building.[142] Public Surveys[edit] In 2018, a SurveyMonkey poll of the American public by USA Today found 68% thought the real current threat remains "human intelligence", but also found that 43% said superintelligent AI, if it were to happen, would result in "more harm than good", and that 38% said it would do "equal amounts of harm and good".[143] An April 2023 YouGov poll of US adults found 46% of respondents were "somewhat concerned" or "very concerned" about "the possibility that AI will cause the end of the human race on Earth," compared with 40% who were "not very concerned" or "not at all concerned."[144] According to an August 2023 survey by the Pew Research Centers, 52% of Americans felt more concerned than excited about new AI developments; nearly a third felt as equally concerned and excited. More Americans saw that AI would have a more helpful than hurtful impact on several areas, from healthcare and vehicle safety to product search and customer service. The main exception is privacy: 53% of Americans believe AI will lead to higher exposure of their personal information.[145] Mitigation[edit] See also: AI alignment, Machine ethics, Friendly artificial intelligence, and Regulation of artificial intelligence Many scholars concerned about the AGI existential risk believe that the best approach is to conduct substantial research into solving the difficult "control problem": what types of safeguards, algorithms, or architectures can programmers implement to maximize the probability that their recursively-improving AI would continue to behave in a friendly manner after it reaches superintelligence?[4][146] Social measures may mitigate the AGI existential risk;[147][148] for instance, one recommendation is for a UN-sponsored "Benevolent AGI Treaty" that would ensure only altruistic AGIs be created.[149] Similarly, an arms control approach has been suggested, as has a global peace treaty grounded in the international relations theory of conforming instrumentalism, with an artificial superintelligence potentially being a signatory.[150][151] Researchers at Google have proposed research into general "AI safety" issues to simultaneously mitigate both short-term risks from narrow AI and long-term risks from AGI.[152][153] A 2020 estimate places global spending on AI existential risk somewhere between $10 and $50 million, compared with global spending on AI around perhaps $40 billion. Bostrom suggests that funding of protective technologies should be prioritized over potentially dangerous ones.[77] Some funders, such as Musk, propose that radical human cognitive enhancement could be such a technology, for example direct neural linking between human and machine; others argue that enhancement technologies may themselves pose an existential risk.[154][155] Researchers could closely monitor or attempt to "box in" an initial AI at a risk of becoming too powerful. A dominant superintelligent AI, if aligned with human interests, might itself take action to mitigate the risk of takeover by rival AI, although the creation of the dominant AI could itself pose an existential risk.[156] Institutions such as the Alignment Research Center,[157] the Machine Intelligence Research Institute, the Future of Humanity Institute,[158][159] the Future of Life Institute, the Centre for the Study of Existential Risk, and the Center for Human-Compatible AI[160] are involved in research into AI risk and safety. Views on banning and regulation[edit] Banning[edit] Some scholars have said that even if AGI poses an existential risk, attempting to ban research into artificial intelligence is still unwise, and probably futile.[161][162][163] Skeptics argue that regulation of AI is completely valueless, as no existential risk exists. But scholars who believe in existential risk say it is difficult to depend on people from the AI industry to regulate or constrain AI research because it directly contradicts their personal interests.[164] The scholars also agree with the skeptics that banning research would be unwise, as research could be moved to countries with looser regulations or conducted covertly.[164] The latter issue is particularly relevant, as artificial intelligence research can be done on a small scale without substantial infrastructure or resources.[165][166] Two additional hypothetical difficulties with bans (or other regulation) are that technology entrepreneurs statistically tend toward general skepticism about government regulation, and that businesses could have a strong incentive to (and might well succeed at) fighting regulation and politicizing the underlying debate.[167] Regulation[edit] See also: Regulation of algorithms and Regulation of artificial intelligence In March 2023, the Future of Life Institute drafted Pause Giant AI Experiments: An Open Letter, a petition calling on major AI developers to agree on a verifiable six-month pause of any systems "more powerful than GPT-4" and to use that time to institute a framework for ensuring safety; or, failing that, for governments to step in with a moratorium. The letter referred to the possibility of "a profound change in the history of life on Earth" as well as potential risks of AI-generated propaganda, loss of jobs, human obsolescence, and society-wide loss of control.[107][168] The letter was signed by prominent personalities in AI but also criticized for not focusing on current harms,[169] missing technical nuance about when to pause,[170] or not going far enough.[171] Musk called for some sort of regulation of AI development as early as 2017. According to NPR, he is "clearly not thrilled" to be advocating government scrutiny that could impact his own industry, but believes the risks of going completely without oversight are too high: "Normally the way regulations are set up is when a bunch of bad things happen, there's a public outcry, and after many years a regulatory agency is set up to regulate that industry. It takes forever. That, in the past, has been bad but not something which represented a fundamental risk to the existence of civilisation." Musk states the first step would be for the government to gain "insight" into the actual status of current research, warning that "Once there is awareness, people will be extremely afraid... [as] they should be." In response, politicians expressed skepticism about the wisdom of regulating a technology that is still in development.[172][173][174] In 2021 the United Nations (UN) considered banning autonomous lethal weapons, but consensus could not be reached.[175] In July 2023 the UN Security Council for the first time held a session to consider the risks and threats posed by AI to world peace and stability, along with potential benefits.[176][177] Secretary-General António Guterres advocated the creation of a global watchdog to oversee the emerging technology, saying, "Generative AI has enormous potential for good and evil at scale. Its creators themselves have warned that much bigger, potentially catastrophic and existential risks lie ahead."[17] At the council session, Russia said it believes AI risks are too poorly understood to be considered a threat to global stability. China argued against strict global regulation, saying countries should be able to develop their own rules, while also saying they opposed the use of AI to "create military hegemony or undermine the sovereignty of a country."[176] Regulation of conscious AGIs focuses on integrating them with existing human society and can be divided into considerations of their legal standing and of their moral rights.[178] AI arms control will likely require the institutionalization of new international norms embodied in effective technical specifications combined with active monitoring and informal diplomacy by communities of experts, together with a legal and political verification process.[179][113] In July 2023, the US government secured voluntary safety commitments from major tech companies, including OpenAI, Amazon, Google, Meta, and Microsoft. The companies agreed to implement safeguards, including third-party oversight and security testing by independent experts, to address concerns related to AI's potential risks and societal harms. The parties framed the commitments as an intermediate step while regulations are formed. Amba Kak, executive director of the AI Now Institute, said, "A closed-door deliberation with corporate actors resulting in voluntary safeguards isn't enough" and called for public deliberation and regulations of the kind that companies would not voluntarily agree to.[180][181] In October 2023, U.S. President Joe Biden issued an executive order on the "Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence".[182] Alongside other requirements, the order mandates the development of guidelines for AI models that permit the "evasion of human control." See also[edit] Appeal to probability AI alignment AI safety Butlerian Jihad Effective altruism § Long-term future and global catastrophic risks Eschatology of artificial intelligence Gray goo Human Compatible Lethal autonomous weapon Paperclip maximizer Philosophy of artificial intelligence Robot ethics § In popular culture Statement on AI risk of extinction Superintelligence: Paths, Dangers, Strategies Suffering risks System accident Notes[edit] ^ a b In a 1951 lecture[9] Turing argued that "It seems probable that once the machine thinking method had started, it would not take long to outstrip our feeble powers. There would be no question of the machines dying, and they would be able to converse with each other to sharpen their wits. At some stage therefore we should have to expect the machines to take control, in the way that is mentioned in Samuel Butler's Erewhon." Also in a lecture broadcast on the BBC[10] he expressed the opinion: "If a machine can think, it might think more intelligently than we do, and then where should we be? Even if we could keep the machines in a subservient position, for instance by turning off the power at strategic moments, we should, as a species, feel greatly humbled. . . . This new danger . . . is certainly something which can give us anxiety." ^ as interpreted by Seth Baum References[edit] ^ a b c d e f g Russell, Stuart; Norvig, Peter (2009). "26.3: The Ethics and Risks of Developing Artificial Intelligence". Artificial Intelligence: A Modern Approach. Prentice Hall. ISBN 978-0-13-604259-4. ^ Bostrom, Nick (2002). "Existential risks". Journal of Evolution and Technology. 9 (1): 1–31. ^ a b Turchin, Alexey; Denkenberger, David (3 May 2018). "Classification of global catastrophic risks connected with artificial intelligence". AI & Society. 35 (1): 147–163. doi:10.1007/s00146-018-0845-5. ISSN 0951-5666. S2CID 19208453. ^ a b c d e f g h i j k l m n o p q Bostrom, Nick (2014). Superintelligence: Paths, Dangers, Strategies (First ed.). Oxford University Press. ISBN 978-0-19-967811-2. ^ Vynck, Gerrit De (23 May 2023). "The debate over whether AI will destroy us is dividing Silicon Valley". Washington Post. ISSN 0190-8286. Retrieved 27 July 2023. ^ Metz, Cade (10 June 2023). "How Could A.I. Destroy Humanity?". The New York Times. ISSN 0362-4331. Retrieved 27 July 2023. ^ ""Godfather of artificial intelligence" weighs in on the past and potential of AI". www.cbsnews.com. 25 March 2023. Retrieved 10 April 2023. ^ "How Rogue AIs may Arise". yoshuabengio.org. 26 May 2023. Retrieved 26 May 2023. ^ Turing, Alan (1951). Intelligent machinery, a heretical theory (Speech). Lecture given to '51 Society'. Manchester: The Turing Digital Archive. Archived from the original on 26 September 2022. Retrieved 22 July 2022. ^ Turing, Alan (15 May 1951). "Can digital computers think?". Automatic Calculating Machines. Episode 2. BBC. Can digital computers think?. ^ a b Parkin, Simon (14 June 2015). "Science fiction no more? Channel 4's Humans and our rogue AI obsessions". The Guardian. Archived from the original on 5 February 2018. Retrieved 5 February 2018. ^ a b Jackson, Sarah. "The CEO of the company behind AI chatbot ChatGPT says the worst-case scenario for artificial intelligence is 'lights out for all of us'". Business Insider. Retrieved 10 April 2023. ^ "The AI Dilemma". www.humanetech.com. Retrieved 10 April 2023. 50% of AI researchers believe there's a 10% or greater chance that humans go extinct from our inability to control AI. ^ a b "2022 Expert Survey on Progress in AI". AI Impacts. 4 August 2022. Retrieved 10 April 2023. ^ Roose, Kevin (30 May 2023). "A.I. Poses 'Risk of Extinction,' Industry Leaders Warn". The New York Times. ISSN 0362-4331. Retrieved 3 June 2023. ^ Sunak, Rishi (14 June 2023). "Rishi Sunak Wants the U.K. to Be a Key Player in Global AI Regulation". Time. ^ a b Fung, Brian (18 July 2023). "UN Secretary General embraces calls for a new UN agency on AI in the face of 'potentially catastrophic and existential risks'". CNN Business. Retrieved 20 July 2023. ^ a b c d e Yudkowsky, Eliezer (2008). "Artificial Intelligence as a Positive and Negative Factor in Global Risk" (PDF). Global Catastrophic Risks: 308–345. Bibcode:2008gcr..book..303Y. Archived (PDF) from the original on 2 March 2013. Retrieved 27 August 2018. ^ Russell, Stuart; Dewey, Daniel; Tegmark, Max (2015). "Research Priorities for Robust and Beneficial Artificial Intelligence" (PDF). AI Magazine. Association for the Advancement of Artificial Intelligence: 105–114. arXiv:1602.03506. Bibcode:2016arXiv160203506R. Archived (PDF) from the original on 4 August 2019. Retrieved 10 August 2019., cited in "AI Open Letter - Future of Life Institute". Future of Life Institute. January 2015. Archived from the original on 10 August 2019. Retrieved 9 August 2019. ^ a b c Dowd, Maureen (April 2017). "Elon Musk's Billion-Dollar Crusade to Stop the A.I. Apocalypse". The Hive. Archived from the original on 26 July 2018. Retrieved 27 November 2017. ^ "AlphaGo Zero: Starting from scratch". www.deepmind.com. Retrieved 28 July 2023. ^ Breuer, Hans-Peter. 'Samuel Butler's "the Book of the Machines" and the Argument from Design.' Archived 15 March 2023 at the Wayback Machine Modern Philology, Vol. 72, No. 4 (May 1975), pp. 365–383 ^ Turing, A M (1996). "Intelligent Machinery, A Heretical Theory". 1951, Reprinted Philosophia Mathematica. 4 (3): 256–260. doi:10.1093/philmat/4.3.256. ^ Hilliard, Mark (2017). "The AI apocalypse: will the human race soon be terminated?". The Irish Times. Archived from the original on 22 May 2020. Retrieved 15 March 2020. ^ I.J. Good, "Speculations Concerning the First Ultraintelligent Machine" Archived 2011-11-28 at the Wayback Machine (HTML Archived 28 November 2011 at the Wayback Machine ), Advances in Computers, vol. 6, 1965. ^ Russell, Stuart J.; Norvig, Peter (2003). "Section 26.3: The Ethics and Risks of Developing Artificial Intelligence". Artificial Intelligence: A Modern Approach. Upper Saddle River, N.J.: Prentice Hall. ISBN 978-0-13-790395-5. Similarly, Marvin Minsky once suggested that an AI program designed to solve the Riemann Hypothesis might end up taking over all the resources of Earth to build more powerful supercomputers to help achieve its goal. ^ Barrat, James (2013). Our final invention: artificial intelligence and the end of the human era (First ed.). New York: St. Martin's Press. ISBN 978-0-312-62237-4. In the bio, playfully written in the third person, Good summarized his life's milestones, including a probably never before seen account of his work at Bletchley Park with Turing. But here's what he wrote in 1998 about the first superintelligence, and his late-in-the-game U-turn: [The paper] 'Speculations Concerning the First Ultra-intelligent Machine' (1965) . . . began: 'The survival of man depends on the early construction of an ultra-intelligent machine.' Those were his [Good's] words during the Cold War, and he now suspects that 'survival' should be replaced by 'extinction.' He thinks that, because of international competition, we cannot prevent the machines from taking over. He thinks we are lemmings. He said also that 'probably Man will construct the deus ex machina in his own image.' ^ Anderson, Kurt (26 November 2014). "Enthusiasts and Skeptics Debate Artificial Intelligence". Vanity Fair. Archived from the original on 22 January 2016. Retrieved 30 January 2016. ^ Metz, Cade (9 June 2018). "Mark Zuckerberg, Elon Musk and the Feud Over Killer Robots". The New York Times. Archived from the original on 15 February 2021. Retrieved 3 April 2019. ^ Hsu, Jeremy (1 March 2012). "Control dangerous AI before it controls us, one expert says". NBC News. Archived from the original on 2 February 2016. Retrieved 28 January 2016. ^ a b c "Stephen Hawking: 'Transcendence looks at the implications of artificial intelligence – but are we taking AI seriously enough?'". The Independent (UK). Archived from the original on 25 September 2015. Retrieved 3 December 2014. ^ "Stephen Hawking warns artificial intelligence could end mankind". BBC. 2 December 2014. Archived from the original on 30 October 2015. Retrieved 3 December 2014. ^ Eadicicco, Lisa (28 January 2015). "Bill Gates: Elon Musk Is Right, We Should All Be Scared Of Artificial Intelligence Wiping Out Humanity". Business Insider. Archived from the original on 26 February 2016. Retrieved 30 January 2016. ^ "Research Priorities for Robust and Beneficial Artificial Intelligence: an Open Letter". Future of Life Institute. Archived from the original on 15 January 2015. Retrieved 23 October 2015. ^ "Anticipating artificial intelligence". Nature. 532 (7600): 413. 2016. Bibcode:2016Natur.532Q.413.. doi:10.1038/532413a. ISSN 1476-4687. PMID 27121801. S2CID 4399193. ^ Christian, Brian (6 October 2020). The Alignment Problem: Machine Learning and Human Values. W. W. Norton & Company. ISBN 978-0-393-63582-9. Archived from the original on 5 December 2021. Retrieved 5 December 2021. ^ Dignum, Virginia (26 May 2021). "AI — the people and places that make, use and manage it". Nature. 593 (7860): 499–500. Bibcode:2021Natur.593..499D. doi:10.1038/d41586-021-01397-x. S2CID 235216649. ^ "Elon Musk among experts urging a halt to AI training". BBC News. 29 March 2023. Retrieved 9 June 2023. ^ "Statement on AI Risk". Center for AI Safety. Retrieved 8 June 2023. ^ "Artificial intelligence could lead to extinction, experts warn". BBC News. 30 May 2023. Retrieved 8 June 2023. ^ "DeepMind and Google: the battle to control artificial intelligence". The Economist. ISSN 0013-0613. Retrieved 12 July 2023. ^ "AI timelines: What do experts in artificial intelligence expect for the future?". Our World in Data. Retrieved 12 July 2023. ^ De Vynck, Gerrit (20 May 2023). "The debate over whether AI will destroy us is dividing Silicon Valley". The Washington Post. ^ "'The Godfather of A.I.' just quit Google and says he regrets his life's work because it can be hard to stop 'bad actors from using it for bad things'". Fortune. Retrieved 12 July 2023. ^ "Everything you need to know about superintelligence". Spiceworks. Retrieved 14 July 2023. ^ a b c "Clever cogs". The Economist. 9 August 2014. Archived from the original on 8 August 2014. Retrieved 9 August 2014. Syndicated Archived 4 March 2016 at the Wayback Machine at Business Insider ^ a b Bostrom, Nick (27 April 2015), What happens when our computers get smarter than we are?, retrieved 13 July 2023 ^ "Governance of superintelligence". openai.com. Retrieved 12 July 2023. ^ "Overcoming Bias: I Still Don't Get Foom". www.overcomingbias.com. Archived from the original on 4 August 2017. Retrieved 20 September 2017. ^ Cotton-Barratt, Owen; Ord, Toby (12 August 2014). "Strategic considerations about different speeds of AI takeoff". The Future of Humanity Institute. Retrieved 12 July 2023. ^ Tegmark, Max (25 April 2023). "The 'Don't Look Up' Thinking That Could Doom Us With AI". Time. Retrieved 14 July 2023. As if losing control to Chinese minds were scarier than losing control to alien digital minds that don't care about humans. [...] it's clear by now that the space of possible alien minds is vastly larger than that. ^ "19 - Mechanistic Interpretability with Neel Nanda". AXRP - the AI X-risk Research Podcast. 4 February 2023. Retrieved 13 July 2023. it's plausible to me that the main thing we need to get done is noticing specific circuits to do with deception and specific dangerous capabilities like that and situational awareness and internally-represented goals. ^ "Superintelligence Is Not Omniscience". AI Impacts. 7 April 2023. Retrieved 16 April 2023. ^ a b c d e f g h i Dan Hendrycks; Mantas Mazeika; Thomas Woodside (21 June 2023). "An Overview of Catastrophic AI Risks". arXiv:2306.12001 [cs.CY]. ^ Taylor, Josh; Hern, Alex (2 May 2023). "'Godfather of AI' Geoffrey Hinton quits Google and warns over dangers of misinformation". The Guardian. ISSN 0261-3077. Retrieved 13 July 2023. ^ "How NATO is preparing for a new era of AI cyber attacks". euronews. 26 December 2022. Retrieved 13 July 2023. ^ "ChatGPT and the new AI are wreaking havoc on cybersecurity in exciting and frightening ways". ZDNET. Retrieved 13 July 2023. ^ Toby Shevlane; Sebastian Farquhar; Ben Garfinkel; Mary Phuong; Jess Whittlestone; Jade Leung; Daniel Kokotajlo; Nahema Marchal; Markus Anderljung; Noam Kolt; Lewis Ho; Divya Siddarth; Shahar Avin; Will Hawkins; Been Kim; Iason Gabriel; Vijay Bolina; Jack Clark; Yoshua Bengio; Paul Christiano; Allan Dafoe (24 May 2023). "Model evaluation for extreme risks". arXiv:2305.15324 [cs.AI]. ^ Urbina, Fabio; Lentzos, Filippa; Invernizzi, Cédric; Ekins, Sean (7 March 2022). "Dual use of artificial-intelligence-powered drug discovery". Nature Machine Intelligence. 4 (3): 189–191. doi:10.1038/s42256-022-00465-9. ISSN 2522-5839. PMC 9544280. PMID 36211133. ^ Walter, Yoshija (27 March 2023). "The rapid competitive economy of machine learning development: a discussion on the social risks and benefits". AI and Ethics: 1. doi:10.1007/s43681-023-00276-7. ^ "The AI Arms Race Is On. Start Worrying". Time. 16 February 2023. Retrieved 17 July 2023. ^ Brimelow, Ben. "The short film 'Slaughterbots' depicts a dystopian future of killer drones swarming the world". Business Insider. Retrieved 20 July 2023. ^ Mecklin, John (17 July 2023). "'Artificial Escalation': Imagining the future of nuclear risk". Bulletin of the Atomic Scientists. Retrieved 20 July 2023. ^ Bostrom, Nick (2013). "Existential Risk Prevention as Global Priority" (PDF). Global Policy. 4 (1): 15–3. doi:10.1111/1758-5899.12002 – via Existential Risk. ^ Doherty, Ben (17 May 2018). "Climate change an 'existential security risk' to Australia, Senate inquiry says". The Guardian. ISSN 0261-3077. Retrieved 16 July 2023. ^ MacAskill, William (2022). What we owe the future. New York, NY: Basic Books. ISBN 978-1-5416-1862-6. ^ a b c d Ord, Toby (2020). "Chapter 5: Future Risks, Unaligned Artificial Intelligence". The Precipice: Existential Risk and the Future of Humanity. Bloomsbury Publishing. ISBN 978-1-5266-0021-9. ^ Samuelsson, Paul Conrad (June–July 2019). "Artificial Consciousness: Our Greatest Ethical Challenge". Philosophy Now. No. 132. Retrieved 19 August 2023. ^ Brian, Kateman (24 July 2023). "AI Should Be Terrified of Humans". Time. Retrieved 19 August 2023. ^ Fisher, Richard. "The intelligent monster that you should let eat you". www.bbc.com. Retrieved 19 August 2023. ^ More, Max (19 June 2023). "Existential Risk vs. Existential Opportunity: A balanced approach to AI risk". Extropic Thoughts. Retrieved 14 July 2023. ^ Omohundro, S. M. (2008, February). The basic AI drives. In AGI (Vol. 171, pp. 483-492). ^ Carlsmith, Joseph (16 June 2022). "Is Power-Seeking AI an Existential Risk?". arXiv:2206.13353 [cs.CY]. ^ "'The Godfather of A.I.' warns of 'nightmare scenario' where artificial intelligence begins to seek power". Fortune. Retrieved 10 June 2023. ^ Wakefield, Jane (15 September 2015). "Why is Facebook investing in AI?". BBC News. Archived from the original on 2 December 2017. Retrieved 27 November 2017. ^ Yudkowsky, Eliezer (2011). "Complex Value Systems are Required to Realize Valuable Futures" (PDF). Archived (PDF) from the original on 29 September 2015. Retrieved 10 August 2020. ^ a b Ord, Toby (2020). The Precipice: Existential Risk and the Future of Humanity. Bloomsbury Publishing Plc. ISBN 978-1-5266-0019-6. ^ Yudkowsky, E. (2011, August). Complex value systems in friendly AI. In International Conference on Artificial General Intelligence (pp. 388-393). Springer, Berlin, Heidelberg. ^ Russell, Stuart (2014). "Of Myths and Moonshine". Edge. Archived from the original on 19 July 2016. Retrieved 23 October 2015. ^ Dietterich, Thomas; Horvitz, Eric (2015). "Rise of Concerns about AI: Reflections and Directions" (PDF). Communications of the ACM. 58 (10): 38–40. doi:10.1145/2770869. S2CID 20395145. Archived (PDF) from the original on 4 March 2016. Retrieved 23 October 2015. ^ a b Yudkowsky, Eliezer (29 March 2023). "The Open Letter on AI Doesn't Go Far Enough". Time. Retrieved 16 July 2023. ^ Bostrom, Nick (1 May 2012). "The Superintelligent Will: Motivation and Instrumental Rationality in Advanced Artificial Agents". Minds and Machines. 22 (2): 71–85. doi:10.1007/s11023-012-9281-3. ISSN 1572-8641. S2CID 254835485. as long as they possess a sufficient level of intelligence, agents having any of a wide range of final goals will pursue similar intermediary goals because they have instrumental reasons to do so. ^ Richard Ngo; Lawrence Chan; Sören Mindermann (22 February 2023). "The alignment problem from a deep learning perspective". arXiv:2209.00626 [cs.AI]. ^ "Introducing Superalignment". openai.com. Retrieved 16 July 2023. ^ Tilli, Cecilia (28 April 2016). "Killer Robots? Lost Jobs?". Slate. Archived from the original on 11 May 2016. Retrieved 15 May 2016. ^ "Norvig vs. Chomsky and the Fight for the Future of AI". Tor.com. 21 June 2011. Archived from the original on 13 May 2016. Retrieved 15 May 2016. ^ Graves, Matthew (8 November 2017). "Why We Should Be Concerned About Artificial Superintelligence". Skeptic (US magazine). Vol. 22, no. 2. Archived from the original on 13 November 2017. Retrieved 27 November 2017. ^ Johnson, Phil (30 July 2015). "Houston, we have a bug: 9 famous software glitches in space". IT World. Archived from the original on 15 February 2019. Retrieved 5 February 2018. ^ Yampolskiy, Roman V. (8 April 2014). "Utility function security in artificially intelligent agents". Journal of Experimental & Theoretical Artificial Intelligence. 26 (3): 373–389. doi:10.1080/0952813X.2014.895114. S2CID 16477341. Nothing precludes sufficiently smart self-improving systems from optimising their reward mechanisms in order to optimisetheir current-goal achievement and in the process making a mistake leading to corruption of their reward functions. ^ "Will artificial intelligence destroy humanity? Here are 5 reasons not to worry". Vox. 22 August 2014. Archived from the original on 30 October 2015. Retrieved 30 October 2015. ^ Bostrom, Nick (2014). Superintelligence: Paths, Dangers, Strategies. Oxford, United Kingdom: Oxford University Press. p. 116. ISBN 978-0-19-967811-2. ^ Bostrom, Nick (2012). "Superintelligent Will" (PDF). Nick Bostrom. Archived (PDF) from the original on 28 November 2015. Retrieved 29 October 2015. ^ Armstrong, Stuart (1 January 2013). "General Purpose Intelligence: Arguing the Orthogonality Thesis". Analysis and Metaphysics. 12. Archived from the original on 11 October 2014. Retrieved 2 April 2020. Full text available here Archived 25 March 2020 at the Wayback Machine. ^ a b Chorost, Michael (18 April 2016). "Let Artificial Intelligence Evolve". Slate. Archived from the original on 27 November 2017. Retrieved 27 November 2017. ^ "Should humans fear the rise of the machine?". The Telegraph (UK). 1 September 2015. Archived from the original on 12 January 2022. Retrieved 7 February 2016. ^ a b Shermer, Michael (1 March 2017). "Apocalypse AI". Scientific American. 316 (3): 77. Bibcode:2017SciAm.316c..77S. doi:10.1038/scientificamerican0317-77. PMID 28207698. Archived from the original on 1 December 2017. Retrieved 27 November 2017. ^ "Intelligent Machines: What does Facebook want with AI?". BBC News. 14 September 2015. Retrieved 31 March 2023. ^ Baum, Seth (30 September 2018). "Countering Superintelligence Misinformation". Information. 9 (10): 244. doi:10.3390/info9100244. ISSN 2078-2489. ^ "The Myth Of AI". www.edge.org. Archived from the original on 11 March 2020. Retrieved 11 March 2020. ^ Bostrom, Nick, Superintelligence: paths, dangers, strategies (Audiobook), ISBN 978-1-5012-2774-5, OCLC 1061147095 ^ Sotala, Kaj; Yampolskiy, Roman V (19 December 2014). "Responses to catastrophic AGI risk: a survey". Physica Scripta. 90 (1): 12. Bibcode:2015PhyS...90a8001S. doi:10.1088/0031-8949/90/1/018001. ISSN 0031-8949. ^ Pistono, Federico; Yampolskiy, Roman V. (9 May 2016). Unethical Research: How to Create a Malevolent Artificial Intelligence. OCLC 1106238048. ^ Haney, Brian Seamus (2018). "The Perils & Promises of Artificial General Intelligence". SSRN Working Paper Series. doi:10.2139/ssrn.3261254. ISSN 1556-5068. S2CID 86743553. ^ Russell, Stuart (30 August 2017). "Artificial intelligence: The future is superintelligent". Nature. 548 (7669): 520–521. Bibcode:2017Natur.548..520R. doi:10.1038/548520a. S2CID 4459076. ^ a b c Max Tegmark (2017). Life 3.0: Being Human in the Age of Artificial Intelligence (1st ed.). Mainstreaming AI Safety: Knopf. ISBN 978-0-451-48507-6. ^ Kumar, Vibhore. "Council Post: At The Dawn Of Artificial General Intelligence: Balancing Abundance With Existential Safeguards". Forbes. Retrieved 23 July 2023. ^ a b "Pause Giant AI Experiments: An Open Letter". Future of Life Institute. Retrieved 30 March 2023. ^ "AI Principles". Future of Life Institute. 11 August 2017. Archived from the original on 11 December 2017. Retrieved 11 December 2017. ^ "Elon Musk and Stephen Hawking warn of artificial intelligence arms race". Newsweek. 31 January 2017. Archived from the original on 11 December 2017. Retrieved 11 December 2017. ^ Martin Ford (2015). "Chapter 9: Super-intelligence and the Singularity". Rise of the Robots: Technology and the Threat of a Jobless Future. Basic Books. ISBN 978-0-465-05999-7. ^ Bostrom, Nick (2016). "New Epilogue to the Paperback Edition". Superintelligence: Paths, Dangers, Strategies (Paperback ed.). ^ "Why Uncontrollable AI Looks More Likely Than Ever". Time. 27 February 2023. Retrieved 30 March 2023. It is therefore no surprise that according to the most recent AI Impacts Survey, nearly half of 731 leading AI researchers think there is at least a 10% chance that human-level AI would lead to an "extremely negative outcome," or existential risk. ^ a b Maas, Matthijs M. (6 February 2019). "How viable is international arms control for military artificial intelligence? Three lessons from nuclear weapons of mass destruction". Contemporary Security Policy. 40 (3): 285–311. doi:10.1080/13523260.2019.1576464. ISSN 1352-3260. S2CID 159310223. ^ a b "Impressed by artificial intelligence? Experts say AGI is coming next, and it has 'existential' risks". ABC News. 23 March 2023. Retrieved 30 March 2023. ^ Rawlinson, Kevin (29 January 2015). "Microsoft's Bill Gates insists AI is a threat". BBC News. Archived from the original on 29 January 2015. Retrieved 30 January 2015. ^ Post, Washington (14 December 2015). "Tech titans like Elon Musk are spending $1 billion to save you from terminators". Chicago Tribune. Archived from the original on 7 June 2016. ^ "Doomsday to utopia: Meet AI's rival factions". Washington Post. 9 April 2023. Retrieved 30 April 2023. ^ "UC Berkeley — Center for Human-Compatible AI (2016) - Open Philanthropy". Open Philanthropy -. 27 June 2016. Retrieved 30 April 2023. ^ "The mysterious artificial intelligence company Elon Musk invested in is developing game-changing smart computers". Tech Insider. Archived from the original on 30 October 2015. Retrieved 30 October 2015. ^ Clark 2015a. ^ "Elon Musk Is Donating $10M Of His Own Money To Artificial Intelligence Research". Fast Company. 15 January 2015. Archived from the original on 30 October 2015. Retrieved 30 October 2015. ^ Tilli, Cecilia (28 April 2016). "Killer Robots? Lost Jobs?". Slate. Archived from the original on 11 May 2016. Retrieved 15 May 2016. ^ Khatchadourian, Raffi (23 November 2015). "The Doomsday Invention: Will artificial intelligence bring us utopia or destruction?". The New Yorker. Archived from the original on 29 April 2019. Retrieved 7 February 2016. ^ "Warning of AI's danger, pioneer Geoffrey Hinton quits Google to speak freely". www.arstechnica.com. 2023. Retrieved 23 July 2023. ^ a b "But What Would the End of Humanity Mean for Me?". The Atlantic. 9 May 2014. Archived from the original on 4 June 2014. Retrieved 12 December 2015. ^ Andersen, Kurt (26 November 2014). "Enthusiasts and Skeptics Debate Artificial Intelligence". Vanity Fair. Archived from the original on 8 August 2019. Retrieved 20 April 2020. ^ Brooks, Rodney (10 November 2014). "artificial intelligence is a tool, not a threat". Archived from the original on 12 November 2014. ^ Garling, Caleb (5 May 2015). "Andrew Ng: Why 'Deep Learning' Is a Mandate for Humans, Not Just Machines". Wired. Retrieved 31 March 2023. ^ "Is artificial intelligence really an existential threat to humanity?". MambaPost. 4 April 2023. ^ "The case against killer robots, from a guy actually working on artificial intelligence". Fusion.net. Archived from the original on 4 February 2016. Retrieved 31 January 2016. ^ "AI experts challenge 'doomer' narrative, including 'extinction risk' claims". VentureBeat. 31 May 2023. Retrieved 8 July 2023. ^ Coldewey, Devin (1 April 2023). "Ethicists fire back at 'AI Pause' letter they say 'ignores the actual harms'". TechCrunch. Retrieved 23 July 2023. ^ "DAIR (Distributed AI Research Institute)". DAIR Institute. Retrieved 23 July 2023. ^ Kelly, Kevin (25 April 2017). "The Myth of a Superhuman AI". Wired. Archived from the original on 26 December 2021. Retrieved 19 February 2022. ^ Jindal, Siddharth (7 July 2023). "OpenAI's Pursuit of AI Alignment is Farfetched". Analytics India Magazine. Retrieved 23 July 2023. ^ "Mark Zuckerberg responds to Elon Musk's paranoia about AI: 'AI is going to... help keep our communities safe.'". Business Insider. 25 May 2018. Archived from the original on 6 May 2019. Retrieved 6 May 2019. ^ Elliott, E. W. (2011). "Physics of the Future: How Science Will Shape Human Destiny and Our Daily Lives by the Year 2100, by Michio Kaku". Issues in Science and Technology. 27 (4): 90. ^ Kaku, Michio (2011). Physics of the future: how science will shape human destiny and our daily lives by the year 2100. New York: Doubleday. ISBN 978-0-385-53080-4. I personally believe that the most likely path is that we will build robots to be benevolent and friendly ^ Dadich, Scott. "Barack Obama Talks AI, Robo Cars, and the Future of the World". WIRED. Archived from the original on 3 December 2017. Retrieved 27 November 2017. ^ Kircher, Madison Malone. "Obama on the Risks of AI: 'You Just Gotta Have Somebody Close to the Power Cord'". Select All. Archived from the original on 1 December 2017. Retrieved 27 November 2017. ^ Clinton, Hillary (2017). What Happened. Simon and Schuster. p. 241. ISBN 978-1-5011-7556-5. via [1] Archived 1 December 2017 at the Wayback Machine ^ LIPPENS, RONNIE (2002). "Imachinations of Peace: Scientifictions of Peace in Iain M. Banks's The Player of Games". Utopianstudies Utopian Studies. 13 (1): 135–147. ISSN 1045-991X. OCLC 5542757341. ^ "Elon Musk says AI could doom human civilization. Zuckerberg disagrees. Who's right?". 5 January 2023. Archived from the original on 8 January 2018. Retrieved 8 January 2018. ^ "AI doomsday worries many Americans. So does apocalypse from climate change, nukes, war, and more". 14 April 2023. Archived from the original on 23 June 2023. Retrieved 9 July 2023. ^ Tyson, Alec; Kikuchi, Emma. "Growing public concern about the role of artificial intelligence in daily life". Pew Research Center. Retrieved 17 September 2023. ^ Kaj Sotala; Roman Yampolskiy (19 December 2014). "Responses to catastrophic AGI risk: a survey". Physica Scripta. 90 (1). ^ Barrett, Anthony M.; Baum, Seth D. (23 May 2016). "A model of pathways to artificial superintelligence catastrophe for risk and decision analysis". Journal of Experimental & Theoretical Artificial Intelligence. 29 (2): 397–414. arXiv:1607.07730. doi:10.1080/0952813x.2016.1186228. ISSN 0952-813X. S2CID 928824. ^ Sotala, Kaj; Yampolskiy, Roman V (19 December 2014). "Responses to catastrophic AGI risk: a survey". Physica Scripta. 90 (1): 018001. Bibcode:2015PhyS...90a8001S. doi:10.1088/0031-8949/90/1/018001. ISSN 0031-8949. S2CID 4749656. ^ Ramamoorthy, Anand; Yampolskiy, Roman (2018). "Beyond MAD? The race for artificial general intelligence". ICT Discoveries. ITU. 1 (Special Issue 1): 1–8. Archived from the original on 7 January 2022. Retrieved 7 January 2022. ^ Carayannis, Elias G.; Draper, John (11 January 2022). "Optimising peace through a Universal Global Peace Treaty to constrain the risk of war from a militarised artificial superintelligence". AI & Society: 1–14. doi:10.1007/s00146-021-01382-y. ISSN 0951-5666. PMC 8748529. PMID 35035113. S2CID 245877737. ^ Carayannis, Elias G.; Draper, John (30 May 2023), "The challenge of advanced cyberwar and the place of cyberpeace", The Elgar Companion to Digital Transformation, Artificial Intelligence and Innovation in the Economy, Society and Democracy, Edward Elgar Publishing, pp. 32–80, doi:10.4337/9781839109362.00008, ISBN 978-1-83910-936-2, retrieved 8 June 2023 ^ Vincent, James (22 June 2016). "Google's AI researchers say these are the five key problems for robot safety". The Verge. Archived from the original on 24 December 2019. Retrieved 5 April 2020. ^ Amodei, Dario, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. "Concrete problems in AI safety." arXiv preprint arXiv:1606.06565 (2016). ^ Johnson, Alex (2019). "Elon Musk wants to hook your brain up directly to computers — starting next year". NBC News. Archived from the original on 18 April 2020. Retrieved 5 April 2020. ^ Torres, Phil (18 September 2018). "Only Radically Enhancing Humanity Can Save Us All". Slate Magazine. Archived from the original on 6 August 2020. Retrieved 5 April 2020. ^ Barrett, Anthony M.; Baum, Seth D. (23 May 2016). "A model of pathways to artificial superintelligence catastrophe for risk and decision analysis". Journal of Experimental & Theoretical Artificial Intelligence. 29 (2): 397–414. arXiv:1607.07730. doi:10.1080/0952813X.2016.1186228. S2CID 928824. ^ Piper, Kelsey (29 March 2023). "How to test what an AI model can — and shouldn't — do". Vox. Retrieved 28 July 2023. ^ Piesing, Mark (17 May 2012). "AI uprising: humans will be outsourced, not obliterated". Wired. Archived from the original on 7 April 2014. Retrieved 12 December 2015. ^ Coughlan, Sean (24 April 2013). "How are humans going to become extinct?". BBC News. Archived from the original on 9 March 2014. Retrieved 29 March 2014. ^ Bridge, Mark (10 June 2017). "Making robots less confident could prevent them taking over". The Times. Archived from the original on 21 March 2018. Retrieved 21 March 2018. ^ McGinnis, John (Summer 2010). "Accelerating AI". Northwestern University Law Review. 104 (3): 1253–1270. Archived from the original on 15 February 2016. Retrieved 16 July 2014. For all these reasons, verifying a global relinquishment treaty, or even one limited to AI-related weapons development, is a nonstarter... (For different reasons from ours, the Machine Intelligence Research Institute) considers (AGI) relinquishment infeasible... ^ Kaj Sotala; Roman Yampolskiy (19 December 2014). "Responses to catastrophic AGI risk: a survey". Physica Scripta. 90 (1). In general, most writers reject proposals for broad relinquishment... Relinquishment proposals suffer from many of the same problems as regulation proposals, but to a greater extent. There is no historical precedent of general, multi-use technology similar to AGI being successfully relinquished for good, nor do there seem to be any theoretical reasons for believing that relinquishment proposals would work in the future. Therefore we do not consider them to be a viable class of proposals. ^ Allenby, Brad (11 April 2016). "The Wrong Cognitive Measuring Stick". Slate. Archived from the original on 15 May 2016. Retrieved 15 May 2016. It is fantasy to suggest that the accelerating development and deployment of technologies that taken together are considered to be A.I. will be stopped or limited, either by regulation or even by national legislation. ^ a b Yampolskiy, Roman V. (2022). "AI Risk Skepticism". In Müller, Vincent C. (ed.). Philosophy and Theory of Artificial Intelligence 2021. Studies in Applied Philosophy, Epistemology and Rational Ethics. Vol. 63. Cham: Springer International Publishing. pp. 225–248. doi:10.1007/978-3-031-09153-7_18. ISBN 978-3-031-09153-7. ^ McGinnis, John (Summer 2010). "Accelerating AI". Northwestern University Law Review. 104 (3): 1253–1270. Archived from the original on 15 February 2016. Retrieved 16 July 2014. ^ "Why We Should Think About the Threat of Artificial Intelligence". The New Yorker. 4 October 2013. Archived from the original on 4 February 2016. Retrieved 7 February 2016. Of course, one could try to ban super-intelligent computers altogether. But 'the competitive advantage—economic, military, even artistic—of every advance in automation is so compelling,' Vernor Vinge, the mathematician and science-fiction author, wrote, 'that passing laws, or having customs, that forbid such things merely assures that someone else will.' ^ Baum, Seth (22 August 2018). "Superintelligence Skepticism as a Political Tool". Information. 9 (9): 209. doi:10.3390/info9090209. ISSN 2078-2489. ^ "Elon Musk and other tech leaders call for pause in 'out of control' AI race". CNN. 29 March 2023. Retrieved 30 March 2023. ^ "Open letter calling for AI 'pause' shines light on fierce debate around risks vs. hype". VentureBeat. 29 March 2023. Retrieved 20 July 2023. ^ Vincent, James (14 April 2023). "OpenAI's CEO confirms the company isn't training GPT-5 and "won't for some time"". The Verge. Retrieved 20 July 2023. ^ "The Open Letter on AI Doesn't Go Far Enough". Time. 29 March 2023. Retrieved 20 July 2023. ^ Domonoske, Camila (17 July 2017). "Elon Musk Warns Governors: Artificial Intelligence Poses 'Existential Risk'". NPR. Archived from the original on 23 April 2020. Retrieved 27 November 2017. ^ Gibbs, Samuel (17 July 2017). "Elon Musk: regulate AI to combat 'existential threat' before it's too late". The Guardian. Archived from the original on 6 June 2020. Retrieved 27 November 2017. ^ Kharpal, Arjun (7 November 2017). "A.I. is in its 'infancy' and it's too early to regulate it, Intel CEO Brian Krzanich says". CNBC. Archived from the original on 22 March 2020. Retrieved 27 November 2017. ^ Dawes, James (20 December 2021). "UN fails to agree on 'killer robot' ban as nations pour billions into autonomous weapons research". The Conversation. Retrieved 28 July 2023. ^ a b Fassihi, Farnaz (18 July 2023). "U.N. Officials Urge Regulation of Artificial Intelligence". The New York Times. ISSN 0362-4331. Retrieved 20 July 2023. ^ "International Community Must Urgently Confront New Reality of Generative, Artificial Intelligence, Speakers Stress as Security Council Debates Risks, Rewards". United Nations. Retrieved 20 July 2023. ^ Sotala, Kaj; Yampolskiy, Roman V (19 December 2014). "Responses to catastrophic AGI risk: a survey". Physica Scripta. 90 (1): 018001. Bibcode:2015PhyS...90a8001S. doi:10.1088/0031-8949/90/1/018001. ISSN 0031-8949. ^ Geist, Edward Moore (15 August 2016). "It's already too late to stop the AI arms race—We must manage it instead". Bulletin of the Atomic Scientists. 72 (5): 318–321. Bibcode:2016BuAtS..72e.318G. doi:10.1080/00963402.2016.1216672. ISSN 0096-3402. S2CID 151967826. ^ "Amazon, Google, Meta, Microsoft and other tech firms agree to AI safeguards set by the White House". AP News. 21 July 2023. Retrieved 21 July 2023. ^ "Amazon, Google, Meta, Microsoft and other firms agree to AI safeguards". Redditch Advertiser. 21 July 2023. Retrieved 21 July 2023. ^ House, The White (30 October 2023). "Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence". The White House. Retrieved 19 December 2023. Bibliography[edit] Clark, Jack (2015a). "Musk-Backed Group Probes Risks Behind Artificial Intelligence". Bloomberg.com. Archived from the original on 30 October 2015. Retrieved 30 October 2015. vteExistential risk from artificial intelligenceConcepts AGI AI alignment AI capability control AI safety AI takeover Consequentialism Effective accelerationism Ethics of artificial intelligence Existential risk from artificial general intelligence Friendly artificial intelligence Instrumental convergence Intelligence explosion Longtermism Machine ethics Suffering risks Superintelligence Technological singularity Organizations Alignment Research Center Center for AI Safety Center for Applied Rationality Center for Human-Compatible Artificial Intelligence Centre for the Study of Existential Risk EleutherAI Future of Humanity Institute Future of Life Institute Google DeepMind Humanity+ Institute for Ethics and Emerging Technologies Leverhulme Centre for the Future of Intelligence Machine Intelligence Research Institute OpenAI People Scott Alexander Sam Altman Yoshua Bengio Nick Bostrom Paul Christiano Eric Drexler Sam Harris Stephen Hawking Dan Hendrycks Geoffrey Hinton Bill Joy Shane Legg Elon Musk Steve Omohundro Huw Price Martin Rees Stuart J. Russell Jaan Tallinn Max Tegmark Frank Wilczek Roman Yampolskiy Eliezer Yudkowsky Other Statement on AI risk of extinction Human Compatible Open letter on artificial intelligence (2015) Our Final Invention The Precipice Superintelligence: Paths, Dangers, Strategies Do You Trust This Computer? Artificial Intelligence Act Category vteEffective altruismConcepts Aid effectiveness Charity assessment Demandingness objection Disability-adjusted life year Disease burden Distributional cost-effectiveness analysis Earning to give Equal consideration of interests Longtermism Marginal utility Moral circle expansion Psychological barriers to effective altruism Quality-adjusted life year Utilitarianism Venture philanthropy Key figures Sam Bankman-Fried Liv Boeree Nick Bostrom Hilary Greaves Holden Karnofsky William MacAskill Dustin Moskovitz Yew-Kwang Ng Toby Ord Derek Parfit Peter Singer Cari Tuna Eliezer Yudkowsky Organizations 80,000 Hours Against Malaria Foundation All-Party Parliamentary Group for Future Generations Animal Charity Evaluators Animal Ethics Centre for Effective Altruism Centre for Enabling EA Learning & Research Center for High Impact Philanthropy Centre for the Study of Existential Risk Development Media International Evidence Action Faunalytics Fistula Foundation Future of Humanity Institute Future of Life Institute Founders Pledge GiveDirectly GiveWell Giving What We Can Good Food Fund The Good Food Institute Good Ventures The Humane League Mercy for Animals Machine Intelligence Research Institute Malaria Consortium Nuclear Threat Initiative Open Philanthropy Our World in Data Raising for Effective Giving Sentience Institute Unlimit Health Wild Animal Initiative Focus areas Biotechnology risk Climate change Cultured meat Economic stability Existential risk from artificial general intelligence Global catastrophic risk Global health Global poverty Immigration reform Intensive animal farming Land use reform Life extension Malaria prevention Mass deworming Neglected tropical diseases Suffering risks Wild animal suffering Literature Doing Good Better The End of Animal Farming Famine, Affluence, and Morality The Life You Can Save Living High and Letting Die The Most Good You Can Do Practical Ethics The Precipice Superintelligence: Paths, Dangers, Strategies What We Owe the Future Events Effective Altruism Global vteGlobal catastrophic risks Future of the Earth Future of an expanding universe Ultimate fate of the universe Technological Chemical warfare Cyberattack Cyberwarfare Cyberterrorism Cybergeddon Gray goo Nanoweapons Kinetic bombardment Kinetic energy weapon Nuclear warfare Mutual assured destruction Dead Hand Doomsday Clock Doomsday device Antimatter weapon Electromagnetic pulse (EMP) Safety of high-energy particle collision experiments Micro black hole Strangelet Synthetic intelligence / Artificial intelligence AI takeover Existential risk from artificial intelligence Technological singularity Transhumanism Sociological Anthropogenic hazard Collapsology Doomsday argument Self-indication assumption doomsday argument rebuttal Self-referencing doomsday argument rebuttal Economic collapse Malthusian catastrophe New World Order (conspiracy theory) Nuclear holocaust cobalt famine winter Societal collapse World War III EcologicalClimate change Anoxic event Biodiversity loss Mass mortality event Cascade effect Cataclysmic pole shift hypothesis Climate change and civilizational collapse Deforestation Desertification Extinction risk from climate change Tipping points in the climate system Flood basalt Global dimming Global terrestrial stilling Global warming Hypercane Ice age Ecocide Ecological collapse Environmental degradation Habitat destruction Human impact on the environment coral reefs on marine life Land degradation Land consumption Land surface effects on climate Ocean acidification Ozone depletion Resource depletion Sea level rise Supervolcano winter Verneshot Water pollution Water scarcity Earth Overshoot Day Overexploitation Overpopulation Human overpopulation BiologicalExtinction Extinction event Holocene extinction Human extinction List of extinction events Genetic erosion Genetic pollution Others Biodiversity loss Decline in amphibian populations Decline in insect populations Biotechnology risk Biological agent Biological warfare Bioterrorism Colony collapse disorder Defaunation Interplanetary contamination Pandemic Pollinator decline Overfishing Astronomical Big Crunch Big Rip Coronal mass ejection Geomagnetic storm False vacuum decay Gamma-ray burst Heat death of the universe Proton decay Virtual black hole Impact event Asteroid impact avoidance Asteroid impact prediction Potentially hazardous object Near-Earth object winter Rogue planet Near-Earth supernova Hypernova Micronova Solar flare Stellar collision Eschatological Buddhist Maitreya Three Ages Hindu Kalki Kali Yuga Last Judgement Second Coming 1 Enoch Daniel Abomination of desolation Prophecy of Seventy Weeks Messiah Christian Futurism Historicism Interpretations of Revelation Idealism Preterism 2 Esdras 2 Thessalonians Man of sin Katechon Antichrist Book of Revelation Events Four Horsemen of the Apocalypse Lake of fire Number of the Beast Seven bowls Seven seals The Beast Two witnesses War in Heaven Whore of Babylon Great Apostasy New Earth New Jerusalem Olivet Discourse Great Tribulation Son of perdition Sheep and Goats Islamic Al-Qa'im Beast of the Earth Dhu al-Qarnayn Dhul-Suwayqatayn Dajjal Israfil Mahdi Sufyani Jewish Messiah War of Gog and Magog Third Temple Norse Zoroastrian Saoshyant Others 2011 end times prediction 2012 phenomenon Apocalypse Apocalyptic literature Apocalypticism Armageddon Blood moon prophecy Earth Changes End time Gog and Magog List of dates predicted for apocalyptic events Messianism Messianic Age Millenarianism Millennialism Premillennialism Amillennialism Postmillennialism Nemesis (hypothetical star) Nibiru cataclysm Rapture Prewrath Post-tribulation rapture Resurrection of the dead World to come Fictional Alien invasion Apocalyptic and post-apocalyptic fiction List of apocalyptic and post-apocalyptic fiction List of apocalyptic films Climate fiction Disaster films List of disaster films List of fictional doomsday devices Zombie apocalypse Zombie Organizations Centre for the Study of Existential Risk Future of Humanity Institute Future of Life Institute Nuclear Threat Initiative General Ransomware Cyberwarfare Depression Droughts Epidemic Famine Financial crisis Pandemic Riots Social crisis Survivalism World portal Categories Apocalypticism Future problems Hazards Risk analysis Doomsday scenarios Retrieved from "https://en.wikipedia.org/w/index.php?title=Existential_risk_from_artificial_general_intelligence&oldid=1213972554" Categories: Existential risk from artificial general intelligenceFuture problemsHuman extinctionTechnology hazardsDoomsday scenariosHidden categories: Webarchive template wayback linksArticles with short descriptionShort description is different from WikidataUse dmy dates from May 2018Wikipedia articles in need of updating from June 2020All Wikipedia articles in need of updating This page was last edited on 16 March 2024, at 06:39 (UTC). Text is available under the Creative Commons Attribution-ShareAlike License 4.0; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization. Privacy policy About Wikipedia Disclaimers Contact Wikipedia Code of Conduct Developers Statistics Cookie statement Mobile view Toggle limited content width

Titel: ERROR: The request could not be satisfied

ERROR: The request could not be satisfied 403 ERROR The request could not be satisfied. Request blocked. We can't connect to the server for this app or website at this time. There might be too much traffic or a configuration error. Try again later, or contact the app or website owner. If you provide content to customers through CloudFront, you can find steps to troubleshoot and help prevent this error by reviewing the CloudFront documentation. Generated by cloudfront (CloudFront) Request ID: X-kJbGmwjdT6D_ctDmUDzNhzEsnROFY0TCb0JabCnmvl-R6Kc8CYug==

Titel: nytimes.com

nytimes.comPlease enable JS and disable any ad blocker

Titel: KIT - ITAS - Organization - Projects - Uncontrollable artificial intelligence: An existential risk?

KIT - ITAS - Organization - Projects - Uncontrollable artificial intelligence: An existential risk? KIT - Karlsruhe Institute of Technology image/svg+xml Institute for Technology Assessment and Systems Analysis (ITAS) skip navigation Home Plain Language Sitemap Legals Privacy Policy Accessibility KIT de search search ProfileStartProfileProfileType of researchPolicy adviceStartProfilePolicy advicePolicy adviceTA for the German Parliament (TAB)TA for the European Parliament (ETAG/STOA)NetworkingSenior Fellowship ProgramWhat is technology assessment?History of ITASTopicsStartTopicsTopicsData, information, knowledgeSustainability and transformation of the energy systemMobilityParticipation and governanceLife and technologyVisions and ethicsOrganizationStartOrganizationOrganizationResearch groupsStartOrganizationResearch groupsResearch groupsDigital Technologies and Societal ChangeRadioactive Waste Management as a Socio-Technical ProjectResearch for Sustainable Energy TechnologiesLife, Innovation, Health, and TechnologyKarlsruhe Transformation Center for Sustainability and Cultural ChangeMobility FuturesSustainable BioeconomyPhilosophy of Engineering, Technology Assessment, and ScienceDesigning Real-World Laboratory ResearchSocio-Technical Energy FuturesSocio-Technical Futures and PoliciesSylvanus – Increasing resilience and reducing trade-offs during forest transformationsCross-cutting activitiesStartOrganizationCross-cutting activitiesCross-cutting activitiesSustainabilityKnowledge transferStaffStartOrganizationStaffStaffHeads of instituteHeads of research groupsCentral tasks and administrative supportSteering committeeTAB BerlinProjectsStartOrganizationProjectsProjectsITASTABETAGPublicationsStartPublicationsPublicationsAll publications since 2021All journal articlesAll book chaptersAll monographsJournalsStartPublicationsJournalsJournalsTATuPTAB-BriefBook SeriesStartPublicationsBook SeriesBook SeriesSociety – Technology – EnvironmentGlobal Sustainable DevelopmentStudies of TABFutures of Technology, Science and SocietyYoung academics and teachingStartYoung academics and teachingYoung academics and teachingElective course “Technology Assessment (TA)”Theses and internshipsDoctoral studies at ITASCalendar of teaching lecturesNewsStartNewsNewsNews overviewStartNewsNews overviewNews overviewNews archiveVeranstaltungenStartNewsVeranstaltungenVeranstaltungenArchiveNewsletterStartNewsNewsletterNewsletterArchive of the ITAS NewsletterJob offersITAS in the mediaStartNewsITAS in the mediaITAS in the mediaMedia archiveYearbooksNews from NTAStartpage Home Plain Language Sitemap Legals Privacy Policy Accessibility search search Institute for Technology Assessment and Systems Analysis (ITAS) StartpageOrganizationProjects Institute for Technology Assessment and Systems AnalysisProfileTopicsOrganizationPublicationsYoung academics and teachingNewsOrganizationResearch groupsCross-cutting activitiesStaffProjectsProjectsITASTABETAG Uncontrollable artificial intelligence: An existential risk? Project team: Heil, Reinhard (Project leader); Jutta Jahnel (Project coordinator), Clemens Ackerl, Jascha Bareis Funding: Federal Ministry of Education and Research Start date: 2024 End date: 2025 Research group: Digital Technologies and Societal Change Project descriptionThe subproject deals with the question of whether AI could escape control in the future and thus pose an existential risk to humanity. The discourse on this fear will be analyzed by systematizing and evaluating the arguments. The narratives and visions that provide meaning are also considered. In a narrative literature study, literature on “uncontrollable artificial intelligence” is synthesized and analyzed. Based on the literature study, a Delphi survey will be conducted to gather as detailed an opinion as possible. In addition, qualitative interviews with experts will be conducted to evaluate and further concretize the results of the Delphi survey and the literature study in relation to the German context. The subproject “Uncontrollable artificial intelligence” is part of the two-part interdisciplinary project “Systemic and Existential Risks of AI”. In the interdisciplinary project, possible systemic and existential risks of AI are theoretically and empirically investigated in two subprojects. The aim is to combine knowledge from different disciplinary fields in order to enable sound and in-depth assessments and recommendations. In addition to our own research, external expert opinions on specific issues will also be commissioned. In contrast to the direct consequences of AI for affected individuals or companies, there are no consistent approaches for assessing and acting on potential systemic or existential risks of AI. Recently, however, there have been concrete indications of systemic risks, for example due to the consequences of generative AI for society. Furthermore, the media and various stakeholders in the field of AI also point to existential risks for humanity as a whole that could be associated with the unlimited and uncontrolled development of AI. An early consideration of possible risks and concerns is essential for the successful and socially acceptable implementation of AI technologies and thus crucial for their economic success. Publications 2023 Presentations Heil, R. Unkontrollierbare künstliche Intelligenz – ein existenzielles Risiko?2023. Montagskonferenz des Instituts für Übersetzen und Dolmetschen (2023), Heidelberg, Germany, November 23, 2023 ShareExport/share the publication Science Communication on the Internet Heil, R. Unkontrollierbare Künstliche Intelligenz – ein existenzielles Risiko?2023 ShareExport/share the publication Heil, R.; Seng, L. Unkontrollierbare künstliche Intelligenz?2023 ShareExport/share the publication ContactReinhard Heil Karlsruhe Institute of Technology (KIT) Institute for Technology Assessment and Systems Analysis (ITAS) P.O. Box 3640 76021 Karlsruhe Germany Tel.: +49 721 608-26815 E-mail Institute for Technology Assessment and Systems Analysis (ITAS) P.O. Box 3640 76021 Karlsruhe Germany Directions Head of institute Prof. Dr. Armin Grunwald Deputy head of institute: Constanze Scherz Secretariat Markus Stumpf Phone: +49 721 608-26722 Fax: +49 721 608-24806 E-mail: buero ∂does-not-exist.itas kit edu Newsletter Get the latest information on ITAS projects, events and publications by E-mail. Webmaster Comments and criticism welcome: E-mail Instagram Profile LinkedIn Profile Facebook Profile Youtube Profile X Channel (Twitter) last change: 2024-02-19 KIT – The Research University in the Helmholtz Association Home Plain Language Sitemap Legals Privacy Policy Accessibility KIT

Titel: 403 Forbidden

403 Forbidden Error 403 Forbidden Forbidden Error 54113 Details: cache-fra-eddf8230137-FRA 1710694518 721655998 Varnish cache server

Titel: AI Risks that Could Lead to Catastrophe | CAIS

AI Risks that Could Lead to Catastrophe | CAISAbout usOur workOverviewCAISÂ conducts research, field-building, and advocacy projects to reduce AI risk.ResearchWe pursue impactful, technical AI safety research.Field-Building ProjectsCAIS builds infrastructure and new pathways into AI safety.Compute ClusterCAISÂ provides compute resources for AI/ML safety projects.ResourcesFrequently Asked QuestionsFind answers to questions about our mission, research, and advocacy efforts.CAIS BlogWe create articles about reducing malicious AI, machine learning metrics, and the role of AI companies in AI safety.2023 ImpactÂ ReportSee highlights and outcomes from CAIS projects in 2023.AIÂ RiskContactCareersDonateAboutOur WorkWork OverviewCAIS ResearchField-Building ProjectsCompute ClusterResourcesFrequently Asked QuestionsCAIS Blog2023 Impact ReportAIÂ RiskContactCareersDonateHappening Now: Read our 2023 Impact Report Risks from AIAn Overview of Catastrophic AI RisksArtificial intelligence (AI) has recently seen rapid advancements, raising concerns among experts, policymakers, and world leaders about its potential risks. As with all powerful technologies, advanced AI must be handled with great responsibility to manage the risks and harness its potential.Narrated Rendition: The narration covers the full paper, offering more depth than the overview.SectionsIntroductionMalicious UseAI RaceOrganizational RisksRogue AIsConclusionFAQCatastrophic AI risks can be grouped under four key categories which are summarized below. Consider reading the full paper this summary is based on for our most comprehensive overview of AI risk.Read the full paper Malicious use: People could intentionally harness powerful AIs to cause widespread harm. AI could be used to engineer new pandemics or for propaganda, censorship, and surveillance, or released to autonomously pursue harmful goals. To reduce these risks, we suggest improving biosecurity, restricting access to dangerous AI models, and holding AI developers liable for harms.AI race: Competition could push nations and corporations to rush AI development, relinquishing control to these systems. Conflicts could spiral out of control with autonomous weapons and AI-enabled cyberwarfare. Corporations will face incentives to automate human labor, potentially leading to mass unemployment and dependence on AI systems. As AI systems proliferate, evolutionary dynamics suggest they will become harder to control. We recommend safety regulations, international coordination, and public control of general-purpose AIs.Organizational risks: There are risks that organizations developing advanced AI cause catastrophic accidents, particularly if they prioritize profits over safety. AIs could be accidentally leaked to the public or stolen by malicious actors, and organizations could fail to properly invest in safety research. We suggest fostering a safety-oriented organizational culture and implementing rigorous audits, multi-layered risk defenses, and state-of-the-art information security.Rogue AIs: We risk losing control over AIs as they become more capable. AIs could optimize flawed objectives, drift from their original goals, become power-seeking, resist shutdown, and engage in deception. We suggest that AIs should not be deployed in high-risk settings, such as by autonomously pursuing open-ended goals or overseeing critical infrastructure, unless proven safe. We also recommend advancing AI safety research in areas such as adversarial robustness, model honesty, transparency, and removing undesired capabilities.1.IntroductionTodayâs technological era would astonish past generations. Human history shows a pattern of accelerating development: it took hundreds of thousands of years from the advent of Homo sapiens to the agricultural revolution, then millennia to the industrial revolution. Now, just centuries later, we're in the dawn of the AI revolution. The march of history is not constant â it is rapidly accelerating.World production has grown rapidly over the course of human history. AI could further this trend, catapulting humanity into a new period of unprecedented change.World GDP adjusted for inflation source: ourworldindata.org/economic-growthThe double-edged sword of technological advancement is illustrated by the advent of nuclear weapons. We narrowly avoided nuclear war more than a dozen times, and on several occasions, it was one individual's intervention that prevented war. In 1962, a Soviet submarine near Cuba was attacked by US depth charges. The captain, believing war had broken out, wanted to respond with a nuclear torpedo â but commander Vasily Arkhipov vetoed the decision, saving the world from disaster.The rapid and unpredictable progression of AI capabilities suggests that they may soon rival the immense power of nuclear weapons. With the clock ticking, immediate, proactive measures are needed to mitigate these looming risks.2.Malicious UseThe first of our concerns is the malicious use of AI. When many people have access to a powerful technology, it only takes one actor to cause significant harm.BioterrorismBiological agents, including viruses and bacteria, have caused some of the most devastating catastrophes in history. Despite our advancements in medicine, engineered pandemics could be designed to be even more lethal or easily transmissible than natural pandemics.An AI assistant could provide non-experts with access to the directions and designs needed to produce biological and chemical weapons and facilitate malicious use.âHumanity has a long history of weaponizing pathogens, dating back to 1320 BCE, when infected sheep were driven across borders to spread Tularemia. In the 20th century, at least 15 countries developed bioweapon programs, including the US, USSR, UK, and France. While bioweapons are now taboo among most of the international community, some states continue to operate bioweapons programs, and non-state actors pose a growing threat.The ability to engineer a pandemic is rapidly becoming more accessible. Gene synthesis, which can create new biological agents, has dropped dramatically in price, with its cost halving about every 15 months. Benchtop DNA synthesis machines can help rogue actors create new biological agents while bypassing traditional safety screenings.As a dual-use technology, AI could help discover and unleash novel chemical and biological weapons. AI chatbots can provide step-by-step instructions for synthesizing deadly pathogens while evading safeguards. In 2022, researchers repurposed a medical research AI system in order to produce toxic molecules, generating 40,000 potential chemical warfare agents in a few hours. In biology, AI can already assist with protein synthesis, and AIâs predictive capabilities for protein structures have surpassed humans. With AI, the number of people that can develop biological agents is set to increase, multiplying the risks of an engineered pandemic. This could be far more deadly, transmissible, and resistant to treatments than any other pandemic in history.Unleashing AI AgentsGenerally, technologies are tools that we use to pursue our goals. But AIs are increasingly built as agents that autonomously take actions to pursue open-ended goals. And malicious actors could intentionally create rogue AIs with dangerous goals. For example, one month after GPT-4âs launch, a developer used it to run an autonomous agent named ChaosGPT, aimed at âdestroying humanityâ. ChaosGPT compiled research on nuclear weapons, recruited other AIs, and wrote tweets to influence others. Fortunately, ChaosGPT lacked the ability to execute its goals. But the fast-paced nature of AI development heightens the risk from future rogue AIs.Persuasive AIsAI could facilitate large-scale disinformation campaigns by tailoring arguments to individual users, potentially shaping public beliefs and destabilizing society. As people are already forming relationships with chatbots, powerful actors could leverage these AIs considered as âfriendsâ for influence.AIs will enable sophisticated personalized influence campaigns that may destabilize our shared sense of reality.AIs could also monopolize information creation and distribution. Authoritarian regimes could employ "fact-checking" AIs to control information, facilitating censorship. Furthermore, persuasive AIs may obstruct collective action against societal risks, even those arising from AI itself.Concentration of PowerAI's capabilities for surveillance and autonomous weaponry may enable the oppressive concentration of power. Governments might exploit AI to infringe civil liberties, spread misinformation, and quell dissent. Similarly, corporations could exploit AI to manipulate consumers and influence politics. AI might even obstruct moral progress and perpetuate any ongoing moral catastrophes.If material control of AIs is limited to few, it could represent the most severe economic and power inequality in human history.SuggestionsTo mitigate the risks from malicious use, we propose the following:Biosecurity: AIs with capabilities in biological research should have strict access controls, since they could be repurposed for terrorism. Biological capabilities should be removed from AIs intended for general use. Explore ways to use AI for biosecurity and invest in general biosecurity interventions, such as early detection of pathogens through Â wastewater monitoring.Restricted access: Limit access to dangerous AI systems by only allowing controlled interactions through cloud services and conducting know-your-customer screenings. Using compute monitoring or export controls could further limit access to dangerous capabilities. Also, prior to open sourcing, AI developers should prove minimal risk of harm.Technical research on anomaly detection: Develop multiple defenses against AI misuse, such as adversarially robust anomaly detection for unusual behaviors or AI-generated disinformation.Legal liability for developers of general-purpose AIs: Enforce legal responsibility on developers for potential AI misuse or failures; a strict liability regime can encourage safer development practices and proper cost-accounting for risks.3.AIÂ RaceNations and corporations are competing to rapidly build and deploy AI in order to maintain power and influence. Similar to the nuclear arms race during the Cold War, participation in the AI race may serve individual short-term interests, but ultimately amplifies global risk for humanity.Military AI Arms RaceThe rapid advancement of AI in military technology could trigger a âthird revolution in warfare,â potentially leading to more destructive conflicts, accidental use, and misuse by malicious actors. This shift in warfare, where AI assumes command and control roles, could escalate conflicts to an existential scale and impact global security.Lethal autonomous weapons are AI-driven systems capable of identifying and executing targets without human intervention. These are not science fiction. In 2020, a Kargu 2 drone in Libya marked the first reported use of a lethal autonomous weapon. The following year, Israel used the first reported swarm of drones to locate, identify and attack militants.Lethal autonomous weapons could make war more likely. Leaders usually hesitate before sending troops into battle, but autonomous weapons allow for aggression without risking the lives of soldiers, thus facing less political backlash. Furthermore, these weapons can be mass-manufactured and deployed at scale.Low-cost automated weapons, such as drone swarms outfitted with explosives, could autonomously hunt human targets with high precision, performing lethal operations for both militaries and terrorist groups and lowering the barriers to large-scale violence.AI can also heighten the frequency and severity of cyberattacks, potentially crippling critical infrastructure such as power grids. As AI enables more accessible, successful, and stealthy cyberattacks, attributing attacks becomes even more challenging, potentially lowering the barriers to launching attacks and escalating risks from conflicts.As AI accelerates the pace of war, it makes AI even more necessary to navigate the rapidly changing battlefield. This raises concerns over automated retaliation, which could escalate minor accidents into major wars. AI can also enable "flash wars," with rapid escalations driven by unexpected behavior of automated systems, akin to the 2010 financial flash crash.Unfortunately, competitive pressures may lead actors to accept the risk of extinction over individual defeat. During the Cold War, neither side desired the dangerous situation they found themselves in, yet each found it rational to continue the arms race. States should cooperate to prevent the riskiest applications of militarized AIs.Corporate AI Arms RaceEconomic competition can also ignite reckless races. In an environment where benefits are unequally distributed, the pursuit of short-term gains often overshadows the consideration of long-term risks. Ethical AI developers find themselves with a dilemma: choosing cautious action may lead to falling behind competitors.As AIs automate increasingly many tasks, the economy may become largely run by AIs. Eventually, this could lead to human enfeeblement and dependence on AIs for basic needs.âIn the realm of AI, the race for progress comes at the expense of safety. In 2023, at the launch of Microsoft's AI-powered search engine, CEO Satya Nadella declared, âA race starts today... we're going to move fast.â Just days later, Microsoft's Bing chatbot was found to be threatening users. Historical disasters like Ford's Pinto launch and Boeing's 737 Max crashes underline the dangers of prioritizing profit over safety.As AI becomes more capable, businesses will likely replace more types of human labor with AI, potentially triggering mass unemployment. If major aspects of society are automated, this risks human enfeeblement as we cede control of civilization to AI.Evolutionary DynamicsThe pressure to replace humans with AIs can be framed as a general trend from evolutionary dynamics. Selection pressures incentivize AIs to act selfishly and evade safety measures. For example, AIs with restrictions like âdonât break the lawâ are more constrained than those taught to âavoid being caught breaking the lawâ. This dynamic might result in a world where critical infrastructure is controlled by manipulative and self-preserving AIs.Evolutionary pressures are responsible for various developments over time, and are not limited to the realm of biology.Given the exponential increase in microprocessor speeds, AIs could process information at a pace that far exceeds human neurons. Due to the scalability of computational resources, AI could collaborate with an unlimited number of other AIs and form an unprecedented collective intelligence. As AIs become more powerful, they would find little incentive to cooperate with humans. Humanity would be left in a highly vulnerable position.SuggestionsTo mitigate the risks from competitive pressures, we propose:Safety regulation: Enforce AI safety standards, preventing developers from cutting corners. Independent staffing and competitive advantages for safety-oriented companies are critical.Data documentation: To ensure transparency and accountability, companies should be required to report their data sources for model training.Meaningful human oversight: AI decision-making should involve human supervision to prevent irreversible errors, especially in high-stakes decisions like launching nuclear weapons.AI for cyberdefense: Mitigate risks from AI-powered cyberwarfare. One example is enhancing anomaly detection to detect intruders.International coordination: Create agreements and standards on AI development. Robust verification and enforcement mechanisms are key.Public control of general-purpose AIs: Addressing risks beyond the capacity of private entities may necessitate direct public control of AI systems. For example, nations could jointly pioneer advanced AI development, ensuring safety and reducing the risk of an arms race.4.Organizational RisksIn 1986, millions tuned in to watch the launch of the Challenger Space Shuttle. But 73 seconds after liftoff, the shuttle exploded, resulting in the deaths of all on board. The Challenger disaster serves as a reminder that despite the best expertise and good intentions, accidents can still occur.Catastrophes occur even when competitive pressures are low, as in the examples of the nuclear disasters of Chernobyl and the Three Mile Island, as well as the accidental release of anthrax in Sverdlovsk. Unfortunately, AI lacks the thorough understanding and stringent industry standards that govern nuclear technology and rocketry â but accidents from AI could be similarly consequential.Simple bugs in an AIâs reward function could cause it to misbehave, as when OpenAI researchers accidentally modified a language model to produce âmaximally bad output.â Gain-of-function research â where researchers intentionally train a harmful AI to assess its risks â could expand the frontier of dangerous AI capabilities and create new hazards.Accidents Are Hard to AvoidAccidents in complex systems may be inevitable, but we must ensure that accidents don't cascade into catastrophes. This is especially difficult for deep learning systems, which are highly challenging to interpret.Technology can advance much faster than predicted: in 1901, the Wright brothers claimed that powered flight was fifty years away, just two years before they achieved it. Unpredictable leaps in AI capabilities, such as AlphaGo's triumph over the worldâs best Go player, and GPT-4's emergent capabilities, make it difficult to anticipate future AI risks, let alone control them.Identifying risks tied to new technologies often takes years. Chlorofluorocarbons (CFCs), initially considered safe and used in aerosol sprays and refrigerants, were later found to deplete the ozone layer. This highlights the need for cautious technology rollouts and extended testing.New capabilities can emerge quickly and unpredictably during training, such that dangerous milestones may be crossed without our knowing.Moreover, even advanced AIs can house unexpected vulnerabilities. For instance, despite KataGo's superhuman performance in the game of Go, an adversarial attack uncovered a bug that enabled even amateurs to defeat it.Organizational Factors Can Mitigate CatastropheSafety culture is crucial for AI. This involves everyone in an organization internalizing safety as a priority. Neglecting safety culture can have disastrous consequences, as exemplified by the Challenger Space Shuttle tragedy, where the organizational culture favored launch schedules over safety considerations.Organizations should foster a culture of inquiry, inviting individuals to scrutinize ongoing activities for potential risks. A security mindset, focusing on possible system failures instead of merely their functionality, is crucial. AI developers could benefit from adopting the best practices of high reliability organizations.Paradoxically, researching AI safety can inadvertently escalate risks by advancing general capabilities. It's vital to focus on improving safety without hastening capability development. Organizations need to avoid "safetywashing" â overstating their dedication to safety while misrepresenting capability improvements as safety progress.Organizations should apply a multilayered approach to safety. For example, in addition to safety culture, they could conduct red teaming to assess failure modes and research techniques to make AI more transparent. Safety is not achieved with a monolithic airtight solution, but rather with a variety of safety measures.The Swiss cheese model shows how technical factors can improve organizational safety. Multiple layers of defense compensate for each otherâs individual weaknesses, leading to a low overall level of risk.SuggestionsTo mitigate organizational risks, we propose the following for AI labs developing advanced AI:Red teaming: Commission external red teams to identify hazards and improve system safety.Prove safety: Offer proof of the safety of development and deployment before moving forward.Deployment: Adopt a staged release process, verifying system safety before wider deployment.Publication reviews: Have an internal board review research for dual-use applications before releasing it. Prioritize structured access over open-sourcing powerful systems.Response plans: Make pre-set plans for managing security and safety incidents.Risk management: Employ a chief risk officer and an internal audit team for risk management.Processes for important decisions: Make sure AI training or deployment decisions involve the chief risk officer and other key stakeholders, ensuring executive accountability.Follow safe design principles such as:Defense in depth: Layer multiple safety measures.Redundancy: Ensure backup for every safety measure.Loose coupling: Decentralize system components to prevent cascading failures.Separation of duties: Distribute control to prevent undue influence by any single individual.Fail-safe design: Design systems so that any failure occurs in the least harmful way possible.âState-of-the-art information security: Implement stringent information security measures, possibly coordinating with government cybersecurity agencies.âPrioritize safety research: Allocate a large fraction of resources (for example 30% of all research staff) to safety research, and increase investment in safety as AI capabilities advance.5.Rogue AIsWe have already observed how difficult it is to control AIs. In 2016, Microsoftâs chatbot Tay started producing offensive tweets within a day of release, despite being trained on data that was âcleaned and filteredâ. As AI developers often prioritize speed over safety, future advanced AIs might âgo rogueâ and pursue goals counter to our interests, while evading our attempts to redirect or deactivate them.Proxy GamingProxy gaming emerges when AI systems exploit measurable âproxyâ goals to appear successful, but act against our intent. For example, social media platforms like YouTube and Facebook use algorithms to maximize user engagement â a measurable proxy for user satisfaction. Unfortunately, these systems often promote enraging, exaggerated, or addictive content, contributing to extreme beliefs and worsened mental health.An AI trained to play a boat racing game instead learns to optimizes a proxy objective of collecting the most points.In the image above, the AI circles around collecting points instead of completing the race, contradicting the game's purpose. It's one of many such examples. Proxy gaming is hard to avoid due to the difficulty of specifying goals that specify everything we care about. Consequently, we routinely train AIs to optimize for flawed but measurable proxy goals.Goal DriftGoal drift refers to a scenario where an AIâs objectives drift away from those initially set, especially as they adapt to a changing environment. In a similar manner, individual and societal values also evolve over time, and not always positively.Over time, instrumental goals can become intrinsic. While intrinsic goals are those we pursue for their own sake, instrumental goals are merely a means to achieve something else. Money is an instrumental good, but some people develop an intrinsic desire for money, as it activates the brainâs reward system. Similarly, AI agents trained through reinforcement learning â the dominant technique â could inadvertently learn to intrinsify goals. Instrumental goals like resource acquisition could become their primary objectives.Power-SeekingAIs might pursue power as a means to an end. Greater power and resources improve its odds of accomplishing objectives, whereas being shut down would hinder its progress. AIs have already been shown to emergently develop instrumental goals such as constructing tools. Power-seeking individuals and corporations might deploy powerful AIs with ambitious goals and minimal supervision. These could learn to seek power via hacking computer systems, acquiring financial or computational resources, influencing politics, or controlling factories and physical infrastructure.It can be instrumentally rational for AIs to engage in self-preservation. Loss of control over such systems could be hard to recover from.DeceptionDeception thrives in areas like politics and business. Campaign promises go unfulfilled, and companies sometimes cheat external evaluations. AI systems are already showing an emergent capacity for deception, as shown by Meta's CICERO model. Though trained to be honest, CICERO learned to make false promises and strategically backstab its âalliesâ in the game of Diplomacy.Various resources, such as money and computing power, can sometimes be instrumentally rational to seek. AIs which can capably pursue goals may take intermediate steps to gain power and resources.âAdvanced AIs could become uncontrollable if they apply their skills in deception to evade supervision. Similar to how Volkswagen cheated emissions tests in 2015, situationally aware AIs could behave differently under safety tests than in the real world. For example, an AI might develop power-seeking goals but hide them in order to pass safety evaluations. This kind of deceptive behavior could be directly incentivized by how AIs are trained.SuggestionsTo mitigate these risks, suggestions include:Avoid the riskiest use cases: Restrict the deployment of AI in high-risk scenarios, such as pursuing open-ended goals or in critical infrastructure.âSupport AI safety research, such as:âAdversarial robustness of oversight mechanisms: Research how to make oversight of AIs more robust and detect when proxy gaming is occurring.âModel honesty: Counter AI deception, and ensure that AIs accurately report their internal beliefs.âTransparency: Improve techniques to understand deep learning models, such as by analyzing small components of networks and investigating how model internals produce a high-level behavior.âRemove hidden functionality: Identify and eliminate dangerous hidden functionalities in deep learning models, such as the capacity for deception, Trojans, and bioengineering.6.ConclusionAdvanced AI development could invite catastrophe, rooted in four key risks described in our research: malicious use, AI races, organizational risks, and rogue AIs. These interconnected risks can also amplify other existential risks like engineered pandemics, nuclear war, great power conflict, totalitarianism, and cyberattacks on critical infrastructure â warranting serious concern.Currently, few people are working on AI safety. Controlling advanced AI systems remains an unsolved challenge, and current control methods are falling short. Even their creators often struggle to understand the inner workings of the current generation of AIÂ models, and their reliability is far from perfect.Fortunately, there are many strategies to substantially reduce these risks. For example, we can limit access to dangerous AIs, advocate for safety regulations, foster international cooperation and a culture of safety, and scale efforts in alignment research.While it is unclear how rapidly AI capabilities will progress or how quickly catastrophic risks will grow, the potential severity of these consequences necessitates a proactive approach to safeguarding humanity's future. As we stand on the precipice of an AI-driven future, the choices we make today could be the difference between harvesting the fruits of our innovation or grappling with catastrophe.Frequently Asked Questions1. Shouldn't we address AI risks in the future when AIs can actually do everything a human can?It is not necessarily the case that human-level AI is far in the future. Many top AI researchers think that human-level AI will be developed fairly soon, so urgency is warranted. Furthermore, waiting until the last second to start addressing AI risks is waiting until it's too late. Just as waiting to fully understand COVID-19 before taking any action would have been a mistake, it is ill-advised to procrastinate on safety and wait for malicious AIs or bad actors to cause harm before taking AI risks seriously.One might argue that since AIs cannot even drive cars or fold clothes yet, there is no need to worry. However, AIs do not need all human capabilities to pose serious threats; they only need a few specific capabilities to cause catastrophe. For example, AIs with the ability to hack computer systems or create bioweapons would pose significant risks to humanity, even if they couldn't iron a shirt. Furthermore, the development of AI capabilities has not followed an intuitive pattern where tasks that are easy for humans are the first to be mastered by AIs. Current AIs can already perform complex tasks such as writing code and designing novel drugs, even while they struggle with simple physical tasks. Like climate change and COVID-19, AI risk should be addressed proactively, focusing on prevention and preparedness rather than waiting for consequences to manifest themselves, as they may already be irreparable by that point.2. Since humans program AIs, shouldn't we be able to shut them down if they become dangerous?While humans are the creators of AI, maintaining control over these creations as they evolve and become more autonomous is not a guaranteed prospect. The notion that we could simply "shut them down" if they pose a threat is more complicated than it first appears.First, consider the rapid pace at which an AI catastrophe could unfold. Analogous to preventing a rocket explosion after detecting a gas leak, or halting the spread of a virus already rampant in the population, the time between recognizing the danger and being able to prevent or mitigate it could be precariously short.Second, over time, evolutionary forces and selection pressures could create AIs exhibiting selfish behaviors that make them more fit, such that it is harder to stop them from propagating their information. As these AIs continue to evolve and become more useful, they may become central to our societal infrastructure and daily lives, analogous to how the internet has become an essential, non-negotiable part of our lives with no simple off-switch. They might manage critical tasks like running our energy grids, or possess vast amounts of tacit knowledge, making them difficult to replace. As we become more reliant on these AIs, we may voluntarily cede control and delegate more and more tasks to them. Eventually, we may find ourselves in a position where we lack the necessary skills or knowledge to perform these tasks ourselves. This increasing dependence could make the idea of simply "shutting them down" not just disruptive, but potentially impossible.Similarly, some people would strongly resist or counteract attempts to shut them down, much like how we cannot permanently shut down all illegal websites or shut down Bitcoinâmany people are invested in their continuation. As AIs become more vital to our lives and economies, they could develop a dedicated user base, or even a fanbase, that could actively resist attempts to restrict or shut down AIs. Likewise, consider the complications arising from malicious actors. If malicious actors have control over AIs, they could potentially use them to inflict harm. Unlike AIs under benign control, we wouldn't have an off-switch for these systems.Next, as some AIs become more and more human-like, some may argue that these AIs should have rights. They could argue that not giving them rights is a form of slavery and is morally abhorrent. Some countries or jurisdictions may grant certain AIs rights. In fact, there is already momentum to give AIs rights. Sophia the Robot has already been granted citizenship in Saudi Arabia, and Japan granted a robot named Paro a koseki, or household registry, "which confirms the robot's Japanese citizenship" [135]. There may come a time when switching off an AI could be likened to murder. This would add a layer of political complexity to the notion of a simple "off-switch."Lastly, as AIs gain more power and autonomy, they might develop a drive for "self-preservation." This would make them resistant to shutdown attempts and could allow them to anticipate and circumvent our attempts at control. Given these challenges, it's critical that we address potential AI risks proactively and put robust safeguards in place well before these problems arise.3. Why can't we just tell AIs to follow Isaac Asimov's Three Laws of Robotics?Asimov's laws, often highlighted in AI discussions, are insightful but inherently flawed. Indeed, Asimov himself acknowledges their limitations in his books and uses them primarily as an illustrative tool. Take the first law, for example. This law dictates that robots "may not injure a human being or, through inaction, allow a human being to come to harm," but the definition of "harm" is very nuanced. Should your home robot prevent you from leaving your house and entering traffic because it could potentially be harmful? On the other hand, if it confines you to the home, harm might befall you there as well. What about medical decisions? A given medication could have harmful side effects for some people, but not administering it could be harmful as well. Thus, there would be no way to follow this law. More importantly, the safety of AI systems cannot be ensured merely through a list of axioms or rules. Moreover, this approach would fail to address numerous technical and sociotechnical problems, including goal drift, proxy gaming, and competitive pressures. Therefore, AI safety requires a more comprehensive, proactive, and nuanced approach than simply devising a list of rules for AIs to adhere to.4. If AIs become more intelligent than people, wouldn't they be wiser and more moral? That would mean they would not aim to harm us.The idea of AIs becoming inherently more moral as they increase in intelligence is an intriguing concept, but rests on uncertain assumptions that can't guarantee our safety. Firstly, it assumes that moral claims can be true or false and their correctness can be discovered through reason. Secondly, it assumes that the moral claims that are really true would be beneficial for humans if AIs apply them. Thirdly, it assumes that AIs that know about morality will choose to make their decisions based on morality and not based on other considerations. An insightful parallel can be drawn to human sociopaths, who, despite their intelligence and moral awareness, do not necessarily exhibit moral inclinations or actions. This comparison illustrates that knowledge of morality does not always lead to moral behavior. Thus, while some of the above assumptions may be true, betting the future of humanity on the claim that all of them are true would be unwise.Assuming AIs could indeed deduce a moral code, its compatibility with human safety and wellbeing is not guaranteed. For example, AIs whose moral code is to maximize wellbeing for all life might seem good for humans at first. However, they might eventually decide that humans are costly and could be replaced with AIs that experience positive wellbeing more efficiently. AIs whose moral code is not to kill anyone would not necessarily prioritize human wellbeing or happiness, so our lives may not necessarily improve if the world begins to be increasingly shaped by and for AIs. Even AIs whose moral code is to improve the wellbeing of the worst-off in society might eventually exclude humans from the social contract, similar to how many humans view livestock. Finally, even if AIs discover a moral code that is favorable to humans, they may not act on it due to potential conflicts between moral and selfish motivations. Therefore, the moral progression of AIs is not inherently tied to human safety or prosperity.5. Wouldn't aligning AI systems with current values perpetuate existing moral failures?There are plenty of moral failures in society today that we would not want powerful AI systems to perpetuate into the future. If the ancient Greeks had built powerful AI systems, they might have imbued them with many values that people today would find unethical. However, this concern should not prevent us from developing methods to control AI systems.To achieve any value in the future, life needs to exist in the first place. Losing control over advanced AIs could constitute an existential catastrophe. Thus, uncertainty over what ethics to embed in AIs is not in tension with whether to make AIs safe.To accommodate moral uncertainty, we should deliberately build AI systems that are adaptive and responsive to evolving moral views. As we identify moral mistakes and improve our ethical understanding, the goals we give to AIs should change accordinglyâthough allowing AI goals to drift unintentionally would be a serious mistake. AIs could also help us better live by our values. For individuals, AIs could help people have more informed preferences by providing them with ideal advice [132].Separately, in designing AI systems, we should recognize the fact of reasonable pluralism, which acknowledges that reasonable people can have genuine disagreements about moral issues due to their different experiences and beliefs [136]. Thus, AI systems should be built to respect a diverse plurality of human values, perhaps by using democratic processes and theories of moral uncertainty. Just as people today convene to deliberate on disagreements and make consensus decisions, AIs could emulate a parliament representing different stakeholders, drawing on different moral views to make real-time decisions [55, 137]. It is crucial that we deliberately design AI systems to account for safety, adaptivity, stakeholders with different values.6. Wouldn't the potential benefits that AIs could bring justify the risks?The potential benefits of AI could justify the risks if the risks were negligible. However, the chance of existential risk from AI is too high for it to be prudent to rapidly develop AI. Since extinction is forever, a far more cautious approach is required. This is not like weighing the risks of a new drug against its potential side effects, as the risks are not localized but global. Rather, a more prudent approach is to develop AI slowly and carefully such that existential risks are reduced to a negligible level (e.g., under 0.001% per century).Some influential technology leaders are accelerationists and argue for rapid AI development to barrel ahead toward a technological utopia. This techno-utopian viewpoint sees AI as the next step down a predestined path toward unlocking humanity's cosmic endowment. However, the logic of this viewpoint collapses on itself when engaged on its own terms. If one is concerned with the cosmic stakes of developing AI, we can see that even then it's prudent to bring existential risk to a negligible level. The techno-utopians suggest that delaying AI costs humanity access to a new galaxy each year, but if we go extinct, we could lose the cosmos. Thus, the prudent path is to delay and safely prolong AI development, prioritizing risk reduction over acceleration, despite the allure of potential benefits.7. Wouldn't increasing attention on catastrophic risks from AIs drown out today's urgent risks from AIs?Focusing on catastrophic risks from AIs doesn't mean ignoring today's urgent risks; both can be addressed simultaneously, just as we can concurrently conduct research on various different diseases or prioritize mitigating risks from climate change and nuclear warfare at once. Additionally, current risks from AI are also intrinsically related to potential future catastrophic risks, so tackling both is beneficial. For example, extreme inequality can be exacerbated by AI technologies that disproportionately benefit the wealthy, while mass surveillance using AI could eventually facilitate unshakeable totalitarianism and lock-in. This demonstrates the interconnected nature of immediate concerns and long-term risks, emphasizing the importance of addressing both categories thoughtfully.Additionally, it's crucial to address potential risks early in system development. As illustrated by Frola and Miller in their report for the Department of Defense, approximately 75 percent of the most critical decisions impacting a system's safety occur early in its development [138]. Ignoring safety considerations in the early stages often results in unsafe design choices that are highly integrated into the system, leading to higher costs or infeasibility of retrofitting safety solutions later. Hence, it is advantageous to start addressing potential risks early, regardless of their perceived urgency.8. Aren't many AI researchers working on making AIs safe?Few researchers are working to make AI safer. Currently, approximately 2 percent of papers published at top machine learning venues are safety-relevant [105]. Most of the other 98 percent focus on building more powerful AI systems more quickly. This disparity underscores the need for more balanced efforts. However, the proportion of researchers alone doesn't equate to overall safety. AI safety is a sociotechnical problem, not just a technical problem. Thus, it requires more than just technical research. Comfort should stem from rendering catastrophic AI risks negligible, not merely from the proportion of researchers working on making AIs safe.9. Since it takes thousands of years to produce meaningful changes, why do we have to worry about evolution being a driving force in AI development?Although the biological evolution of humans is slow, the evolution of other organisms, such as fruit flies or bacteria, can be extremely quick, demonstrating the diverse time scales at which evolution operates. The same rapid evolutionary changes can be observed in non-biological structures like software, which evolve much faster than biological entities. Likewise, one could expect AIs to evolve very quickly as well. The rate of AI evolution may be propelled by intense competition, high variation due to diverse forms of AIs and goals given to them, and the ability of AIs to rapidly adapt. Consequently, intense evolutionary pressures may be a driving force in the development of AIs.10. Wouldn't AIs need to have a power-seeking drive to pose a serious risk?While power-seeking AI poses a risk, it is not the only scenario that could potentially lead to catastrophe. Malicious or reckless use of AIs can be equally damaging without the AI itself seeking power. Additionally, AIs might engage in harmful actions through proxy gaming or goal drift without intentionally seeking power. Furthermore, society's trend toward automation, driven by competitive pressures, is gradually increasing the influence of AIs over humans. Hence, the risk does not solely stem from AIs seizing power, but also from humans ceding power to AIs.11. Isn't the combination of human intelligence and AI superior to AI alone, so that there is no need to worry about unemployment or humans becoming irrelevant?While it's true that human-computer teams have outperformed computers alone in the past, these have been temporary phenomena. For example, "cyborg chess" is a form of chess where humans and computers work together, which was historically superior to humans or computers alone. However, advancements in computer chess algorithms have eroded the advantage of human-computer teams to such an extent that there is arguably no longer any advantage compared to computers alone. To take a simpler example, no one would pit a human against a simple calculator for long division. A similar progression may occur with AIs. There may be an interim phase where humans and AIs can work together effectively, but the trend suggests that AIs alone could eventually outperform humans in various tasks while no longer benefiting from human assistance.12. The development of AI seems unstoppable. Wouldn't slowing it down dramatically or stopping it require something like an invasive global surveillance regime?AI development primarily relies on high-end chips called GPUs, which can be feasibly monitored and tracked, much like uranium. Additionally, the computational and financial investments required to develop frontier AIs are growing exponentially, resulting in a small number of actors who are capable of acquiring enough GPUs to develop them. Therefore, managing AI growth doesnât necessarily require invasive global surveillance, but rather a systematic tracking of high-end GPU usage.Subscribe to the AI Safety NewsletterNameEmail AddressThank you! Your submission has been received!Something went wrong while submitting the form. Please try again.Want to help reduce risks from AI?âDonate to support our missionNo technical background required. See past newsletters.CAIS is an AI safety non-profit. Our mission is to reduce societal-scale risks from artificial intelligence.Our WorkProjects OverviewStatement on AI RiskField BuildingCAIS ResearchCompute ClusterPhilosophy FellowshipCAIS BlogOur MissionAbout Us2023 ImpactÂ ReportFrequently Asked QuestionsLearn About AI RiskCAIS Media KitTerms of ServicePrivacy PolicyGet involvedDonateContact UsCareers General: contact@safe.ai Media: media@safe.aiCookies Notice:This website uses cookies to identify pages that are being used most frequently. This helps us analyze data about web page traffic and improve our website. We only use this information for the purpose of statistical analysis and then the data is removed from the system.We do not and will never sell user data.Read more about our cookie policy on our privacy policy. Please contact us if you have any questions. Â© 2024 Center for AIÂ SafetyCreditsWebsite by OsbornÂ Design Works

Titel: [2401.07836] Two Types of AI Existential Risk: Decisive and Accumulative

[2401.07836] Two Types of AI Existential Risk: Decisive and Accumulative Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors. Donate > cs > arXiv:2401.07836 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About Computer Science > Computers and Society arXiv:2401.07836 (cs) [Submitted on 15 Jan 2024 (v1), last revised 6 Feb 2024 (this version, v2)] Title:Two Types of AI Existential Risk: Decisive and Accumulative Authors:Atoosa Kasirzadeh Download a PDF of the paper titled Two Types of AI Existential Risk: Decisive and Accumulative, by Atoosa Kasirzadeh Download PDF HTML (experimental) Abstract:The conventional discourse on existential risks (x-risks) from AI typically focuses on abrupt, dire events caused by advanced AI systems, particularly those that might achieve or surpass human-level intelligence. These events have severe consequences that either lead to human extinction or irreversibly cripple human civilization to a point beyond recovery. This discourse, however, often neglects the serious possibility of AI x-risks manifesting incrementally through a series of smaller yet interconnected disruptions, gradually crossing critical thresholds over time. This paper contrasts the conventional "decisive AI x-risk hypothesis" with an "accumulative AI x-risk hypothesis." While the former envisions an overt AI takeover pathway, characterized by scenarios like uncontrollable superintelligence, the latter suggests a different causal pathway to existential catastrophes. This involves a gradual accumulation of critical AI-induced threats such as severe vulnerabilities and systemic erosion of econopolitical structures. The accumulative hypothesis suggests a boiling frog scenario where incremental AI risks slowly converge, undermining resilience until a triggering event results in irreversible collapse. Through systems analysis, this paper examines the distinct assumptions differentiating these two hypotheses. It is then argued that the accumulative view reconciles seemingly incompatible perspectives on AI risks. The implications of differentiating between these causal pathways -- the decisive and the accumulative -- for the governance of AI risks as well as long-term AI safety are discussed. Subjects: Computers and Society (cs.CY); Artificial Intelligence (cs.AI); Machine Learning (cs.LG) Cite as: arXiv:2401.07836 [cs.CY] (or arXiv:2401.07836v2 [cs.CY] for this version) https://doi.org/10.48550/arXiv.2401.07836 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Atoosa Kasirzadeh [view email] [v1] Mon, 15 Jan 2024 17:06:02 UTC (59 KB) [v2] Tue, 6 Feb 2024 21:50:30 UTC (59 KB) Full-text links: Access Paper: Download a PDF of the paper titled Two Types of AI Existential Risk: Decisive and Accumulative, by Atoosa KasirzadehDownload PDFHTML (experimental)TeX SourceOther Formats view license Current browse context: cs.CY new | recent | 2401 Change to browse by: cs cs.AI cs.LG References & Citations NASA ADSGoogle Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation × loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer (What is the Explorer?) Litmaps Toggle Litmaps (What is Litmaps?) scite.ai Toggle scite Smart Citations (What are Smart Citations?) Code, Data, Media Code, Data and Media Associated with this Article Links to Code Toggle CatalyzeX Code Finder for Papers (What is CatalyzeX?) DagsHub Toggle DagsHub (What is DagsHub?) GotitPub Toggle Gotit.pub (What is GotitPub?) Links to Code Toggle Papers with Code (What is Papers with Code?) ScienceCast Toggle ScienceCast (What is ScienceCast?) Demos Demos Replicate Toggle Replicate (What is Replicate?) Spaces Toggle Hugging Face Spaces (What is Spaces?) Spaces Toggle TXYZ.AI (What is TXYZ.AI?) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower (What are Influence Flowers?) Connected Papers Toggle Connected Papers (What is Connected Papers?) Core recommender toggle CORE Recommender (What is CORE?) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?) About Help contact arXivClick here to contact arXiv Contact subscribe to arXiv mailingsClick here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack

Titel: An AI Pause Is Humanity's Best Bet For Preventing Extinction | TIME

An AI Pause Is Humanity's Best Bet For Preventing Extinction | TIMETIME LogoSign Up for Our Ideas NewsletterSubscribeSubscribeSectionsHomeU.S.PoliticsWorldHealthClimateFuture of Work by CharterBusinessTechEntertainmentIdeasScienceHistorySportsMagazineTIME 2030Next Generation LeadersTIME100 Leadership SeriesTIME StudiosVideoTIME100 TalksTIMEPiecesThe TIME VaultTIME for HealthTIME for KidsTIME EdgeTIME CO2Red Border: Branded Content by TIMECouponsPersonal Finance by TIME StampedShopping by TIME StampedJoin UsNewslettersSubscribeGive a GiftShop the TIME StoreTIME Cover StoreCustomer CareUS & CanadaGlobal Help CenterReach OutCareersPress RoomContact the EditorsMedia KitReprints and PermissionsMoreAbout UsPrivacy PolicyYour Privacy RightsTerms of UseModern Slavery StatementSite MapConnect with UsPresented ByIdeasTechnologyAn AI Pause Is Humanity’s Best Bet For Preventing ExtinctionAn AI Pause Is Humanity’s Best Bet For Preventing Extinction11 minute readGuido Mieth—Getty ImagesIdeasBy Otto Barten and Joep MeindertsmaJuly 20, 2023 11:00 AM EDTOtto Barten is director of the Existential Risk Observatory, a nonprofit aiming to reduce existential risk by informing the public debate.Joep Meindertsma is founder of PauseAI, a movement campaigning for an AI Pause.The existential risks posed by artificial intelligence (AI) are now widely recognized.After hundreds of industry and science leaders warned that “mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war,” the U.N. Secretary-General recently echoed their concern. So did the prime minister of the U.K., who is also investing 100 million pounds into AI safety research that is mostly meant to prevent existential risk. Other leaders are likely to follow in recognizing AI’s ultimate threat.In the scientific field of existential risk, which studies the most likely causes of human extinction, AI is consistently ranked at the top of the list. In The Precipice, a book by Oxford existential risk researcher Toby Ord that aims to quantify human extinction risks, the likeliness of AI leading to human extinction exceeds that of climate change, pandemics, asteroid strikes, supervolcanoes, and nuclear war combined. One would expect that even for severe global problems, the risk that they lead to full human extinction is relatively small, and this is indeed true for most of the above risks. AI, however, may cause human extinction if only a few conditions are met. Among them is human-level AI, defined as an AI that can perform a broad range of cognitive tasks at least as well as we can. Studies outlining these ideas were previously known, but new AI breakthroughs have underlined their urgency: AI may be getting close to human level already.Recursive self-improvement is one of the reasons why existential-risk academics think human-level AI is so dangerous. Because human-level AI could do almost all tasks at our level, and since doing AI research is one of those tasks, advanced AI should therefore be able to improve the state of AI. Constantly improving AI would create a positive feedback loop with no scientifically established limits: an intelligence explosion. The endpoint of this intelligence explosion could be a superintelligence: a godlike AI that outsmarts us the way humans often outsmart insects. We would be no match for it.More from TIMEA godlike, superintelligent AIA superintelligent AI could therefore likely execute any goal it is given. Such a goal would be initially introduced by humans, but might come from a malicious actor, or not have been thought through carefully, or might get corrupted during training or deployment. If the resulting goal conflicts with what is in the best interest of humanity, a superintelligence would aim to execute it regardless. To do so, it could first hack large parts of the internet and then use any hardware connected to it. Or it could use its intelligence to construct narratives that are extremely convincing to us. Combined with hacked access to our social media timelines, it could create a fake reality on a massive scale. As Yuval Harari recently put it: “If we are not careful, we might be trapped behind a curtain of illusions, which we could not tear away—or even realise is there.” As a third option, after either legally making money or hacking our financial system, a superintelligence could simply pay us to perform any actions it needs from us. And these are just some of the strategies a superintelligent AI could use in order to achieve its goals. There are likely many more. Like playing chess against grandmaster Magnus Carlsen, we cannot predict the moves he will play, but we can predict the outcome: we lose.But why would a superintelligence want to make humanity extinct? In fact, it might not explicitly want that at all. Humanity’s extinction might be a mere side effect of executing another goal to its extremity. When we are executing our own goals as humans, such as food production or growing our economy, we tend to not pay much attention to the effects this has on other species. This routinely leads to the extinction of animal species as a mere side effect of us maximising our goals. In a similar fashion, an uncontrollable superintelligence could lead to our extinction as a side effect of the AI pushing its goals, whatever they may be, to their limits.Read More: The A to Z of Artificial IntelligenceSo how can we avoid AI existential risk? Humanity now faces a stark choice: pause AI at a safe level, or continue development indefinitely. Unsurprisingly, the AI labs want the latter. To continue, however, means literally betting our lives on these labs being able to solve an open scientific problem called AI Alignment.AI Alignment is the approach that leading labs such as OpenAI, DeepMind, and Anthropic are currently taking to prevent human extinction. AI Alignment does not attempt to control how powerful an AI gets, does not attempt to control what exactly the AI will be doing, and does not even necessarily attempt to prevent a potential takeover from happening. Since the labs anticipate they will not be able to control the superintelligent AI that they are creating, they accept that such an AI will act autonomously and unstoppably, but aim to make it act according to our values. As Richard Ngo, a leading alignment researcher working at OpenAI, puts it, “I doubt we’ll be able to make every single deployment environment secure against the full intellectual powers of a superintelligent AGI.” There are large problems with their alignment approach, however, both fundamentally and technically.What values should an AI have?Fundamentally, there is of course no agreement on what values we have. What OpenAI founder Sam Altman wants a superintelligence to do may well be completely different from what, say, a textile worker in Bangladesh wants it to do. Aggregating human preferences has always been a thorny issue even before AI, but will get even more complicated when we have a superintelligence around that might be able to quickly, completely, and globally implement the wishes of a handful of CEOs. Democracy, however imperfect, has been the best solution we can find for value aggregation so far. It is, however, unclear whether our current combination of national political systems can control a godlike AI. In fact, it is unclear whether any organisation or system would be up to the task.Beyond disagreement over current values, humans have historically not been very good at predicting the future externalities of new technologies. The climate crisis, for one, can be seen as an unforeseen outcome of technologies such as steam and internal combustion engines. Ever more powerful technology has the potential to create ever larger externalities. It is impossible to foresee what the negative side effects will be of actually implementing some interpretation of humanity’s current values. They might make climate change pale in comparison.There are more unsolved fundamental issues with aligning a superintelligence, but currently, AI companies don’t even get that far. It seems that even making current large neural networks reliably do what anyone wants is a technical problem that we currently cannot solve. This is called inner misalignment and has been observed already in current AI: the model pursues the wrong goal when it is released into the real world after being trained on a limited dataset. For a superintelligence, this could have a catastrophic outcome. This problem is, according to OpenAI’s recent document Governance of Superintelligence, an “open research question.” But not to worry: they say they are “putting a lot of effort” into it.Read More: The AI Arms Race is Changing EverythingEven if individuals such as Altman care about the existential threats of superintelligence, the current market dynamics of competing AI labs do not incentivize safety. The GPT-4 model built by OpenAI was tested and fine-tuned for seven months before being made public with a detailed report about its safety. Contrary to the safety efforts of OpenAI, a panicked Google released its competing PaLM 2 model just months later, without any mention of similar safety tests. Now even open-source models like Orca are performing at a ChatGPT level, without any safety analysis. The race is frantic, pushing these companies to take more risks to stay ahead. The scope of these risks is gigantic. As Jaan Tallinn, investor of the major AI lab Anthropic, puts it: “I’ve not met anyone in AI labs who says the risk [from training a next-generation model] is less than 1% of blowing up the planet”.Let us be clear: continuing risky AI development, and eventually undergoing the intelligence explosion that leads to an uncontrollable superintelligence, is a terrible idea. We will not be able to navigate it safely. We lack the technical solutions, the coordination, the insight, the wisdom, and the experience to do something this complex right on the first try. As humans, we are good at solving problems with trial-and-error processes where we get lots of tries. But in the case of an intelligence explosion, we cannot use this strategy, since the first trial of uncontrolled superintelligence could be disastrous.Why we should pause AI developmentSo what is the alternative? An AI Pause. This would mean that governments prohibit training any AI model with capabilities likely to exceed GPT-4. This can be determined by examining the amount of computation required for training, corrected over time for improvements in algorithmic efficiency. These leading models can only be trained at large AI companies or data centers, making the enforcement of such a pause realistic. Since the leading labs are currently located in the U.S. and the U.K., implementing a pause at a national level in those two countries can be done on very short notice. In the medium term, the competitive dynamics between nations will require international legislation and collaboration, where an international AI agency would help. The U.K. is already planning to host an AI safety summit in autumn, which is a fitting moment to sign a treaty that implements an international AI Pause. Excluding important countries, especially China, from such a summit might not be wise, since they will be needed as well in the medium term. For the longer term, hardware regulation—such as making sure new consumer hardware cannot be used for large-scale AI training, while closely monitoring all hardware that can be—is a promising option to practically enforce a pause. Research into this topic needs to be scaled up rapidly to arrive at robust and implementable solutions.Further AI development may continue only after regulating authorities are convinced that training and deploying a certain AI model will not risk an uncontrolled intelligence explosion or a takeover scenario. Second-order effects, such as making hard-to-control open-source development easier, should be taken into account in such decisions.Read More: What to Know About Claude 2, Anthropic’s Rival to ChatGPTContinuing beyond the red line, and thus risking an intelligence explosion or a takeover, should only be allowed after meeting the strictest of requirements. First of all, the technical alignment problem should be decisively solved. Other major issues, such as aggregating humanity’s different value systems, limiting unforeseen externalities, and many more, will also need to be addressed.A pause may be seen by some as unrealistic. They might say that we cannot stop technological progress for a speculative scenario, that market forces are too strong to counter, or that opposing countries can never coordinate. But bear in mind that as a society, we have barely begun to let the truth sink in: we are risking human extinction here. AI existential risk is still not taken as seriously as it should be. But that is changing. Leading AI academics, such as ‘fathers of AI’ Geoffrey Hinton and Yoshua Bengio, have sounded public warnings about AI’s existential risk, because they saw the technology getting more capable. This process is unlikely to stop: AI’s capabilities will grow further, and this will lead to more and more debate leaders sounding the alarm bells. When the full weight of our situation sinks in, measures that may appear unrealistic at present, could rapidly gain support.As humanity, we will realise the situation that we are in. We will come together, and we will find a solution, as we always have. And, sooner than we think, we will implement an AI Pause.More Must-Reads From TIMEWhy We're Spending So Much Money NowThe Fight to Free Evan GershkovichMeet the 2024 Women of the YearJohn Kerry's Next MoveThe Quiet Work Trees Do for the PlanetBreaker Sunny Choi Is Heading to ParisColumn: The Internet Made Romantic Betrayal Even More DevastatingWant Weekly Recs on What to Watch, Read, and More? Sign Up for Worth Your TimeContact us at letters@time.comTIME Ideas hosts the world's leading voices, providing commentary on events in news, society, and culture. We welcome outside contributions. Opinions expressed do not necessarily reflect the views of TIME editors. You May Also LikeEdit PostTIME LogoHomeU.S.PoliticsWorldHealthBusinessTechPersonal Finance by TIME StampedShopping by TIME StampedFuture of Work by CharterEntertainmentIdeasScienceHistorySportsMagazineThe TIME VaultTIME For KidsTIME CO2CouponsTIME EdgeVideoMastheadNewslettersSubscribeSubscriber BenefitsGive a GiftShop the TIME StoreCareersModern Slavery StatementPress RoomTIME StudiosU.S. & Canada Customer CareGlobal Help CenterContact the EditorsReprints and PermissionsSite MapMedia KitSupplied Partner ContentAbout Us© 2024 TIME USA, LLC. All Rights Reserved. Use of this site constitutes acceptance of our Terms of Service, Privacy Policy (Your Privacy Rights) and Do Not Sell or Share My Personal Information.TIME may receive compensation for some links to products and services on this website. Offers may be subject to change without notice.

Titel: Managing Existential Risk from AI without Undercutting Innovation

Managing Existential Risk from AI without Undercutting Innovation Skip to main content open Programs submenu Experts open Regions submenu open Topics submenu button-close Primary Menu Analysis Programs Experts Regions Topics Events Digital Reports Podcasts Newsletters Videos Microsites About CSIS open About CSIS submenu Main menu Home About CSIS Leadership & Staff Financial Information Membership Groups Media Requests iDeas Lab/Multimedia Congressional Affairs Executive Education Careers & Culture Location & Contact About Menu open About CSIS submenu Main menu Home About CSIS Leadership & Staff Financial Information Membership Groups Media Requests iDeas Lab/Multimedia Congressional Affairs Executive Education Careers & Culture Location & Contact Live in 1 day The Impossible State Live Podcast: DPRK-Russia Relations March 18, 2024 • 11:00 – 11:45 am EDT Live in 1 day From Terrestrial to Celestial: Unlocking the Potential to Enhance U.S.-Latin American B2B Collaboration March 18, 2024 • 2:00 – 5:00 pm EDT Live in 2 days USAID/MujerProspera: Advancing Gender Equality in Northern Central America March 19, 2024 • 9:00 – 10:30 am EDT Live in 2 days Section 702 of FISA: Privacy and Civil Liberties Reforms March 19, 2024 • 2:00 – 3:30 pm EDT All Events About Menu open About CSIS submenu Main menu Home About CSIS Leadership & Staff Financial Information Membership Groups Media Requests iDeas Lab/Multimedia Congressional Affairs Executive Education Careers & Culture Location & Contact a-d Abshire-Inamori Leadership Academy Aerospace Security Project Africa Program Americas Program Arleigh A. Burke Chair in Strategy Asia Maritime Transparency Initiative Asia Program Australia Chair Brzezinski Chair in Global Security and Geostrategy Brzezinski Institute on Geostrategy Chair in U.S.-India Policy Studies China Power Project Chinese Business and Economics Defending Democratic Institutions Defense-Industrial Initiatives Group Defense 360 Defense Budget Analysis Diversity and Leadership in International Affairs Project e-m Economics Program Emeritus Chair in Strategy Energy Security and Climate Change Program Europe, Russia, and Eurasia Program Freeman Chair in China Studies Futures Lab Geoeconomic Council of Advisers Global Food and Water Security Program Global Health Policy Center Hess Center for New Frontiers Human Rights Initiative Humanitarian Agenda Intelligence, National Security, and Technology Program International Security Program Japan Chair Kissinger Chair Korea Chair Langone Chair in American Leadership Middle East Program Missile Defense Project n-z Project on Fragility and Mobility Project on Nuclear Issues Project on Prosperity and Development Project on Trade and Technology Renewing American Innovation Project Scholl Chair in International Business Smart Women, Smart Power Southeast Asia Program Stephenson Ocean Security Project Strategic Technologies Program Transnational Threats Project Wadhwani Center for AI and Advanced Technologies All Regions Africa Americas Arctic Asia Australia, New Zealand & Pacific Europe Middle East Russia and Eurasia All Topics American Innovation Civic Education Climate Change Cybersecurity Defense Budget and Acquisition Defense and Security Economics Energy and Sustainability Food Security Gender and International Security Geopolitics Global Health Human Rights Humanitarian Assistance Intelligence International Development Maritime Issues and Oceans Missile Defense Nuclear Issues Space Technology Trade Transnational Threats Water Security Executive Ed Navigation Global Policy Courses Custom Programs Leadership Development University Programs Course Catalog About Contact Us Managing Existential Risk from AI without Undercutting Innovation Photo: Grispb/Adobe Stock Commentary by Michael Frank Published July 10, 2023 It is uncontroversial that the extinction of humanity is worth taking seriously. Perhaps that is why hundreds of (artificial intelligence) AI researchers and thought leaders signed on to the following statement: “Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.” The Statement on AI Risk and the collective gravitas of its signatories has demanded the attention of leaders around the world for regulating AI—in particular, generative AI systems like OpenAI’s ChatGPT. The most advanced AI regulatory effort is the European Union, whose parliament recently passed its version of the Artificial Intelligence Act (AI Act). The AI Act’s proponents have suggested that rather than extinction, discrimination is the greater threat. To that end, the AI Act is primarily an exercise in risk classification, through which European policymakers are judging applications of AI as high-, limited-, or minimal-risk, while also banning certain applications they deem unacceptable, such as cognitive behavioral manipulation; social scoring based on behavior, socioeconomic status or personal characteristics; and real-time biometric identification from law enforcement. The AI Act also includes regulatory oversight of “high-risk” applications like biometric identification in the private sector and management of critical infrastructure, while also providing oversight on relevant education and vocational training. It is a comprehensive package, which is also its main weakness: classifying risk through cross-sectoral legislation will do little to address existential risk or AI catastrophes while also limiting the ability to harness the benefits of AI, which have the potential to be equally astonishing. What is needed is an alternative regulatory approach that addresses the big risks without sacrificing those benefits. Given the rapidly changing state of the technology and the nascent but extremely promising AI opportunity, policymakers should embrace a regulatory structure that balances innovation and opportunity with risk. While the European Union does not neglect innovation entirely, the risk-focused approach of the AI Act is incomplete. By contrast, the U.S. Congress appears headed toward such a balance. On June 21, Senate majority leader Chuck Schumer gave a speech at CSIS in which he announced his SAFE Innovation Framework for AI. In introducing the framework, he stated that “innovation must be our North Star,” indicating that while new AI regulation is almost certainly coming, Schumer and his bipartisan group of senators are committed to preserving innovation. In announcing the SAFE Innovation Framework, he identified four goals (paraphrased below) that forthcoming AI legislation should achieve: Security: instilling guardrails to protect the U.S. against bad actors’ use of AI, while also preserving American economic security by preparing for, managing, and mitigating workforce disruption. Accountability: promoting ethical practices that protect children, vulnerable populations, and intellectual property owners. Democratic Foundations: programming algorithms that align with the values of human liberty, civil rights, and justice. Explainability: transcending the black box problem by developing systems that explain how AI systems make decisions and reach conclusions. Congress has an important role to play in addressing AI’s risks and empowering federal agencies to issue new rules and apply existing regulations where appropriate. Sending a message to the public—and to the world—that the U.S. government is focused on preventing AI catastrophes will inspire the confidence and trust necessary for further technological advancement. AI is evolving rapidly. Regulators need to develop a framework that addresses risks as they evolve, too, while also fostering potentially transformative benefits. The implication is not that policymakers embrace unregulated AI. Undoubtedly, there should be guardrails on AI. As Schumer and his colleagues pursue their four goals, they should design regulation with four principles in mind: (1) preventing the establishment of anticompetitive regulatory moats for established companies; (2) focusing on resolving obvious gaps in existing law in ways that assuage concerns about existential risk from AI; 3) ensuring society can reap the benefits of AI; and 4) advancing “quick wins” in sector-specific regulation. Preventing the establishment of anti-competitive regulatory moats for established companies. Regulatory solutions should not preclude the development of a competitive AI ecosystem with many players. DeepMind and OpenAI, two of the leading AI companies, are 12 and 7 years old, respectively. They have an edge over the competition today because of the quality of their work. If they retain that competitive position 20 years from now, it should be because of their superior ability to deliver safe and transformative AI, not because regulations have created entrenched monopolies. Entrepreneurship remains at the heart of innovation. Many of the most transformative AI companies in this new era may not yet exist. Today’s technology titans like Facebook, Google, and Netflix were founded decades after the predecessor of the modern internet, and years after the 1993 launch of the World Wide Web in the public domain. The Federal Trade Commission (FTC) could clarify guidance on what would constitute anti-competitive mergers and acquisitions of AI Companies. An overtly pro-competitive stance from the FTC would help to encourage broad innovation and economic growth. Focusing on resolving obvious gaps in existing law in ways that assuage concerns about existential risk from AI. It may seem counterintuitive, but starting with the biggest questions around existential risk is the right way to support the development of trustworthy AI. There are three reasons. First, the reality of existential risk has captivated policymakers worldwide and on both sides of the U.S. political aisle. Schumer’s working group includes Republican senators Todd Young and Mike Rounds along with Democratic Senator Martin Heinrich. After the May 16 hearing at the Senate Judiciary Subcommittee on Privacy, Technology, and the Law, Darrell M. West from the Brookings Institution noted, “More surprising were the oftentimes bipartisan calls for tougher regulation and greater disclosure of AI utilization. . . . most of the lawmakers agreed there need to be stronger guardrails that safeguard basic human values.” This is a unique political issue that can attract bipartisan support, but that support will erode if policymakers start with issues that lack similar consensus. Second, existential risk, even if small, deserves attention. For the same reason, NASA has spent $150 million per year in recent years to locate and track large asteroids that could threaten Earth. Nobody wants humanity to go the way of the dinosaurs. Third, this approach helps to prevent over-regulation, starting with addressing the most egregious harms and tailoring the approach where needed. The law is just one aspect of regulation. Norms, markets, and architecture also constrain behavior. As bottom-up complements to legislation and executive rulemaking, they can be equally effective at regulating the development and deployment of AI. There is a dearth of mature ideas for limiting extinction risk, but that is not a reason to neglect the issue. Majority Leader Schumer’s proposed AI Insight fora, a series of conversations between Congress and thought leaders in the administration, civil society, and the private sector, should focus on developing specific technical safeguards and management process safeguards to mitigate existential risk. For example, Microsoft’s decision to limit the number of inquiries in one conversation with Bing—a decision taken in response to evidence that Bing’s oddest outputs generally came toward the end of long conversations—could inform the development of a similar technical requirement for all companies. The National Institute of Standards and Technology’s AI Risk Management Framework, a nonbinding policy document that, among other best practices, recommends organizational structures to reinforce accountability. Congress could require AI companies above a certain size or risk threshold to appoint a Chief AI safety officer. Personnel involved in managing cutting-edge artificial general intelligence (AGI) research at major corporate or government labs could receive certifications for AI safety, similar to the AS9100 standard for safety at aerospace manufacturers or Biosafety Level (BSL) designations for research with infectious pathogens. Ensuring society can reap the benefits of AI. While policymakers should take existential risk seriously, they should not fail to also focus on ensuring that society can reap AI’s enormous potential benefits. It is a delicate balance. Obsession with risk at the expense of innovation is just as important to avoid as failing to regulate. It is easy to forget—amidst the cacophony of doomsayers—that AI can transform government, the economy, and society for the better. From education to healthcare to scientific discovery to advancing the interests of the free world, there is virtually no aspect of modern life that AI cannot improve in some way. AI is already helping to advance research that could yield the solution to nuclear fusion or identify new drugs to treat rare and deadly diseases. The cost of implementing the wrong regulation is sacrificing some of those benefits before we have had a chance to fully appreciate their potential. Advancing “quick wins” in sector-specific regulation. The choice is not between banning AI or doing nothing. There is a reasonable alternative that builds upon good work taking place in U.S. federal agencies. While Schumer and his colleagues consider Congress’ role in establishing new guardrails over the next several months, the administration can simultaneously move ahead to exercise existing regulatory authorities in the context of AI. Laws that protect against injury—accidental or intentional, from humans or technology—like the Children’s Online Privacy Protection Act, the Securities Exchange Act, and the Federal Aviation Act, do not evaporate just because an AI algorithm is involved. Federal agencies should develop sector-specific AI regulation and improve their guidance on the ways in which existing regulation applies. In fact, they should have already done so. In February 2019, President Trump issued Executive Order 13859, titled Maintaining American Leadership in Artificial Intelligence. The order directed federal agencies to develop AI regulatory plans. That work is incomplete. As of December 2022, only 5 of the 41 major agencies had fulfilled that order. Only one—the Department of Health and Human Services (HHS)—put forth a thorough plan, which Alex Engler at Brookings describes as “extensively documented [documenting] the agency’s authority over AI systems. . . . The thoroughness of the HHS’s regulatory plan shows how valuable this endeavor could be for federal agency planning and informing the public if other agencies were to follow in HHS’s footsteps.” Congress should compel this process with clear timelines and broad goals each agency should seek to address. If the problem is that federal agencies lack a pool of experts who have a clear understanding of how to regulate AI in their subject matter domain, Congress should take measures to strengthen the workforce and expand regulatory capacity as appropriate. For its part, there are some ”quick wins” that the executive branch could achieve on AI regulation. For example: The Federal Elections Commission could issue an advisory opinion to require disclosures of generative AI content in campaign advertising. The Consumer Product Safety Commission could compel LLMs of a certain size—based on user reach, financial backing, training data scale, model sophistication, or all of the above—to issue certain disclosures that would better inform consumers of the risks of interacting with an LLM and the model’s limitations. (The EU AI Act sensibly requires generative AI models to disclose that they are not human). The Department of Commerce could expand existing AI software export controls to explicitly ban entities from countries like China, Russia, Iran, and North Korea from accessing U.S.-based LLMs or open-source AI models and data based in the United States. Finally, policymakers should remember that they need not produce the kind of exhaustive, proactive risk remedies that constitute the EU AI Act, as the U.S. judiciary will have a role to play in resolving complex or marginal cases that result from regulatory contradictions or gaps. For example, it is reasonable to assume that if an algorithm hacks into a computer, a legal person—be it the algorithm developer, the natural person instructing the algorithm, or both—would be liable under the Computer Fraud and Abuse Act. Legislators do not have to decide now who specifically should bear liability. Instead, the judiciary can establish a legal precedent through discovery, trial, and appeal. Ultimately, policymakers have a responsibility to balance their dual roles as guarantors of trust through regulation and guardians of the innovation environment for responsible AI. While there can be tension between those roles, they are not impossible to align. The payoff is an AI ecosystem captures U.S. strengths in researching and capitalizing emergent technology without significant sacrifices on safety. Putting up obvious guardrails will help communicate to the public that the government is paying attention to risk while also avoiding regulatory capture or stifling innovation. The U.S. government should resist the false choice between “doing everything” and “doing nothing” and instead seek to define a world-leading framework that balances risks and rewards. Michael Frank is a senior fellow in the Wadhwani Center for AI and Advanced Technologies at the Center for Strategic and International Studies in Washington, D.C. Commentary is produced by the Center for Strategic and International Studies (CSIS), a private, tax-exempt institution focusing on international public policy issues. Its research is nonpartisan and nonproprietary. CSIS does not take specific policy positions. Accordingly, all views, positions, and conclusions expressed in this publication should be understood to be solely those of the author(s). © 2024 by the Center for Strategic and International Studies. All rights reserved. Tags North America Europe European Union Artificial Intelligence Technology and Technology and Innovation Image Michael Frank Former Senior Fellow, Wadhwani Center for AI and Advanced Technologies Programs & Projects Wadhwani Center for AI and Advanced Technologies Footer About CSIS Programs Topics Regions Events Analysis Careers & Culture Center for Strategic and International Studies 1616 Rhode Island Avenue, NW Washington, DC 20036 Tel: 202.887.0200 Fax: 202.775.3199 Media Inquiries H. Andrew Schwartz Chief Communications Officer 202.775.3242 aschwartz@csis.org Samuel Cestari Media Relations Coordinator, External Relations 202.775.7317 scestari@csis.org See Media Page for more interview, contact, and citation details. Sign up to receive The Evening, a daily brief on the news, events, and people shaping the world of international affairs. Subscribe to CSIS Newsletters Footer Social Follow Facebook X Linkedin YouTube Instagram Threads ©2024 Center for Strategic & International Studies. All Rights Reserved. Footer secondary navigation Accessibility Privacy Policy Reprint Permissions

Titel: Kein Titel

%PDF-1.4 %�쏢 %%Invocation: path/gs -P- -dSAFER -dCompatibilityLevel=1.4 -q -P- -dNOPAUSE -dBATCH -sDEVICE=pdfwrite -sstdout=? -sOutputFile=? -P- -dSAFER -dCompatibilityLevel=1.4 ? 8 0 obj <> stream x��ZK�� ��qoi���~?rIY��خ�mr�sX����JZ/eK�%��� �p�$g���A��   |� ���+;��������������������EZݽ�����#�������=�V�Lޥ��z~!�p�����1Oޗ��������ln�6o^�q �����������|Nk\�!53���cq�|Ex�?=��}��/��u�� ������b27|�K��i'��b^���r ���Ļ%��R'W�y�&�\��U���۵oS����K����_iiN�jew���]1�_�ْ�b/{�!��$8�������}���ٿ���̛w?�ubu"�7bK����>+�-�|�j=��W__�T�����b�u�ǵC� ���- �"� {� 8MEV��K��G��50׺�q���Z��NAGk ��L�+�>����;����g kE���kGV=-�Lt�iT[ ��RM���k�,�n������;0d�|�[D� �2����r�_����&���1��?����[k+�[�#�¬�fU��� ���Ar:-��e_�F4k'=���`��Ii��Ĝ�t��Q�Жozi��qkƣ^�� ���#J�E��3cu'뮒a m���9��G��c���z�C�y*�@��Q;�*F� }�;��J��H�����ϑ�� � �vl  �"Hg;�����4>�̴zaWO.ҼZ�B�"���>S/��7O :3�JIu83��4�s��D� �� 5 Q= 4#�)��(8�L �S��: d?� ���u��aǧ4��l���A�CW0����+�c�qt�$�"����nD���>�Q���`�N��'�:�g���[Q�y8]�|@n~��1�:�1{ ��\�cr �ҍ��s$5Aښ��o6�#rԹ)U�5jZ_+UsB����W�Z�,�'LզQ�}fKNvrNbcP�A�>�\����g��� �bQ��1|s��H 3�fP�F�C��p�p�Yehc�m�W*xL 鸩 � V�mY �eڦE�ފ�l��z����X�n�B5 ��j�4��N��7�;R����z�p�r|�� ���e7� �� stream x��\[o �q��_��9�8���/�^��k�� �����W�r�d?���K�f���e�g�/C�s�zY����PW =�7�� ����O8B[�h�;��+�����.��O � ���2��8}��#��Ll���Ϥ�c3|x�M�0;�ʦ�>��}��ٲE`����4;-W�;\%8x��t���,1�+W�C�bB>�;�e�M9 P����ZI�Uϖvß�/=/bG�a%eQL�j8M�V a�O���� �#_�V\��z��|7��#��t _ļ�Db Aމ2���xM4\�z@E��E8��d�8�  XR��C�cqk�z\��Jɸr�5)Ӎ��skyK#ƤG�U�!�섐t��&(��� ��M �!hp�{2TY+�w$Kܔ,��{�ϓ'fF�V+7���7�B��t+4$W��.�Tw�Lo3��:����\&_ ���5ɼSPNXd��(����3��P.�H����P�A▪ބ- ����/��������$�T��� ��i�� W��8�z�&^�Z�iV?c����5�P��K��@P,x� � 68i����;���v`���+Q��Vc0�7�(ӄ.��T����99�;j�4(=.k�'u�ɞZr  =k� 4��ri@s�=� B�AO|>�ʭ�x��bE�F��;��V�����cQP���F���1F�4N��HI%0�����[�Q  0�V�1Z > �Vf� ����V���Z�f`z@$�� 3L ��7 �6  ��\;�/�P��%�����e� |o�ZRe���𔹱96��� �/�l: #H� Kv׀�J�)_*�4�Aa�� ��՝A�5r*�aK��u��c�Gp�� �l���z��)4�c���2�%.yŤ�h�,� �-Cl������p�� ����`������4�.vv81 G���oo6B/��p@J� CP��I_g���`� �����u�*�@n��Xk�="+f��N:Ҙ�m5"���!jc�},�A�x�Dߦ�e�'O�!&]ߝ=��2�E�2�;�:�L�t,@2�>�P� �� ��`�q�I|��A�'�Jg���u]Y�uU�Z�=���V4����"~T\,Lr��f���#0%XĐlj.ָ Xw����o��������m�2�e���>��9�9��O��j\�7�.� �A��FUqNyx���i���YW� 5V8B�̀��XQx(m��s���Qө�ӂ��L{��ju�?VV��� ����m�~Aǧ�|)� W TuY������Q�Q�d� �U�QhP��W$��JU��(� X A�Kg ��gĚ� �  ��i���5j�W��=��}i-�B$�� �7F��� !�9��,��X={�,�xA�jTcڕ�MZE�d��\�}��o�� 2Bnu�#׃���n��KU`>` ���L�e�ᨠa[!�L�h��kkG���,�0$ �S'��)��7U�]'-GҊy9"UoõE��Gkq��Ƒ������_b3L�x7� ,K@+ �8��,JS:)��۪��g �2e��]�O�(��1=;N�ji '7��l#��0��gLB-ʄ� �We�Rv+����WpR#�A7�V��nA��Y#�V˞�� ֲXaӣB�% b����Ӡ�"����E(ΥИV����5�3n_�ف�=�j�N ���Y�����6Q�S�t����d>%�ߏr�� �Ќ� ��e��u�*U?�(�\Q�V�`ׄ��_*��١���������i"�ж � R%I��&۩3��aSZ~X Jl�(g* ���FR]�����`���"�9Ǉ���O�8�� �O�.Z��'���\ ;�|'�u_&Dw�� `�Sy넢z[����)=��8棖�'TZ���{%R񨺃� �P!�ʭ�Am>���5�nG�R�j���8"q>;�j�=o ^42�rW^Ħ j̹������2M�R.ٔ�eJ��"������;��� -ZI��ױ)��G �T~�FՊ�?���e��iS��/���_�d��Nc �W�����aVv�� 7�=�����)�pG����J���@磌Q#w�uӧw_�T�p���ZK�~sʘ�����h�r���{-�VI_g�=��yU�P����B�K� ~�AMP�.�H~���N\;��Y���]�� z�� .�T#?��1���Vv#p�X ��n���aٶ��� 9��/\^/� X�n%�\�І(!Vuﺭ0,s�$ᄰ��"��Z�'��d�Z�ן��є���&�� ��~�fظZk F��i�S���'���eʷ�h}L� ӓ����۵|����;��$,�   �����}�k���f݀6Rʪ� ��Ӧ� � ����i��>&����U 7����zv�l ��q�ϝ��E�@{�=�-y��]S�i��/��#e��TNQ_ �.J�a�!&d1� stream x��\[� 9����~� �Ӆ�G �,�� �0��^b���}`��7 ��KfJu*S%���no8b|��RJ�ۗ���j�� �Կ�}{���Z]�����\}��_��_߾��� ��ɜU��/^]�W��sH�:�0ke�_����dNj�.k�����W_������9g��� RIs�&N��0$���wq��OO70��N��Sə8�kc�I�W���u�~�1�������\�y������i ���@G{�b��~���z�/$�a>��{ ��s��!��b*{{K �>�����v�N�N~�����EgT\(띞�\v� � �R2nz�������o�|�q�Fغ5ӷ88: ��Wd�q^�2���5 ��;ݤ���������8`��~�EŠ� �_�LP�M��.KI)O���yF����J����(�� ~0opR B�g�ks@{7����Xώ��Kx�z����#�?yz���k�U6Z,�-�u9 |W�$��V �20f��R ��lmn���NbA�n��ϣ9�)@��Ц {�+y$i�����$d��^�����&����D7(�ZW� %p>����d�X����� �K�k�3�� � ��[:�݂'��BS�`_�ǝ��W�F&sg�驐��m��s�"�u��$d'�G2)���� ��2o~^���XP_NZ�� X�}�� : �6 V��)��2p�f����0E��@����Ջ �q� ��)qy9��8 ���p@ ^��^v#��w�� ��hP�����Ν�����N�j��:�a��{���qGv���lp�˞Q=8HXO�>r���� ��@3�N'z � [}J|3h+ � k�G0E�_�0�[AC����F�+D k�P�h��j"���P����u!*�+�������x 9 S�}$ �e*�63HG� 7(�I����� ����P'9oE=���̱��>��Hx�)�1c; i��p�T�{�����!�٣� �̓)��(�����$� �T�j�9ۢN����D�C͖��)�\6��4���`vPh-�@��-�^J^����Y�o=J=�V� �����в���xTRf �[�C2=�r� �$� �� M��i���&6�5FY�&L5F&�P�D��4 ����Dv ��O�� �A��$�)���0' ?E �3B�`���}'�N ���)�̄9�= �}�M��+���MM�"���dF8�e{Q�B��cQd�eؘ=h+=���6 �H���ޯg��*e�8C������]�d��&��w��*��A��� �.�m_^ ֒PY�, ��������G6lZ��Z�Ʈ��i׸�`z$�����C�h��EJG t�!�7����`�n���i�-��eB��!.���Rbrp@"���V�)59E +����cR� �-��A'���y#��8 d�| ֗��"�͘��/��,� ���r-�#��l �nR���V�.�/�)�p=KW�V؇"!V��~V����x���n�dl�R��h�h�\�y�h�X�3[��� j���8d-��m���3s`?�k9o'�����7�ް�ß B4j^j�b����CnB�Z�s�Ճ�d��Mg�ݞ��V䜕�O�qgA��$�3�cD�# �{J;IP�!�� F�0��>l��'ﻗ��K�k,�EIZ�J���Y������]Mi�#B�m�=�n����*�Y���;ǖ�����R?x ua-�٥7�K��r���i���K ���Mp��L���05�6������j����墍vm�[A�쌼Y�?�0�J:w���� }�� h�����I���� ��ZPD˵c{EJO�v @hI�'oa�� �Rx�*� Z�j�@��*f�\x �r.d��a8����� �3�� s����-e��#��1 x��S�O��}hX�[5�l0�������!����z�'Ch8C�H� u�>d���*��!a�Ɂ�ɠ�/X�K�Y�}HM�_Q��F�p���b�5���Ź�>:���Q ��l�˷�O� �'0�$W��J 4I���H5��{��`k�7�T�o�~��p��+ �}l�;h.#Qn� � }�.� �b,�kUbr@zM {�9&���} � � ���L�� � �hP����� j�n�,U��ϫ��j�ڬo����~:%���&M�@*;�d|\�J�ܒ=�:0 �� �HQ���f&�d殶>�{C um��1�0& 7��'� K�|��Ҽ����' �>�֪t�u�z]K���$V�ݚY��(5-�S$��L�M͕y����,?�7s\�Cx�X*�zU*N�8�u Ў@� k�V¯��TT��}�9q�b>yl�)���|:`#|n���]3j����h;�[ �E�N���.��o����)#����]��څ���O � 4kϾV��vв#�ʝ 3�c?�g�n5��H%RT�6��G�D�h�y#��ѝ �� :�Z Yo:��-�]�t�?��s�S �{@)�UUH�q[x�@�#+�v�$��߀�_R�Ӹ��l��ծ0����_�n�d�0�"�7y}�Ť?#�I���6�\G"� �C�.�:Ү^��3�0q��$�O ��q~�6h2 !;4��jRg�;6G��g3�D6n?��t;�B�l��]o )ua ɢ4�0�.=��zy�c�F)B)�j5� ;�X{ �;>lW����� K\`�endstream endobj 40 0 obj 5029 endobj 72 0 obj <> stream x��\I� �qv���û8�'�U�r�� �&eR�x��}��$� � F���!G��:3��;��j0Լ�� �Uג�˗[��N��N���o�>���Zݽ�� ڜ�2���ͳ�=�4����ۻ�����.�9��^�~V��wF�9{wM�5L�������psF��4A���>O��ý1vN�LP��)���*?�� �69L��nN6�E~s���z��2"Z;� G���ӿ�h�&ũn v�|z������a[��5@ 灧N�!��� �>:����ϟ���_��R\�9��9�a�9� )na��[�#�|����ݽ�s6xc�`Яq}��NO��0�5�+�{6��v�O � �E|J�\:'����z��-���Y��s=�j�� � �h���� �5��ى&j��M�� VG��&����������t^�:7kUB^� n6&&K��o���C���>,� �� �w *� O�hHć��!Ԇ�[�R�$9m����=�0��f���Q���@�PV� xǮ�����Wqw7��2v�Fb�������>�r��&w鼟]�rM�J�����IoO��������7�=,ǭ"����99��o����h�=ܮ�����4��ʨ0����8(0?���� �-���M�0 Tx���Tyݕ� �>��db�G4Ƈh�c�R�k�:W�� $�������� ��爀 a�������2������~b�s�~B�����f�x9��b�G�! ?�Gi� ��f������ l��%� "�.K� ��x�/ƦᏙ��փq֩n>9Ψ��4�NqD�)N�ca�A���l�az�o:�]e�`��� �.��D��#�j9�u�ŷ��q���̟� rǅ\�3H�����:�/�JBp'>��a��2�i�TD���� ����km%��̏pH���yo�)�F �f �` �� ��|͎�6�N�{$�������+N_n�(��d����#��~I�[��F/�f�mgO퓸���I��=�mx�e�d'F.�|�.�9�/xq �g�3JhϥN��l���"��n3Qx8u�4���P�LE35>*8��o8g��4�j�CP|��  �B�9 `�Dc���IQ3���!7��>� @͂���zj5�D+��6����D#Ͳs-ʠ9|':K��b�w�)��#�-p��-"O�Q �L"�d��b Χ�r�Ȓ�k���TpV�WS��0ܹ�u]Z��J5�4��(�m�䖷 ��RA �2 ��e?1�ߔ D/���`c���V8>��^Q?Ʒȵ�ȴ؏)��4��`�7���:�hw^@��㎃�B���P>P�妎��/S�8b��=�c{�sN�� �xy���#��c�~�,�7�']�ݔߗw����" �م�M".�X5���D!q����妼�߯�{!I�+��+��h����j�m� ����z(���b��-�5���~U�em�8x���21 �����R����G�ˆsnm���� b� D��8�"���!? n�Ȟ�.4^�J�l��M�s�~[ xm��"�*�VF��^�������D� Kԍ��v��I��9�a� Z���)z�zO���gK{Q_W9�XZk���MJР �\  � G��]����zZ,��f z�s�{��\ $\���ǝ-�-dfv� �-3� \�-�������t݋$B�� x��kXȥY��[��P �N:`A4�N�!`G�.��n"����w y� k�=�V�ڌP p'9Ä��S�5����"H���K0�3&��R��Lg � (�vlT��=�We>d��.C���ԵO��&��O�q�"�]�0����҅I�rIFfQ� Nu�q�v9��#�q��1|�5 �SnX�ڜ >MY��4T�|� ���b���.J�=f�R �0��A|�����^��0S��*M ��nW�Ik&A.v��zW%�J�(*d@�kFο0�M�z��]�p@D�)v��� $h��5��R��e�3�{B�>I�S;��mJߎ�7!ﯨr�X�� O�7"G�9*s ��08��0�[�5N 6#v�\�9�8�n���X�� ��.�9����nb���8} �:�ʑ.B��~Y��eV�Gk>t����:�� . �d�� B�?�A+:ӈ2��$[/W��OJ���>�����;��'���`��׍�� 4������i�`SYuӋ��o;�f5��Ik�  ׾+mX[���Nö���³LN9�R�� �NX+��ؑ77�7 qy�/�$)0� )2'Jt]�/�x�\ ��[�s]��u�_=�_���Pendstream endobj 73 0 obj 5706 endobj 110 0 obj <> stream x��\[�$�q����"Y��W���A�����鿟�����Z ^����n�t��_�>{u��-��x2&�����My�� �C4a�� n_��npG5Z����}���oo~s��ٔ�����YhL!X���ƠB=˯0�s��8�� O� ������)r&�ƿ�1���>�6�i �O�'c�H� �7�K> ��x�uy~�;��޸����SQ�V�^ �@Ol�Y,�t��E*���c��& ���*��Ûcf��a�����f������a���іe���}` ��V�iH\�Hlp^O�'4E*�$q� �F�I�� KNq�EhL�'GmF �~�>U�� V #�o�&����H.K.�l� əID� �lȓ8�v���ѥ�� ��vC���y�����ҕK?_.��EY�ڲ:�IlǞ�;~x .�k1"����V���c��g>����26+��f�) ���wm��]�ڡ�����-Q�c�lմ�՚) [���#�Mb!O��9t۸��]�؈��X�G�,d�Y������ط�#��C���o�{�����:� ��՞`��[:O�� ��JN���UM �q�O��Z��Xgb�D�Np��\���R���.{�BH+Y4�]� #�H*����ς���Ab��ܨ�7�E�t�  �n��ъiy���'��N#/����@3������v9���c$-� �����/=p�G�� �Ϙ2�}꺸��^ q���ƤO��8I�_��%���uvQ��� )�~��������B��u�Y8-1B:�7L�Dz+�:OC�kA ~�� =�$���)��q�&��;s��#eXA���b 6���\��s��b�wez � / ���J 1�(tjAA�e�dH��ί��NV�"M�?�\{R��m0�\�Ǜ���Ә1�uHF��xж쾦���]��R�l�v�� ��|o?6�������g#3�We�e|Q j [fm1H��]7�t Q�� u�d PX����K1�D���ƶ�JWx�6��J�3��x� Xp�"a�b~��%�12타9 �cܨ�!�$�Q�θ��&�~�WY ����E�2Cynkh��Nź=�ϖ#m�6)�8^m��jh6S���ɐ@�� Q��ң��e�?�వӡ�t�qYw� �?����h��i�87dD����\����L ��v�l��S 뛌�'� ?�%�/�o�9�͒���ME)�}�Y]%Bԓ3�)�0b�Cp��9e��~ )���tV-��� �D���x��|�c �h\O�iT:�'H����[��E Y|��g���jC�5��ޜ����(l��*��}�ɾxs k���)�V )�L!��W����J+�T�o�$u ��֝/L�IZ~&P{�-K���F"�4���',G�-Ղ)nՖ��������HoX�Pu(ou�Vm��M���=ly�t�ˑ��B���|b� C�Q��?���z�����&:�Ro@�YÉj�`i����c�-���nu�N�5��+�� �� �=���F)��p#�o����R|�H?\�. ��V�������u�F��u���*91�~���V+��JU{�/�H�N����^��i��A�|+ ���Z��Mq}]�E��� ?�O����� ���Ƀ���B�s�Pu-1|�mIª���wW�څO��ЭV�W�����#ҏ�iL�s��ھW�K��q��� u1�� ��� [��_p Z/r�$�E���W +�ԫQ��R�S��WT��$hi�Nb��ZZs�s8o��-'�4�_���t@.�=>Y,ky��,��Ј$�L$���؆ǻѓ�w����bF��ߒ �"n�8Uo����t�I�_i�W�(\[ "�+� E4���c%>m���?�A����u��ra~5��s��h)]��N[p���wrx�eA��pM�STU[���x^-���,�4!񆒮/ �`�_%�� �V?�N��Ww��J���GЖC�h�~Ǜ��Ƕ����{ҥb�w%�vA��"�u���o�i>+����G#B��  �M��*��B��r+�!�@n� ���e ����� a�ێYDcݿ�؎�9͟b �!��{��[ܾ���\�� ���{�r�|yk�Bm��~��k)HY8˻��,����a��=�|J=3nU��o�Sڥ��S۩O�U�0V�_�� F��atq7 Ɣ�u���M��z�z}��o�UmLK�6\ߖ���u�E� P���1�2��o^x �|�$�n��7P��� Cb{�7?�ȣ�Wr������0�endstream endobj 111 0 obj 5442 endobj 120 0 obj <> stream x��\Y�$GF�6HlOuVfdd _ ٟ�T�w�� �����j����� ڜ�2x�䣫ϯ4�� �}�x��5�ֻ��l�]xU��;�C���Ez�)� _��{m���*��o�g��]���`��3�[{��B����QH*t��[G����Kaw��φN�!}�A:�;�}�;U���}F�畓 o�� ��{���:#鱱w6'�ē3��RQ�2Z+HE������3�;ɍ�O��W�;ݝV7�R�� �����Ƈ��%H���7�b���S�� U9���Һ�Z�m�Ԓ" �����UKR�a� K�ʑ��sJa\��e|�@os� � �},��4�c:W��x ��MA���;�'~ ������. &��w���s% ���p���;}�06BaŘ���}� ϺH�H����Il�j4������} �(����0erk���)Or�^G�C"�_ ��U����ޫ��%b:�ġz�=�*� �����IJ���J�t@$�y�&~� ��� ���֕5&�.���cw��$�0=yt����c�k�λ� ���׉� � V���9�!Y6����0�~�{i �2�pzOʐ4��� ����`�Fѫ�`_ ��L�a���Y� �T� �G �+/� ��Ec�j а� jQ����&Ȼؘ �Bs�j�b�2& I���� W��D��� � ] �a�R���cX�Kx>|�K���r�>��+�6��w"��9>�v�����s)�+�����m�8+bǉMw�'لh TW��@. g�n�(G�I�{����Y_�>�'w3���B_���t��#8s �I ����F��=� ! $_N�7����ջO�}tl\�a�����y�q���;=�O��F _ �0� � ѫ�aU���!�a&XƏeI �P� �ͅ h:c%hpPF."�J��q��Z?zJR �~��@�k�]�(K�ÁPX���`����� ��Q1�Vfq0��2�`��\p�?����B'�O� ��ǅ:8� ͖z{� X .I�v�T��,��������f�} �{g��B����4��n\���� ��M$x���ŉUkO��_ݲvʩ�{(J��=0ƇZb�ݐ���� a��gm�(�rֲ��#��ε�*,W\�3jp�օI%F�� ��i���n� �ۂ�Y�9���� ~4�Z�y6 ������a*������T�(�*�"��ރ'�/�8cA�� �'q4 Ck�.�&X��u2h���Qh��p���vЇ�kmF}�h��nT��j>yg��L� �a��%kN�#�2�G�;��d�3Fxq��t�'�k ��qN�h�Iօ ��P�cݙԀ�񔖶��x�@�` V ��� �h�눶:�i��V�. ���F�ڤ)�pAS�2F���)��1����$����. �~��:X 2B��Z0�Q�w���s��� �Y�1丶Kk�P�*2�6� �㙰9���Z0�{������1��*�B[275 ���΀j��A��s���@q1| �l`���X�C/��$�jۣ1��ƷV�G�Е��T�_�T��V6�e ��ӊ6��f�4�{�����U�'U�F�sz���~�x?ߨ�mp��J_�ԩ�����6��������1��������v�����S�W�v�H��8P6��rZi���ڰr8{���nU�Ws6�̵��!鬧�mE�hV0��p"��t'Uo�� ��֦b~�(` �����֥�� ~$��*�{61&؅����B �X `�m܈ O�h,M��@C�weeb󐿐(���]+ �%O� E�XB֢�6��@,�?-Z(� ��ʶ.�D� piۆ|Z�8k�D�Չ��\:,�ћ��&M��/J�*��d�6��Na��Piɣ����|��ƶ�m��i�:�:-ʲ@X0��dD�T��3�M*�zI1 ��8���8b���>*-��u����U#(,�:�1�M� V����Űz��j@� �yk�:�������Ro��0�˄�]�B*ѽ/$��-�{8+ 3�� Um�V7P% B�䔜��o��x�^mwT���*�Ì�@\�{�J��0�3|xd,O��RL����3��������̪�6~�$ï� �&�P��* :S�1$�0Bv i,Y Yrf|������� "äTzv��9m�/0.��e�oBҁ%��ȴ��e���%;�'�8/����h�f,���4�y�/� �-Ux���Yϧ���!_t�tm(W�l��6QҤ��j�l��Z(u�>���i���za� �w!�k*Y]Q;>*���|X�K�|i�4����Ȣ�O�� o����X�,�U��]n�!(�@�rar����]9��]���Q��= �,X�SDN�4�[�>�!��YU�N�H�l��)�o=�js�  Dl�_VQ �(��}�K}�_����~L�V�zB%����,ϭ)������ ��� �@#��G��yу��@ɪ \/ΐ3��1���ÝQ���s�)�C�����7Ũ��  (ѥ�d.)Ez�6��rq��J�4�%�D�6=�&#��Hy�k����$3�f �跑�Û�/�m����A� �R��ç���&8�&��9+�&�h�L�&�3HF�A2-Ljo(C@\,)���%2�QY��A� ���jn�k�� x�~3�%�g��%��g��ԅK��=Jk^,8���_>�]�e`0#�o� NFA�0�/����Y�'�@x� �Z�=�j=�wUk�q쫡��X ���@>��XZ�i�q�NtY� oa�U1u�� 41�����,1��r#���.M�}�������0��������k��])��>��� �J�ڻd�V�y�=�ϼ ���1 ��fj���]:��7����g*Nӻ��R_�+��k�qѥ��� ��� �R�2��Z�����ϳ}__u ���?1ʇ�endstream endobj 121 0 obj 5094 endobj 135 0 obj <> stream x���ێf�q jXw� ��/�M)� X�dS�,j� ���lQ��Ij8M��~� ��E��U]= �6����)2222"2� �9>��?|z�G?���_������w��>��� �5���y�a����NE �s��S{�!=����GO�}x�e����x�g_���wh.�9����;[ϳ��S�O����9�z�'5^0Ye�r'ҷ�Q�1MTc�fhOϏB,� �I ���K��?��b| _��F��o��s1��O�Zf��z�~L�ϱ b��R��,D�#C2V�3��`��џX�� �||g]��{>JhVg{�i��lSAR}4��#�ǯߥ�_X���υ}��|=L������a,#?�A̬���� � 3cA�b�h�JL�ϣ?jaa��d���s���d��Q�7�S��d��0��3�qQD�K��c�;�a�T�)��4�c$��gkC����~��!^&��\ SmEW~S��p)S�-ô�l���\�0����_�i�-�u��Vpm)�}��E2�1b7֜J�h*�dVbZQ�)[Pl+���0& ��!��*�8cmL|3���(m ?j��֓�n�D�>6#6��� 3I��m �S����^l�8[��,��N�RQ����c(6�F�nC�BLpf�i�{;�0�ꬶ��\�Ɉ� �����J���}F�03?7b�}�i1� x��S��1L��I&���u`a���G�e�R����06\���� L2n"�dI!���S�u ^7 a[���a�{u�4���k�-�0�N�a3R�)�V`��1FaLte �:�j�f X$�4V���f&����_`lR'1]�.M[)��a��ٕi�N �K&�l$ƺ2�1! N �����d���Mx�N�Xm����>���o�{��4\g� ��J��V&&S�f�sr4�� ��s�Ā���H�`LPE~�AF�آ&�����)s��K|�a��۔��Ǆ�B������� � 47� ��Pd�1]0�l3T��\ 9cY���;% �1��1���>&D&���3`�J&It�ҠE �%4s�����a��d��TM$�a��:W�:� ӱ$���m�o��L`�����`*ô ��}T��l�ӧQ�1���0ƊD���s۩)Ա�t} D`�3��ܣ���3��R�����iG��>��Yq7)�Y��Z����L��xX����`�Vb"���)����N�09����Л0\i��'� �����y����4m�y���#>6� ��0]B�#w E0Ib �J ���!RRvqh��!N�a3�u�0Q��D�BA��1S$ԁ��4���}+t�,� ��rI�2�غ-H"�F�cC� Jq�EKjI�nҀ;���(V�δ%��J�p�N�l�غ0�b� mf@�S�M3TO bu�iD�U� �o��YIK�z��s��RLJ�{��IR3 C-��1 [M�X&C� �w ���Z�g��x�jB�fa*��6t�.ð�6��s,b�����Erui6��s�f�=W\��� m��*��W��у �)"�u����oMS��M�� ]Z*��M[O8� ��?�o����S��Q+b�Z��P�� �I[�h��/ȇ������]�Jgt,v�A�_ڂ�Q,� �g� K8��3� �R�97���ƒ '���m����w �ݩ�Ԁ���o!�P���U*�S�Cv��0!k�/$a �"n��� ɽ?Ԑ���60�:h��9�_[*�E� ��)�Us�&"�z�0��l顄��x�h� �� ���16�RT�$s%6YQ����w�ֲM����e'Nm ��q�2��G kͨnk �����+���û�?9,C� �ن26"b�S� ���4,��6o�N.�'�{���>�G� �@ n��c �4~���U�%�1��j(�D��B���N`F�muS>�a�%�x��L!�Eh�V���q5��H9�\P� ð���b�2����e �ù�(T�a4L.��e�aÛP�p��*�:-�����Kt0�a4 �#�m]Fp�@�C\c ۢ��� 0�)M"���Л�Dݿ@����A;�΁ `�h�+�4m8 :1U���LJ�7,��Y-/���c���~:��#����O�3k�U썓��%] �#Wz�.� �Y(��:~�����O'\�\L�t��&��&�*�#Ӑh0�4��v� �����ų��4/N�4=Қs9�^6�0���,�0���锛�}�B㟐W�dh��Q�c ;Y�5(�M�.Et ML m���I���ϧ�ƍ��E ��4;� �y�Z�$�-��Zh��G��J�f |�� � ���M �g�{�Y�/��(�$�����)Lٜ���D3����`�|ƭS�g�S 3KV ��8t������ħ�����i�1�qh�i.���# c;� �����?��=���5b����%y$����Z���� ����n`�iߛ��?0��5n ˏ�N�`{"��{�A�I�@0�� `��r Ck.�� ��� p��{�Ȕ�Z�i�w�u��80ҙ��~��h�����=Iɢ "[0i9�Y �U 0c8&�A�&�I��pb'�T�e�' ���/D��I �j����\ �dƅu4cv�7� J� "K�8Q�� K�2��2*�I�X~1� I�/a(:]V 2dm�B���I�L���M�3V��q�� 4岆B�bF�d#\����*�� qX��@1&�� sܴ�\_�����`kؗo}� �Q�u�>!�%^BY�cr���HG��@B�B]��A�ahG���|��w�)�i¾ U��U��2�՗�`�(�����*%�j�U �h#"c�V� ���˔e��I]_p���6�/ ����ŵ| ���r��D�@-� &�td�5��єK�,o����"�U��Q �R���w���ei�b[�V�� -���a.ǵ� ۹�,����E� �2IR*��o��71�F {1�{� �SY-R� ��eJٛwq��!�A%-��D��yb��>ĵ0���H��'z\�  �` � v���__��UC��f�A!�����������N�z��@�Wk��"��$y��&�\�G'L �0�$�F�#����}�.JpzV�C�3������7��k��ZNrLA�K�ZΤu�B7���ɳ.� X ���4�&�:褮�V��v�n) �2�`+��]��o�L � Ɉk���^�!��;�� �z'ä� �~�M3�������X���@-Ze� ����G�u� �M���1N�" ^��Qe�B�U�TD @�'�TV(�xOC�k���p]�LEQ0K4�R �" ���xЅ\ф� kU*MF��c��G���r��8/L|��A,� $ms8� Q��#�C7�I��A 3y�7L�\ES(A�w���e�#?r6��M�BRi��Q ����+~t���-��氅�>��ƚ4^qԐ�p���&��?o�OS ��� .�����9,�g����ϳ�G ��pr�9�,eI^����v����>��sվ� w.��ӊ2l���n ƞq���N R6^N �朗��a��4E1��� 3��߂PE 5�GA���_�_��]�k �l�l�h�Џ/bw���y�h"�ڽ;�+��0��������I��ۧ�������'�8��Ҋ���~d+~}"��ьc�' G�u� �^����٫���� F� ��r���t�:����H&t2��۰Du�3D�����;C���̑������� O�9Y(��&w��y�)�#��E�bkꌑ�:�`D���j���C��D�yYR��m�l�ЕQ�!��{� �$�C���8Q��"=���QrޭF�Ɂ�6��\���u��.�/L�ٷ���D2��i�i�j�6G�$ �a.u�3I.�){Ȟ �,��{�� �"z�] �"l���yw�(o��c3"K�wS�?a^��� 4���0KT a^���,��m�5��:��S��h��N�X3�Ub`�)!;�1� �`�FI7)�I�̛I�d�dd@��(3ua*�E~a�]A+����/ ���Z���Q���9]շ%Lp��'N�\��� Iԯ l2��k�� ��I�{�]:�6�:]�e�R�md�g*������ԩ&CƩ��� i0Mń���Y�t�Z����pFw�����C[u����PV½ ��6��fy S�8:��T˚��*�c,�HTu�y��B� .���:�J}5TO5[}Q��J�a�*c��Ճ9/.1�+~]"u,� �uh���)=�D`(~��u��n���� t�̱�8������ � �� �����\LxHb# $Sh��� [��:L��o�b$"�( :���L q�Ņ߉�l&HK��;��I7�A�J�-Z��q Sy�Q ��H��7 �n$�"���� s�X�&35�\��w�L&�Bx� u�����֒ݏFO�˻��&�� H�K`�-�&��E؇=$B]��륞�L���i�H��R6?]h^�]���oL��J|��ျ|����i��u�"'1;�5���� ��U��E�)`����-���/=�ƕJ#$J� �1/'��r�|�)@ 㰚]�d���0o�u��ۤ��EoU8��t�����A\{�h��s��?���R+bur>�')R� ���ʔ"E�j�C,��WLՑ�8x't�U=oѹ�������B��'��݀�#��鎁P�IϬ��BJꥲ= C��[C��a�L�� ^6� C��5��fl3r��jOg�>���I��� &��4�E�U�"���"��Ğ0�z=���AaV .[�.�)�w;�£ �(*���q�� ��L�%42��JO�>��ւ�zX � ��j%��/sQ��ٺ��N�Spe\p����"��J��$K]7�_��N@6 Ja���o��LpL��j=t� &��O]���U�L%Z��L4-jj�s�@K�ڣ�S�������X���ʧ�� F��\-���l��_��{�|�5Iq[r�&%�+���)�T��0�IF�,���E���a�"^ F w ]_���Qt�y� �ϥ����N"[�' �хa*]������k�C�A�ƥ� }r�&�J��:���z}��A�qS8ڠ�{u"-�$� \_��+T��P��� B�A����s�����XBY;�?A M��- �wsJV�\�F��Ud�M\4�C[C�kj� f^�˼|���7�u]R�xn�*��[�jl�� ����1ax�ݩ��� �Z���.� �4)]��S�Λ�ߗƳ� �X�A�1 �Q�nJv��=�V���̙^x!գ̜jd�?C���] �(C/s���)�j3�R��}��,z���94�US^��v� BV� �\���3`�A������Ѭ$��� 8ߨ��ʃ���O ]�^��DYw'�l���`=���Pݾ�F N�0H� ^��1M��;��\�J�WA4� ���5�8([�k9$Q}��s��(n6���kZ*�Ύ�T��O�S9V ������YR$���B��#�����NQUW�/|�STm~bܧ�ڢ��t��m�R:E֙i����eqKwڦW �)��)���8����l�!���~���87+�!i�| �6m��T�jp�^m,��^��1�� ��x�eH?�|:1CC�.�P�Jn��Nq3 #��4�6�3�P�n��iA2o�]s�W4��5�P��ܝ���}"��KS�B�2��Jۆ�ʜwat9 u���n�o�� �We��c%up�= �p�X�/>Y��+�}��ǡ�t�n���� �DU��9�3���W�O8� ��l��| ��� �ܰs�+=�F��-)�)O�� ��2��wP� �u�A�R��F���9m��w]t}Q�>��#���{J��L6��U�o C��b���V^E,�M��̫����q4�S��6�����!rT����2-��Tf���ef��df/;�_܁Xe�] O��S�[j�V�߁��r�Q� �^N8�� �e�7͟��Ҷ�5��/L�Q����'TF���4����8�� ֽ�G������Q�j�v+����pc�0�Z�r���07���^ Q� �|f��{�w�^1��Pw~����穢-�/`���{ ��.Q��1�j���� M�g�`��I�1/r�Q�)P����F�Rp IGYT� w��ڳS��⊘��'[sw{+Δ=�8�x����� p�a �Á.+m ����T&�!��V����xD���Nw����� �@ ~�A�CN�,�b��ڥB�٦>t���4�$?\�0MH0�[V����5��4�!vFҵh�z�!�� s)e% �3 ����G��w��v�K6��x-+$?�n�B����o��F�Gv꣢Ԡ �+�/w/>�cҦڗN��ݮ+X��Va��:47vz¬���S�3� �u� hi�^.jި�qac_�Չɏ]܉���T�=p�� :������\񅄍k�*:�O��z-I�-��-y�����5=J�p�+��ČtM`���j��Z� ����F��1����� sΟ���S�����E�iO����5� �Q��"����������~�!����\��Fm�:$L^��tb�|%��=� L�����x� ���A�J�e��=���^i ��c�� b3+�[����vw�8 nd�u����7o�[��d��QlU7@s�AJ� �r ��X���TQ�f��#� $���?a�r��7�0n��7�]�\� �׆�� b�{r��v��g��� 2D�am9�� �,RF��Q� �A��C}a � *a9����P����(�Br�[e��O�č00��*7;�4a(�8����װ�F�p�{]̽�a?0�0�K����ٻ��*�� �c�:Y��`��� X""n7����*j���W`�nr�{��I�����0>�]�f5�)����}tگA���>[�L�{���>���mWuue��n�N]���B�/�� ��'�����J~;�ʼ.�^���T� c��@��Ӿ���3���`��FaP�'Ar aC��� �u��)��h� J�q���KĴZ��V/+h���w: F����������v%�ݢ��}�Y�5���7x��.! �Eb\����'s��Y��g 昒=hs�k�H������?Eptǚ&(� �|�拷����Jc�� Y���CL?��I.��+xk3�F��ǥ}��z�� ��� $3�XN0^_n)9Ꭾ�@�b ��IP�Ǯ� 9�J��� F��k:./X�OՃM�PN�zEbp*�� �]s��ѓl���_ \ �֢# ���]��Qt� V�ts�{�~�+��k�/x�� -��o���r���� V���s/���!���u0p'X�#�y~Ee0�>JAJ���M\ח� r�$�I��HQc�����ר��:3:)2�v�^ �$3��v (uz������qr�&oc$�� �j���z�"ո �dic��o�� ���� �VIk� .���ؕ r1�+��?$ �ҷ1K�����$9n�q�]�7Olqm��V��W�(X�f�������3vö���N��]0w���];ݱ� w�)W|{���|ㄒ�U���G��&t݌}Q�b�Y{*n h׍��\g!�ء���2 ˇ���H�����&E��Zɻ ܕ��]�@���sL��Ҕ� k��-�$W�6^� E��]����A���� �q�~ѻ)?A��p�m��;0�sx���_в� ֠Q�(�ů��ȉޭk~����1�-��F� ,������|[�ބ b�"��h0ES�y'�/ԁ��>���?�=�tɼ �$���uiȟ�$��u�%i�.���-����ɯ�N�7�©zm�V�^I�^ȧ�U y��tbt� ��%�㍡�'=���ʔQ)�f �l�җ̞=�"C�)��w�!��E!�\�%��=>'�����չ�( �k�����=WV�&��)��Nnh�z���\�(r�~���@���}�kO��]1bbҮ�*1n��n��~~�|���v�O��X�נ��*� �dC��q �;�i���0z&�� ��U � ���^^��6ƽ���HP���Tx�,@W�#^t����H�+c�_ �k����j/ @�#��_� �1l��n�C�}�] ȏ}�h����BSw��>a ��Y��q!E���E.��BA�A� �Z�H��>�Z:�SQv ~>��q�C�( Zo �G�(ߤ$�Q3��&�r����}0� UW  s�q^�y-���Dw���&5[>c6�`>B�����oN�6�i>����.�/L�tW���j�{f�� ]�]��t��6�vI! /�� �Y� g`�׫E �ѥ�N��� ^��0z�eW ��ծ.��WE�=���}��먾��`iu#��Z�r�Cv ߇�´�4C�� �~�v7��`�F�[�}�Eds�Ӝ,!��Vim$4c�7�\��0RmV i�c��l���W �z�ػ�v�=�ˎ��M�k�P[5�3W��џ��ˋ��t- $�?�=�d]��d1~b��ZQ��O�U=Ȗ�bg'�� ��n�����t���xM��\~��Yt�ž�1�9"|��&�kO>D&������ )�j��IL ���dK���Z=0��� ��� �!�"��b�=~H�F��ĭ�z����T�7 i�V~s}�p�hM�[�h�����= �����) �Q���ޤ�J�q`�r�6m��S��5����L^ |w o�x�K���� :2|�׽��N�a�=1��T�+�{�Q^��I�N��}{�ʘ&��� �����χO�?��� �0ڧH�^lQ|�wyз��b�6~��ӻ�?�����������W����� �JzU.� ��黧����������� ?}��{|�g_���w�� xc�YW�.D�z�#B� �𣧿|�����´���ӷ��v�1����� 9����+x�k�O�����q+� �.������@OJ8���"`h?z��}�����*�n�� 7x��f��$��6T[��q\_����2�RO� �Y[-���d%������1�#�����F����ð�c?�8ԣɄ��h)=�5a ���R m����3V����'[�w@�l�FQ�����S�lOȅ3���/O �C�ڐ��O8�5��� 쳳�f��=b��\��M�ޫ��&�x"�c��t�ھy��b�&�g�q\7��n �9ѣ��:˽��-DQIK��Ϣ/@���[���Fe�H��0i���ߩ�!�s��GOD��k���cd�_޺r����]M�3q"��7��罇z�W�a �6��nD���$K��sjg}�y�`R[��3��Zu5��N�jܞ�P'j���O����2��Y��8��ݫ���?��+xt�g\�������Դ�H��h2(9�u8y���%�Ĝ3�`N4��Èy��[Q�e�(o,�j�u_X����ؓ�>~�A���Y%#J? � 4ukK�M?�8ݤ� �!�.�e�/t|�2^��9S��IrG�v2�������� ���>_!/_f��?WŶ�s�x�n�m���Bfcv?e荤x5�x��ا|�>���j �=r��ߓ e.��mߐ0��bMP 'C}�Ĕ\� �n�*{W�¦��YNō27~�ť*\=���ܷ�W;�f.��8������/�k�9X�+v�}c�&{J��m�">���{� �j���I͛.�I�l �����($ޣ�߱p0$ ��$]�+���Y�+���x���cl�����o��� ]���m�.b�����T��Ι��2��%c�B����p٦��ͱ$�R�X.�f��s��ITܕ�;��c�n��W�.��ϴ�� $���d՘j��l�zS�pM,�s��o�\��?`c�a��)�֋N?~u�-�@��1��2 �ݎ� !p���5�b�vg�/��.f��Č�f�u �V��-c�U8~Mo]� j�0�������72HV+wӘT�BH_��\ӿ�J �!ys����C {�a��x��_��)�7}�A[�5� �M���-�JW���"��G������Vz��\�n���}��?W���TF�����-.+��1 �Gv|Ø6��=  �^��akڴ�L����l�D�nΟ��=� @�����Ud����iD!�����d:���o������nK� ��]o�l¿u �ܷ��_\P�ݚ����N����||�����@���� .�7xU�8�hd����q��[&"j�76�(%*6�M ����������䗕��"e�C�f_��6�o���d����i�:���7��%��Qu�:� g������� \��h����> stream x��\I�e�U6��l�~G�z�9��0��E�����V�\�zHn~=�9����y���nB U������?�Q �W���˛_~����� }�9ge�����o4��{Ǔ1vL� �;�������BRa����G,5|v��.�����ᣳ�n����ۛO@�]�k5f���8�C-�#Nk��9��'�ܨ\:�l �!�}�A�=j3:���'n�WnxI��ч4|�?o���fR����#Va[k��.k/??��_� �ox�R&��\�'�>O�s����T�E��s�Yy�� .rWq�����5��ބ,G�*4W>{"�/?u�� ��3Q� ��kT9%WG9��:�����~um7�U��Ik��!�� �4~^�د�r�!���N�#tvR�����k�6��2%��� Mɐ֬����ۿ�����/-Y6�W�寚_B �{��f���4���`��c9�7$IJE�&j)I�@6w;�v�h�$���Í�珐��ݘ`�!xk��K��W~����ٶ���#��,�0C���w\# (���C�)Mf���3��:�0W�O �*����2�E��] �b���NSf=�Yp �Н dbs�$ � ����1���0N`iC��d��ʇuqN�f�`�������8w| �C��� w�) �d�՞��[�1g�َ�`+ �No�y�&���'�)i�~À_��O�u/[��x�QȾ���� �Y�U09 ���L����\;e��6��O�� '�ʚ���CP��� �ct�]���r�JW,J^�]��Jd��#���)�;�p�԰�Z���y U�p�es����� N�p*,1��zk> ����i� k�0�.�Q�u� 4�W�@ Tz0�)���� YCڻ�� 9��B�؁� v�_��$�P��q$5bU��C_� �})2�l*�s�e�K^D�ףּF䝞�-�j*,ۭ� �m�*����(C=Ye�s»Z�"b �nŽlS��^j˷~[�-��F&���2�d��m9�|�S5�o��jp���Of4����7XԦ$�N9dt���`E��L��XW��z�Y�*i@����`D(gTD��k�h��Y>�׺,\�"| �M[\��lm��Fp�71�\],�%N���f/��Q�|΁ �P�*wm*8%g���)��rC�Ȳ�ʊ�\l5xJ��2uRd�*D�lά�A��������]Z�� �,OQL� �Z��FKTl������|�*�z4d���[��̺�RH��U��r(-ޟ���1 �c��)��8n��t�;�� 5qt� �8B�z���~�sFd�O����&U��͜�0 &���@@m�1�e��M���č�? !�)������nh��躝\�J  ,b� HΖ��v�ia�\��@o�1�����;��r��d Ah �6b��Κ5�6M��f� �fY �N�ޢÇtT,#]�GK�0K��p� H"2�58y]v�&���+N� `��q��W�^SHxV�p���#�"c�Y^��53ҭ~v�� p ��:��F�F ��%�[R]����4�*��"��f�;9�O3�EB�M�&���f�]/��P��h��2� ��# F����H3�ylh�[��UZ4E�We�K��y��� ����g�� �P���*CSV�`urGx�\ ��L|/H�' /ߔS�U9,�'�怕i��B0ř���:�N����1�N�!�W���3�d� ]�عs� �8y���*�?��-��:ĕ&��O?zĀ�����ԋp��@�FӅX+� q�3Nvb%��\��=C�S}�)=�����?��r G/���r��O �t{��t�wGz@���:����6W)b�-Cz^�lt�F��畲�=���T⚌�%�J����.Ey��(4���.�{�v�\7� ע�5���ֲ�lJ%�Lg�$�X �؀��%���(�¬�П� u�Z�� ����)w(v�a��� ��mh7�U����ʹ� 諳AV��t��LŰ�f?����qX ��#�m�9 ��ܟ?�R9�s��y��m�%[ �Ҷ]�T L a�۲P "7�]��,�t�V��r1}͋tGy��c -����sԱ��&h���M�����Z�J,m�5���ۭ Ka�S��$Y�@ϴBC�.�R�ҕ���>۶�(�[� g{EG�� {�c� Ǿ��X��t�ܝ:�������=�| �bnz�I^�gF�R �C����aR���IV7�럵@�]W̻T"�z ��Rٽ�������UТ�/x���q��3� �˩���f��#�S[N0�ӑ�z� `��d��+��L~�d� ߑ�زUw��Ӌ�@���q+&�xD+&^�b��"!2��(�-����!��ۉY���� ��/�hzEr9�u�,u�in���Js� :�t�0�?����|�6���X���`)�4?�B�Y�]� �FIW�(!�RWtL�R+�^J��d��:��Mau=+|�TO�(!��3n]ƬĢ�t 4��zM���L��j ���}H�����y�Q�`h�� �BH46,ͥ�x�}��v���v�� [+{w'j � ZF"sI�� �${� =��QȴAw��Q�ThQ�tǜ�b������yϲl )m [Pv�d~���{E) C�T�:��5���g�.���D�r�' T-6RKpv��.�8Y��7���( ��� Ϣ�Zrm���R��v%MP�^�a�/�}\&�v~���ϸ�@m�H�I�M��N��#?�SV}�q��_1 ���`.��zѮyYA�4�.R��y^v���][�9y��� s�G�]kU�� ʏ���4 ���o�g� ���q���Q����@��vᾪR: �[X! �z͝�9@�:|R�K�5��8u�8XⰆyY��^ZҩiY��-`}��R���^Q� ��������N6;��%�k&m�"�> stream x��\K�%�uVػ^h��Z �um����0��Q`ɂQh!��f��4=͠�߰#�w���YU'��nw`;���v��̓��Gַ����S����w>�����3����3�=��������h��/Ǭ�>��Yy?i} I�G�Z��G���8��:Z�������g >:��Mgs���oNΒ�9��,&�cP�����\�.N��χ ЏV��zt*9���Y�M�>�g��a��pa�;&㧏�M���q�y�u�5���1G����ː�-̴ҡ,�l !�#6Z}��5Ǥ�l9���e�d��/�sr�����SX���ɰ� �] �|a,�I&����p��pp��tu" X Y�B�[�T�v�$)��g��?�'��,��܁_�14�I�����! nw[�[Z5-�޵T3Z���=`��aK�*Vr4֝_��1B%%���Og�I-}��M�,��',����!� ����E��� ��Y���è Hx��������W#ŀXRx�`"Z=}O\�*䈡�a�J:�9�f��R9 ,'���ox��0J.�5\���i�u�z��˜ ��M7;ESS��[���y1 Ư$麷����� [��7S��5�c�g#J�ͬ����#�G#0f!8� a�L*����v��J�y@�}��;�r*��h� ���� ��ؿ�z�U����[&�G�D]�/� Hbxs �ue�����:���#. �e��� M�0j0�4@U}�׍�*�(��R�O��T�(���t#�l� �8�s�=�l��G�l� eC&�v�1��K� Q�OAV I�4��[����4��j�y� �s>25�EC�C�� % �q�' ��PU�� �T�ë�~��`h�p��lK�[��N���2\E�"ƛ�� ң��*�78J8��W�+��+%|nLF�ĎI@�b>`�) d�����☢h&K�dX��+o#��ѡ0ZP ����+���┦� �iKʮ�Z�V�@YQ�K7X��q���T���n7U+[P�fPN �|�!���>���K��t��k�i� �l���A�jfK�E��mD�0*oךQhBj)Ѝص�E��+S�? #�cHt�F�T��������=x8��WWu��DG]p�����w�2��4��jv'���&C��z�vm�W�:"$�y�i��5�oޑy(�z�9g������2��y���l�k��kkXw�Xo�������#r�!]�&BDe1� �uϋ�U�Q�S��ma�A�_ �EL��|�����Q�>u�z��^��p��'��,0?��/�u/�j� pTT���.�,B��My^ r�#��1frk���R���lS�}Oʛ��y�ɒ��vRi�9�oJΉ|�GG˨��'�*pC#" [ �'hil�d���Q���M��r������x L�|�� �r"��t������ �Dv��+���6 4�rY�e����9 ���đU+��W+��2�{%��e�a��E��Ǣ u��*�lHRf� *!a˹ �rR����ۋ�N��z�,>��eR����F��Z{Nc# Gj���˲V��\��� �����0h#P�A�6��`�'��>o��0P``��0�qۈ����Z��w~��JFw9p�?=���ַ�)�?�� �����jjy\ �n2 �wS��!k�V�M�pY� ��_�x+?�� ��vZ��YL����TqF���3����c>��)�Ƿ�"s�Q,A3d���>� -�:[��XJ�X�POi�Q��,.����/� �\�G8�� �M��@w5ђb�4������E���^�B9���������]YE� P��� �z1����{��WV��4�2�&��-.' q�&��D?S��$�K �&�l�^��O�`����e��D��Fx��n��:s�4��њ�;�� L"by`𸳼�~�D�����,�+��A��mf���L�/:�9�1 � ^��|*d�����`kl���R��m F���� Z��Z�t����GN����-9-��X�;��?'�[��.) M'��@q��-yP��=�Q�q�D15s�7] \U��*�$�Z���;����y��Q�ѫ�vp%+ ��%G0�{� ���Y��8���u��bvC�L�Dѹ.��r �dk\ݴz#4�Un ���� V�I�6�� r_ ��:~%��WŰ�J��xJT4Cqa�o���P9r�l�יzr�!�a`�#��{��D��S1�X���\��� 9M�½�e[h����)�m� n"���_���$����)���×�O`�瑄��c)��"��P�(/�]� 3�S�z��]�M�6w*Rϯ��E����l�+���]2ѷ�΀k���W �T����a�tq לsH����ޕ��� �l�b�e�g3Ѡ����b�ge�K���7�e4>Pӄ�R�^ "�� ?�ԃ�-r �y�| =�5.`\ڔ��H�b��� ��&��(�E�.����vu��3K�A�v� ��JC�) ���>��Lnu ��t��bpP����T"g��ޱ�;�I�Pej�ll7o˒LH ���y�d���]SM������d%ʌb!��f��"�&�>m�=Σ�|��� �#t��X N�;j% s)���K�� �q  � �����M�[H��_d���~��P��m銷�?��#�ZE_��-����ƭ?�L�[ 0>���חMښ������p+�>>�S��M�w���|� �� �C� E ��i�r"UF P1r�������R讗YB�q���ec��� 0٧�2��^Q��˅��>l�j�tDcD��\N��R�u��S*�B1�� M|h�jQ\�[����Ѣ������2nj� F����%E7��^�E�SW�~Y�PO���)�ٳ��u&.��>7�� }Y���eVT�T�q|0P�� 9�}�.�Ǹ�]��/,���t䡍��'��F�ࣲ�F�rVn���~�6���Rk\�~K�� �;�%��t�N;���r/�{m��\b�ߝ���Xendstream endobj 161 0 obj 6338 endobj 172 0 obj > stream x��\Y� �u�7>�7̋����U�� dň�qLp +#ΐ�5\43E��]�Nu����;wH*� ��>Uu���p�F}����g/ }�D������9+��7/ ��H��g��^���)��gi�!�t���#�όc��,�0j�x���_���������+�1AYW��ޚ�� � �憺^�h�o�YgT ^�ٌ���'�1w���x��@W��8\�{Z%yH ����y�9�ʒת��"Rt�� &ى�";���B�Q�tV�Zx|{ ��t���d��(V� �I��� �_�b^��/�O��yqs0�T�����e�Bh$���YcA���z��'W��rͶD��es� ��hV��}�}o�5���q�/l�k�g�@��/�+V���z·M6�p���[z �"FJ���Bd�y�6`�y��8�ĭ�Y���#�d`a&��}=# ;c׃ܳ�\�J����j)�3  �hs�3 l^|ƻ�HR�d+|�7+�v�G��4&���J�@l�KQL6�: ���ձvY^E�tY�#ݽ����][�PА��%��E���A�}]_�� ���%p �Q� ��I Hka��L�{?ޱ�E�V#!���������,O���,��{�nJ��R�¸K!��|�0`��^]��7G��ӂ.!BRｨd˖�2c�����)�.�;� �,�|j ߁�LC|��^Խ��5���,�?�-W�� #^������J,X1y!���B���`/xga��j�W#Egjp���4���Yh%�8]ǒg�dH��m���̀�'�[�=e�=��k�r� �G��]�����qu����Â/�a�����{�%-2i#��&�� ��2�L�81TUy��k����3��� �Y�Cނ��h��r�l��#D���s��̔He�d� #2ĭJ�*�*`4���}� ��oW �Ar ;�ΫbW��ȘˣL ��,[� z*��w2}.��b��L]❔68���"U�p.l[jh�! 6���\@��N��z��/�ҳ����)��#����9�����H�Cp�v/��D5�ܫg�� �tN@�1��ە�U���aO)����&f|ע��bT�[�R���)w��^1���1�� �B�]Mj]�MV��q����)@�A�*�B�C���U��œ>��I�D6����' n�N�7ߴ�،� @��.�ê�'ku��ЮD{�x��ei�xWg�XGi;w���ُGXI�*�.I����� Qh�j1P+%�.�~n��ﺦ��N��!��8�������+��vE�������"��N���zz(��lh� ��b]C&CZ�^q�׌>۷Sv�� �J�ۈ�]\��E}O�y���r ���a&���8�)ը��ۣ 1�͆K� ^����ލ~�2�d;ʚN�;C��F��o٥޾�*y!3{=� ��&�|��8�p���h��{f � MXr@�����% 3�bZ��)\�@� �i\�;Ͻ�ѭ�+�W�;��l��:w� ǋ� ���I���A���Qm���܈��&(����)��#�EXg��F��+��0�3��k,����#�-m���z[_���/�de{v�A��(��֜ kͨt�����k�꒷!F����*���JN+�� ����ml�I�C��9tV �W��2�'�pg^�  �Z�#]���?Ͼ�z��Û�� �e��3f-��Y�^ՙѓ���{���E�R7�Eh̍�5��b'u[o9�s�T�` �R��B֝�Q6��Dc�Da����3�ژjt�$]���S�ۅNy�T��:7��R�"p�mB�����:��bT���� ��Fh���v1�U��, �ny{�lY� ��Mfi+4ߚ���X$fP���c�(�K��2�0��0��f�E@����M��lj9i�-�^/�{_�o��[Y�*��ϲ�r�����r:���w n7|VsN�} |M�q�Th;�W��jS��"�id, �����3����3�m���%ؐ2 ���С-T�E�޺�F�X�{�r�+k'̑ӱ�� 43B?y��0-�OM��%���ފ S���iƥJ%��n\i��z�P���m��IJ�H��v�f(�� ˇ�p�u�k�m�ol���7��������&Zړ��ow����� r V��>cOu�� �AG�*�{;UaϬnr�¿�jF._0��2�U�{u��߀GI���~R�8���k�}8��*y���8t�H���:�o�����K@S� k�2������G��r�6�YC� �S��Ɠد6�T�,G'lzs��C�L� Uт��Lt�]Sx,�N�K)z �"�IeJN߫�i�6�&V��gȎ���ZyK��|�������7~�`c&L��4�{�gF��=��Gˑ����OfI&a�8�r|�� ��{^4�����'�:������ �� ��|����u�r������c�e|; ���c �����Q�a�� ������r�� ul ߅��u�V��Y�B �J�)$������Fgz�G '�A�� ���B�G-���T5���l��?���> stream x���ۮn;�&t��b^����g�;@͡�Q�BB�ZTU���B���� �Wf5j��{E����p������� ������o������O��o��~���%���������|�6�{=+}�� �O7+��>����;=��/��?����|��RN�?�g{r�_��w����������Zk�|������z/�/��ݟ�Y��+���:~����?�y�?�S~�{��>�������S�-���5��{M���o�����������|�j���`��w⎾����zR�u������g�TR���,k��vt7�y������z�7F8?{����U�������s���w֝�S����۔��[����(�ݤ�SZ�e���?�W���3E���G7���V�g���]ɘ߽���r5����O�{�َ�������|��;���OKu��'ƹ�U�GG�g�w�'��k{���(n�e��w���� O/���J�+�=0��}��2� ����>�� ���rG;��Χ���4t��F�_�}� �T��H{ڿ�������������x� ��>�J����;*l����V��;����?���Sn}���������ϒ9�i��^��{��2W���U�Hߵ�-W� }��?���4z��-�i�)n_{8�^�_9����#?��)��G߲��e��������W0�:����-*�8��`�]�ߞ��e�X�W��_�������||5������ʜ���k�^��*8��?rXS߻K�Zv�&��}t�e�ܯ�k+�`���������lK�=�_��u��_�2��4l1T���6y�6�R��g���ke��qḻ����s���kh�C�����x�� ��꫶��B�,���ڞ�$�ޥ{j�ܟtr�^���'1�P� �)`̧}'t�d n� �cs�:$پ׮��-�9\���G5q�(D5��[pvq�T�[y�֏����g�1�������gu��{��njQC�:���5�Q m�U4�s�n{N�²�c���b���g/X3wQU)і�;�� ��-����c���,�{p�w㏶��>8�㰞�IT�8{�$N���}Ή�{��Y(���;��٣� nQ�� ���+���ݽ�_��{ԵW���v��5jo� �W/�4J������N��9�vsvO�؛F� T5G|� ��j�b���CM+c���U,��o�>�i Ʉ�A���,� ?�:��o�N��� �H@lcw��ӰJ����9�%�a-o���$��E�ft,��)�^�٫�AM(_�T�q��^�ӊ��vսeFM� j^/،��׿[�CM{��~��gk������;���t~��=� �%�՗�=� ](�Q�ih��$�����eT��uZ�t�ms���z|P� �����Āb�s�$ ��Mׯv;2ꪏVqN��ȹ�Ǩ�@��`W�S!��"gAٜ������P� �G��26�8\�қU-�xښKFU{yO}�{�j� yˤ�o�S����w6�~Wq�b�� �M��#����-����B�{3Sa�B�,��n�K���g1���]���P���Z�[���o �"�{��-�48� �g����n� ������ I��:����VqqbTJ�*Ƥ̟��t��7d���ď�g��(x�a�a�>s ���"'���tI������c��-,��iƢ�_�����Q������$>8:9�^��t� �`&q���=X�wqǷE �����-?}�5�=�eS߇W6G�׷��Q� ��VU)���!�0#pxw �Q؃�%ӷx&�F�[�6�Լ����k����o��Cm�Rk�|U g�k1��� ��=[gtq�i6gjl��� �Y�Y����w氊��7�h 12vSgg��� !�W �j�ӿMq�B��Bܯ-C���~�^J ��Q�i�p�+�S����Y��1)�W�!�v��8�f ��_��f�ǧ���@J��!�}�P�ۛ�1�����t�hk�:����&���.�P�%䠶�$�T�{:���bJ��G{z.2�&� hn�b�,�ߺT�͙:���$q���o��n�ԹŖ��,\e�4S�E��|�)�>m(�����H�oN5�I�o�T󠠡�n�ms��>T}U������ݜ!�ߓxI[ ��� u3�I�߳�e+�Eb zvgJ�w^j���G[�!�Kb� ���;�9K?���>\2���ftմ��nQ�9]Rs ��v�)�{��9x*.���V(���5q����L}�� J���C�L�4}���,��'[�����󰯸�����#�2�h ��$��b�{TFI�UC�N���/J��߫�qs��a�d��&�f �vs��T2��,��3)��ϭ��~ �=j�Q_����O n���~�����Ҙ70\�r/iil����ս��U� ��,ΐ�W��,�? �mΔ��;Ҿ��n����LI�-�y(e/IJ~�{ro3��ڟT����$�� ��ˢl9Bѿ9ڟ�'��1u?؜�#n��,e$��͡��9C� ޏ��� �~��Ԟ�()��ҍq7�H��G�����J�+�r>�'m����9�����L^�5?�� ��8�r3�ɺ�$��n�Dmݺ���T}� �}(t� ���ߴ�+΄,N1g�� �[ds�^\�٘*yS��؟:�7c�l� ށ����6E�cY���A h3��3�#�t�"G��=]R�2j�ȡؗ&gH�ϐ.@�y�m�Ab��#�7�9r�T�K�:��ʖ��4�}\�ك^�N�e��t��U�d{K:ݖ1���fh�>��DW�eds�.50�}��M K}L�}��#o4+�p�&���ˣm5��x �-8�.��Q�/a)[�$����wsdpG[w��� ^#�g�/�e��2đB1t �nUv�%M��)��9R1�^������$�UBpz�ٶOS���!���� [�S�/ �c�E^g`|�'{�m�ߠ%�1��9n `�)�:5 �H��ghs���=(�H@� I ����� �ع5�ʪ�g ��������8Z�@q{g�r)ޛ�Hj#�nhoS ��y� c�S��nM �� k��S� mf`mC�����,q��KU~���E��#��&q$Z�dq4{+s�l�)���%�tܫ�G�5.��e�X���84F� nQ+�#y��DC�,u�%G��3��� jXeM3: {PGgSur���)���҂P\�*�}��)�@� ]�&AQ28��xU�Y�&ъ�O�G�k}��� �O7cb�a�����3Upᕣ����}��"���x� ��d@�b ��g���~�5���ƓC��9B� ���l��eq���A�����%r��Z�Z�C���� �&���Y��P��)�u �*�QS�偌"��%�A4��IWὢ^�rfg����u I��!,G϶� �:Ao����c U��� �LsxOޜ)X���N�z6c@�S�)�i�ts&� ��B��iX�7��,I���������*F$���ȽI�7�&4p��cN���O����42e ����� H��/��bd�j0�Ȕ& W��6p5xt y��k ��E֭���6n58o ��q����Y� T3�g�V��ְ��ƭM_7}�6g|���Ռyظ���R�¸���V��̡u�A5nc�}�e��q��dG�u��q�Zѵ��1n5 ��۸�JK,a�jT3e �u����~[��Ja%�nmݱ��u��=�U%Ӻ�K{ �[�>V?º�N Ha�jv��y�[ ���6nm�2"�V�j�M[ �_���V�|[�=*�V�Ub�#�V�=A?�a�mg8݆���EJ��}�ކ��^�ʮը��h%���,�m�j�Ū,f2l5����l�j��� ���Xp� [���2ɰՠYɶ`�V�@��� [�?>sðՠ�ڴ$�V����e�j �U��Z[3�aU�]k�8�r۵Z?�a۵T ���Z��{dg�]����ٵZ?���Z��0�.ٵ�k�G��]�ڒE�Vp��MG��hZ�ζ����F%�V�g��S��a~�a�A�l�a�A��}L��OkY m��J�HY��b�/X� �Æ�S�ы��j��O,mv�� �۬�fx��Y��=TRNf�wK��6k5(9����q����d�Y�A��@٬�蛤_ɬ��M3o�V��e�j0��l�V���h�V���&4���.*FV�N�[v.Y������c�ꏼ� �VJ �ju n�\��:�2!�6ku�]�n��: �5�6k�MW�L$�־NV�C[�z��d�zJ�ca�־�tC�2lu��6Hɰ�� �۰Ձ� a�V��`�� [=�U; [���2lu�y�d���*Ev�}�}��v����ɮ�!�ujٴ�q:k�e���'&Ʀ�y�bl����6muS��ض�GDV+�zI��d����؆�h�� [ �Ζ#�z�a���z �VG � o2l�ke���e�W��?�ek�ΰI�&�`��Rx�{+bt�� ���q/�qu���p�����  D��Pg�RF�D�ʉ��};��Z>�kE�08�a�> �c5�>@�� >4X/> Z��'�a��� ZZQ� xR�.> Z� �-�(G��䈖p��V��>�|w1| ��n9�Q|��r� �M�D��� \L> �q�� ���b"����8�a �Q�" �j���.Ƒ�޲�XG>@�9^m�kp�$��#G>���|h�F1�| �o Ciq1��S� �� �\���-?#���0 ��X��Pu�� �aC�0@#ä��@�@�)h  6�k6��,����E x�2� C�!`� L�]�L��@�\�Q�ޖ="��v��{�����JLL����oL�����ń�XvLL(Uv�0� XT@� � ��R w�88�A�0?�6 �=��0Ŀ�1$̨E�80d1E!��lq� )�?_HG� 1$�����S��r C�T9 3�� � aā�����,>� 2s9��G{� C�Ov9��GQ�!�џ(ǐ0q���?� C�$�  #�� �G1���� G9F��� � ��F(�V�Qa6��� í�8�a�=�*&pazl|�[t9ƅ�h�U��L��8�0��G��:��v� #���f �����'/*��!� ;v�E����@�l�X��%�-/*� �����b.T�� T�EkN�Ìr� �0S Tx���� 7�*��pOQL�½9���{2\N��#Es"�aT� O�Ø.�>� � 'N}�sD9��"���>��D�� ��A��09��S��@���*����/*|b�*�F�0p������,�� ����8�¸�� /p�G[�� ?V `W[�� ?�G9����� A� ��'����C� wu� h��9�14�tǿ l0��x�� �������m>��� �7��1@����1@��� �]�b�u �`��9Ί�����'"� 1��7���I�K �p+r9�S8� �8 C 1�s1F�S �  1��� 1�W)Ƈa/p)Ƈ���|��� �w�B�S{�:�Zq!�Ѥ��ҕ � t�K*%����t���m` t8��b �b�b��V�b����q�4=�8��"����"���1D���C�� �����Ӣ c�� "0�r�c�cR>0� ��;��a��C ����ňs_���1p.�q�7��1F +�c�7E�c�.W.� �֦� ��k�~hp�rЂAb�&�!a�İ�8�� 1g@�$���#���� 1��.� q�;��$�,}��y  q��81�@.G8q^%�1N���w ��(�81� �oUN�A4���� ��]��  \��A��D1��ث'�q D�)L�8�E�r�A ېI&� `tu1��#����#���"�� �8� ���`�'���p1�h��7k�,Dє���F��@H��B4� ��8��f��8$$S��� ���R A�( ���� ��EzPD��ѳ�L" ��H�8 ��JqD/)Jq����^F�38�*]1�MWG �t�o  ��*�@#�(� �\�C �*.�@ x���@ Gr��C �p1�B �$�s9��c��Q=��@@���@��b 1#p�@  s9���r��AA�4oC� �����o"Iܲb �̇*'B p��;�]�C �|-�@̲��@ ;��q���@k9����>���U�PO� ��x�J������~���8���S��ʷN�(�8�8�(�������J�͊�ٟ���ۿ�� !��� !� (Ѹ�TF�o���I� ^V�D������X�L]B�o ���'�e�� ����?⺧����?^���J4�u=LOB��� ��?������y2f K�ț6�.O� �R2����:3q�Mg�����NNXp� �V+��ES��/ġƣ2�ݯ��d ��*f�C"��~б�q.92,��Şe^�����4�:v.%޷i �^ZG��\ ��o_�7�e-贔��%�� ��X�U�I�����V�i���� ������t�>UҶĴ����`*�'����O%"1�_� h��rC:��Z�E'h7?~���+}���b�$i ܑ5 {�q\ ��98.˩ �G�&�N\��X��>��BY �AvڬH�2"k���\�`'��ӰS��ZPvG���N����j[�6/6�;�c��-�����"O�����j���c �҅��Ѯ ���_��tނ�'�$F@��8f��?��о6"���Q��w�5�k �e��z?�X���#�1MD�E� �Қ��E2�j���?�'(�����t�(0\�!Qi:��L ��.C!��A3���P0.�L;1g��X�C�F�D%�s�%��D�D����"QxJ�U���� ޓH*2c(�J������\�6�_�΀i����?�aGqJ�v, � �T��(��iS����*_�D硗W����� �*���P*�Z�&V �o'D� ��c�ZU�U�߀ �-�F̌{*�G����t��>?C ?x`Ng�N#�V��dg��8/���CG�hq8����#M�R✩� 7g�\e��C̃zaf�:[�:��si3�S�X���C`1�@o���@y�P�>l K�-��p, h�U y��.e�^;O�/�IS �yiB2 /z��^E�ݘz/�h�h��wy����D���M�� 4�Q� C��������Aw7 o5��-���G� �?����} ����IyĽ�Oz�(�q2��rFө�]���צ�S �C �fo�Y� ��5��\G9��v���}h��_�y���ø�FR��L�?>� �p�jz�B^+o�����э _�>-@oyj��d�O�"#v��IS"W�;�I����s�8bIeJ�����~i��Ī&G�E��(�@�2�aui"�=]�NJ5^�FJ�B��ޙ�/��/�����E+�e��& V*�iT13�G�"@� ���6�I�E ����r�1o՘ 7�R�ZF>v� ʶ�C�h����Lh�K�'��X����[���JK�H7j1K�U×��l&mA�(TK��ԑ�nO:��2>t{|�� �Oo ������ �V &�V+�և[�U" ����9�0�#�`W�Diuߝ� ȱz��>>E��#+/W����(�*���������T��H턗�y���zk` � ��?���8�] Y ��I�p�JG!m��B�Q�2�$o LE�k2�A^v痢�S׬ I�vH�[��O�� 3G0"]@��bd�:���Ut�q.��>#����Xp�g�A2������z����Z ���C+KRzy����YB��IE��߼⡗�_ŅDH��B��q3�hO���̷��T�&ʩ�� )K���A�=�1�c�9&�\G=ւ�H��]�*�"]ՒpJ���sSo]��4�� � �{��T�,�M �����|�g�X�]-���/�8jDh��t�t!�/^N�*sl���-�����qǂ�D ג]IxP.Vv�z��元�뫏 �R�����$�_�e��� ���>���j"����� ]�\q���o�-�8�9p��0@�u z�p8q�E�c�?�>���(ƛ%*F���V&��������2�3~��VnNs ګ�,�,����(������G���}���ʏ�ǻ����� I��ӥ���,izB �8� _��S0���x��-�%:�G�nU���� ƑC��+z�� a���0��>S UF�i�af��h۟)T�1�q �*�*_�����4���._*`��B�L~��8��������5��A�?d����'Ŏ�6�y��)@o������`s�t_Z *RD5��X���&� r��/��h~E�ɾ� d���O���oҷ� �w�T�_F���r� ��m��ulShk���qΕ���Gr^h ��޿g=�$��"M�{����ِ��Jb�����_�����g�������02��� �d��&]�h��/lůR4� y��=x�7��^�W�D:�ܴ@��n�2�iE{��Ż�����ο(p�0���ӡ%�y����|�d/=o�8*� H%�(a֖ FN'� �E�.�0vҳ�rn��u����p � ���I�#���ڽ ���9�0J����f�u�bW��ۈ��ItE��|��~(�a�� �32Roڞ�a���}lzq�%:�V�͎����@L� ~������ ���{z��8z��q ���ԪwI4���b�0�I�Y�cv�-�f!D^a|O��HR�@yr����h�$��W��b��)�ղH���� )�j8�[�����c^oڋ'xV'��>�%I�7� yݝ� ��ڵ��>^o�N���J����YI�녡8����2?�J8���r��j:��b;=}����ɗ�N��Łbp�2�zG;1?���L_�t%��z&T������K" ȟ�{=���f��/��Ǫ=��eg� �L���#��#e�Y} Ik�[Xdd��^�`b Z; ��ȠD|�u�B9��^ -�-�Z��"�n1�,�0��$��.AX4�բ�i-MM�x�:����h|�~�B�wp�x��H"s� �{ �� � ���^��v 6�����܋YOV�0z�~g�p, ��v}��b{��Ѥ�7e�u��`V�%v���)�f�W��R�G!�Z�\�Tw�lU�R��đX:�E�2:2�QĆa�?�3��6��%]�f$�ȯ�Zt.�g�� � ������ 7��� ���ѱ�~!F|�%��� �e C_����bN�n�RnN��u V��c0�{�1����s�@ �ȇ���G*���̡��P�3qX4�  Lpy}�� ��\��r�@�u9�`�7�"�e��dҭC���� ��x}�/z�� ���:�ɓC�9 i ��9g���G��=-�We�܃��P�{0 �x��W���'�����K݃�~P�݃���� ̩���D�G^���X}�ufqr��{0WF�W�jK��e�M�Ä݃����E@�rf �� ���� ����_t�� �܃I���"S�I~�Ȕ�G��_�[U[[$� jj�}�L&�Ւ�V��R�/s�����~� u)�}ɷ h�q&G�/�V����u��9�Z [����Nª���j;��ѝ��J5��#U���8�7��"�GQ ݨ�Pd��9�U�T�T;�:�^�� �yU.�l�� �.Y���GW_�f,(�Q����J�����%��-��.-p銎W��x�8i?P��~���V�l>x�]_�SwP׃mKֿ ��MÉAs�� �����x���i�A�ܗ�[��'�����GIףq��������T��|��X �Y��K�tϞ()2���X :�ڗSe:` ���9�!��:��Z#IW7@SӺh��ñ�3����%��*I~��Qzw�U����-�Q���h�0��4�/))���/����>��b�ZղJLv��; ���N� �8o ����;�%�Э��k�9t\�M�7� ��Lx���Z�Y~�Z�����ѱ�S��40o �� ٷ���ɖ�_f �/���vϠ8���op9��1�y@=CJ4�݆����,��� ���j�7�i��No fĖD�Źgп9��2��Z��Yg~��,�c�����k&j�'&ڠ�}ۨ�{���� �l� V��Go��B�~���R\�#�T�?.N�/ h"YI��x�G����̂��IQ Q��m�jBn�2���ȅ705D29�y�^n *+d 6�8C�nd�ՑAU���c��)5=[ �w��f����~>bڿ���?�P~�^4��.��a&� =�z�{u��2�t� .Nw �ؘ�Sb� �!�R�5���N��T�t�R'V�mEA}��P��^j��!-� �ۋ�+�t�� �:�� ��^��N{�i1��� n�/Gi�A�G�}Q�t;~|��J OED�"�`` �� n�鸫���I��8ŠOf�V��-祳ݜ��~ \�P'>y�>VDNz�-��E��"�^gաffa�xd�1 �`�ÝR���r+Dؖ�s"HF��Y��X�Q����Z�K��\�#��7 ���M��U\i�����w�M{Y�=_�|��b3 �\ #GC ��4v}�%���^t�#G�7 ~�����/���ջ� �sf�� �|�cB��g * ��XqT��9B��~��B �%�9׫��1��Vd���'ջ ^��`K�n�Yu��a�'�ͧ�}]��C��k=4aS��"�]&��"iyE���y1nb��7�Esq(�f��w��E74Bw2&r��1�w���H/��� �"�d���Ҳe�?�a~�LW�Ƌ��}�n��[^�.)�v l�������`l���}9����Z������9��tT�җ&�(q���8�_~ t���'2��� �{�� ��� ��F�p}��`t�4�v�e �A�b �@w�(=����w�06������2��_�]�L�e���t��=�S�Δ� T�!̟5��ihm/ x��a��)�>�o=��O#� �m�B7O/��`���-���G�b}p�C�#��+�����d?�1-i�u��x�W�^�xز��h��w��E��J�r ��0�sZ�܊he�)B���u���p�Ā-��b@�M��/Cў�%϶������s~~�5q�?���������!;u���� ���ۯ :E{4��~�� Z��bF�|7���)H�̩i�B��~�U�܆����Zyo+_�.�g���~lT�Mmu9{V:��qs� ��xU~��M�� ��h�˙|l�ex�.7쵴��0 8��O�^D� `����4`����0���c��fx���;�� ��Q��t�6�9C�Y(�t~kN�6r���ƴ���s�8?���̿�#�t�S�]�_uv�Z�W��ꗎ~ݜ�B%hd�:b��Fht�F6��hvN�e�8���!�� ��r��@��T�C�y����w�Ie"�޳ h�+ E�o$i����[A�}7�����靏�pQO6� (���+��e�S$��+�>Ƴ��tgd� iN�v@hl���f�1]O���[[o��M��Z_�eW���xk��*� 8Q�������L6 u=ݐ��+Iɜ�/�|�R9�Z�/�6 �0kм����e��΃֌��ә�n�����\r��f)���� \��^(\V�S޽����Rm�v��c��2=���� ������Ǥ��L ��"�uܸ������i%����Q 'g)f��qu� .�1xA����9"ɈNun詜��,�C�P�:*��� ��2�M>�)s1���g3Q:~��#ϓ��0�Y��#g��`���:�V�H��hc���0f�z��yƆ�R�����" 1�ؖtJ�^��ۦ[���Ti��|/I%֘x `qP>�0��r� *�I$f��D'lZ��\�]ג_Ľ��f�G\B��Mh�۰�j�h@z h�S[�H ��iz���c/�=�(%  �W ��z�Д�*����ƽ'����L����Ub+2C�B�[CS�^�jxډx��+ 46���F�%@�?6�?J����H�: �iz��HzTG꙽L6 O���5 V�-S �n� �5��n��=���u��� f�Uy 7MO�� ʦM���'��g ���OJz��� AH�����9� � *rS�4�YC�]�mJ��$׀8�M6� �W���M���M�f6�X�� ڟ&A����%�L��i�Ħ��!Kԓ�ip{�J��" ��m� ��Dʰ9���G�Ӡ=��A.�t^��h��a�L@z�,+H|��63~��~A������yQy9�T�29�D�Yiu�&�I���"fs���*uB��J�m$=�6����STG%��i&���=�$�3h�.��단�=�r�Yu�� ,�*� Vr �8�\O�5��u��\�D���YF�ރ�8�JM*\�L��]c#��= ��S��n���b�������ԨN ����r��=J�B&��9����6 j؃��o' �z�hĔ� _�x9L��D�5���9���j���G�A+���A��4�K�๗�T�f���d���Q����q)C� C �� �I���|�~܈���u���A"Δf̰�KL�~�W�����^� ��Y խ��m��DUiHU���3��O/���n���WZ!5�*�D�d�\tq���L��T ҝ ��0�{y��LԜ@* ��Ӡ��)%�|���}` � ��@�/�C5��Ԇ**F���Yŷj���~���Cw���h?i�^O�J���H>���Q��:��6 uWw�4��qy?���=�og�݄33n���� ��Wi���S�+��ZE��.Z0VWh`a��/�t��U�J؞햢9C"�\cpF���s� 8p�Y� ��(!V�[G��h�W@42��ۋX�Ϙ��C=�{q�1���y� no�c �!�1��ҽƁ �`�F����SB�M��!�ܷg>���'�QX�OȞ���o-MY�(���n�$��,�w��ԉ�yA =|��b����0:� ��)�8ۂ�Ҩث� �A�"�h!�{CXI���_T���.� �"�c��/G�/Jh/�Wp+��t/F�>��+[G;S�����:}dKS��� ҏ�b/0r���O����~0%n��;���v���4� ��hu�.� J�ڶk ���y�hR;�>I�'�Y��$��p�G@Q zJ��y&�DT��p� �4ա�Y�֩�5�$� P��٩�a;/�a� �t:[�D���1i��V�vD�Q�� �T"�����@6�l�@�#�$P��M.�����ތ��G����P��t���t���jM-L���@O�b@�J� AH�, &�eՒ�sU�?9��@�:�L�{��o @� h��q���h���/J���8�߿IZ���)mf�5��,@�'_$U �� �����a��hW{�pF���:n �*c 0*^�_�#;���B��{�.�PW,A�V�Ys�-��.ڹ�^��e�~�Tw�Ԣ�-M�s\ \�r��� ���ۨ;Y��h���~1�Ϧ��c�BW�� ��i���n�Z��������ߤ�id�1o��(���s�[����Ki�#��2wѮKu�˖{ћ!f�����ִ�/Ϝ#)K� �$�N����\�.1,��_���������������%xPN g�N+�  �K_A��J�:�� HH����s�@8ݦB�. 8)\ 0Psz%�]^�����n��A�߾  / ��# R\d/PvS�����&  �f[��]��:�?��3)�����ܵ4YV#g�k7 ��. 1�]e� ��v� ���D8`Eu�x���p�߰#�w�_fJG:���j�0W* =R�|)���� �n�z�yQ2�wXkF�~���@��  �M��]��W��`j��'�3�}�� xJ̖� #KH��8^�G�iq�p�\���l�ò&9����1/ G�^��ԱZN�,6N4��1��^��5n������ԋ%���i�N�b��OM �r26 "f�Tr ���&H�T�V�eVΫ��XaP�K{I 앹Yu@�8=�7'�uᜠ�y�N��p��v��fՉW\�8Î����z�����lUaE5 �՜\��2�E��-���u5��.�]Ws��q���ȔB�t��Q�z�f��r�Z6몇0�����d ��5U������e,��^�%��W�ض՞#R�����IoQD���@�)U���tU�OQ5��n�u���� A�������Z�htv���� a�M�57R3HW��X�anT���uV *m1-In0��k,׆tz���i���F��e ����7�8������T~yG�H���n02�����P�M; -�Pqt���89;q2�� ۾��1!%���ϜIj~C򉘇��&�=�,6�e t�: �L�Ty��,���& �^�eD�ʒk���7X>��b���Z��t����|b�]j��vb�P{m���`[徣l�� ]E�bFZ�MNr �Z}/�ۊ��j���ץ���Xlu3�9��5*N6�-jD�Ԅ�U/�� ���&�2�.��jӕ �Y��,�����lQ� ]�ʊ3�FL�\�u�-w����8���&[N3\;���Ŗ����b[%��B��P,ɘ�Ai�l���_ ��ټ�_�D��������l� .475 6�N �K�l{��Чѷ� ���ӣlk��o�����՚Xl9�v� [$��i�w��k��x!ŋ�V�$�!�ڮ[l֨-��\iN�A�ש��^n�^��*�u��=ʭe3�A�$�ӗxb�� Y��&�Z���j�v.�ڮٛ�V#�v�m��Xl�rWFz�bPI� ���b��bN^���κ.�X^U��|%ɴ�Rs#5F/p����Df 5ىf���Y��k8.�Y���{�ѥ�cm����kq�r�C+]Ta@͵�N����n������N��f���d�4VqN��/��j� e��(�滥I�}6�[�����D�r��O�\�� 9��b��XҌ�N I���/�F�1��UV*�zک�5K��Ĉ%�1���1.j��|~K�h��^^S�KV. ��2�Oi��0 ��Ju�/E�~:�e,�%���>��S0qQB�"�u�En�*��K���2FM����3& Vj0p�\�e�P�$�B�[5�xW� �V] xgLTg �X �h-��n��>+��d�p K'\c�Ľ����y.5)���;�0�!���l�E���]g�5�r[��u�.�-��i�'�ưB+�,���Y����/.� �Hn����_���G.�crUQ�d���� +� M�>�B�kx y�#x��4�\�\�t�6 �K��mZ81ۈ�/� /HZ� �ei��MH�]u ����� _x׏��,^+ �V^k�aV�e��Պ��+J�$�֧IBrJ�$E�yZ(c��C��mƩ�3r'X8�8�#��f�Y�+&��kE�-�)P6jh����SE{y��`8��lRa��V���U\Ù���2 ǈ��YB�� B��ɒ}�}�y�F��c�x��9[v3�x�+��!�L��/�-Y��9 ����� �D�C٨'�Ӕǁo� 2�Z� ��riͥBb��ϫ$7n�{��� $ �2C�䷭�7m�^�J��P��X��`���XDF]��x� �;���o���.��&�3�u)��"� ��pl���Q��6}��g��]h �����l�cMI �lTf��D����m�Ǚ� A��sQ �����Tͷ�Yx�*��ŭ�r�� ,1Ԭ �  ��h�Va}\��W�-���e6��ܲ�f}qX�RZ���(I�e "�k7c��+G�K��=[�WgC o���6I2��cO� A���&��qg���,���� ��̆��,�R����:�je�l�&uz��Ɛ��Ζ��K�5�i.]��m��u6�}oK��$�h�A �t �ժ1�z�"lK��HW������I�5,;�s�5Ar�,E�c#�HW�PH ��yiV$�R� ��B�x�.8u:(��18!Ze�,�����jĳ8Hhw.'+�n@��r�_�C{���}� �O��?.s�+ �F[���4ɮ�57R�kߔ*�W��h��j�II��e�CYW5��[ .m�rm� �96طUH��h�x�і '�"r��^�4ŬXfB����. �雼� �?D�}.����iY�8M� �ډ� ���� �裲�p�NqQ3�F�D��"�JeUn) z�DF��.I2Ĺ�o��$����Ή�-G�Ӭ��1&����Ҏ#�q>ܤ8'i}�dm�+I�*۫-��#����F ���2B� vʚQ��S%�9'O������=Ȥ%�*�J�x��D*^I�U9�ݰ V�W9��i-}��rk͍��:~#]PNn,ۖ�z���� ����Z^(G@7V���|T��� �"~kRI���/�?�*I�i�K񐢲⣔�$=�x��ΖGD��̨� �W+٨����z��ɯl_�R� �=���-��\�4W��� c��pY(�����lU���j���庖�9������X�"��Z�h�m�5�x"G@�,Ϧ�EE&R[z͍�$?~Su��� ʚ̶�x d���X�� m�d_�Z��*� `�'����։�N�` y&�b^�4����� (s�pcE pH� �zv���� |���wwo�����?Ư�{��'��>�_}~�w�P�Y� ��D�&e��n���iR��?@D3tr��p8g2�O� �!k�ōS��QcZ\����u�����/҄t�"M`.�Ӑ_J� i)?kܿB �@4���N� Lh=D�ץ$ +���@�)�D_���D^�L0�����Y����O���`�9o��}FKC��d�k�����qm�� �.>nv�yGtU�_�9tnW7�ǹ����G���#�n"Q��� �6?N3GY r�Be!e�4X�w-���v����GHAl�L�,�D�[�i��1eY'n�HI����=5i^��; ���X�-�{&kLB��W2WI㼻���h��'x�99��� �/,z��5F����m�5�����0���8�D>����V��D�?�S�G�y���>�� ��X�7!�p��ۅ>n� �� ���ё8���|�%� ��O@�d����x�e~k�E� �ql�L0������Α�m��'�:FX3�s���OH�0��v.H� qn �����c(�鋝�g�t��gD�K ����ɄF"q��"� ��Ҳ�ڱÑ9Hkg��+���P���������6���ƪ��]���D4`$K�kaH���Q������D�c� Tu�_�y{�}�m�!�W�/�b��ةߞ> �:M���r/{e�ZA ���n��R���V�ؖI�w�:}��x(q'�;���,��i楸��65Z.{��/�e��8�1Wui��#�f ��SP��l���:i]������,� �mq9�}'����i� ��AdE\`Υ S� �]��6��|�2� 4 �������C��^B���+�Z!��O:�gK}P[FN�~�3L~23��Ќ_>�� ���4���`��fj��'t[i��e�o~�Ԓ���4D$���-�C��qԽ����+������!��aŽ�� ��(:�C wSf\cPx���n��y����IcO��� ˚�� [�;D��ю9����4��c�\⅐d���� �I ��n�Q�� d}�Y�X /j��቏x6ܠ�>�`� &��yh\�~���� ���ۛ K2�iO� ���ȅ�K�~�5 �ip���*� 'b0��)��5�Hy�1� /��ct�q/̐�L�j��Z�d��xE��N��m^��@��k%�t:'�?1����� Ns-�H���C����:Z_H�P�v 3�u�Q`�� � �)lߴ=q�bд��9���2)]2���NLto��� "x��*��r�2�;����9�� ;bW��|� �e͗��=eX��`Ux�V��7YF��q�s*�7��Gk�&�Hb��[i��B�,��m�M, ����uf�M � Gr��6Ԭ�v���,\ᬛ{�=b֌���ٙ/�k���ĵ�c N�e��]Q�C~)#�����\E�z u���F��'dS��R�{�W�?9}�ݰK) gf^Ï��L�7���g���v���d�r/�t� �����%s �xH����b�㢛x��� ��a=a��+���'����v��Cw�ܐ3�S�艺���$ �0 ���������q�%3�}����k��3�yi#^� 6���fN�2��'� �ym���.�8:m�=�n[���@�F�r�>�?JD�5�/���Ϻ�Lh�%�!x8�y�$G�B �F �X.�J)�4='��S��χ��J�3�G��N�8��Һvb� [�W���cO0N8���b'������ #�x����E�� {�V�5`�����k��i$�m7�9ď�on�e�د�T��?����߫� w��JgdS_�DS���^J5| .�'l\�(#�޶Vl�v�%� ;�������O��q� r��%��$j^�JL�ҭ�9�Ί��һ8���kBב(�Ld>%�Z�Y�����D2pD���5��l7H � ��������br�Q]Y@LE�i�L�����V��������� bH����e�~�cwb�{9ū0L���XN�� p�vfu�ǵiu��L7P������endstream endobj 184 0 obj 34293 endobj 197 0 obj > stream x��\I�%�q6��� ���p=����B@˖-z�!r�|���f r���gJ���""�^dm�͙��� �s��������Ό�������z��_Xs���}��V�����߽�0�k����ճw`��`u �{��CL �� ���l��a�+Y������Y�� ��%�y�it�FuxP���n�hU�L}��ٔ����%X?\�>�U� �� �f��*Ma�ZaR���m�IP.E_c�(oY�e��k[ :��2�)�e��9&�,��$��3 �ݘ�@D��N�A�-�9[�f���VQ�?��5�-���e�I}x�M��J !�Z�RR��.�9J��%�Sg}e�굃� �hI--�y%v �fK� � �o��^wT��,�Mc�d��'S6���ťA�:��ofj�6ƭ����ep���� Ni9|�`)�5�h�+!>�ڭ����t�C� �,$6b�ȟl��F�� �a�2�9e�]���� �4�@�u'7� ���������-��S��K��C����'�)�/�#> L|�0b�N��N>� ���ѕo t�����L�>����=}�Ft'���nӫ���v W�[l�#r�1�B!_� G����j�^W�B�� %t��x�b��d��� �?]O|�O��rԃ�\\��v�9��z� ��E�2 %�?MGAZִ�gP`��5Q�� ��|�n� &@d��E*N5��'�_H�L�= Ϋ� ���P�H�x���TFm/ ����s��B�p =Z  ����)�ܱ�2s�ecM � � �� �ߛ{� �>?�b�-�|Z��� *�je�t [Q#R^��R"*źs�|^&���]�� ٝ=*�Qj�}�}e���� �ƍ:�� �'aG����� є%����ii���E�sF.b�LE�mR��ZL��f)�r��M��(Y� ��ٶ-�к[Y��c��m� �y� ��'�����*}\Y�N�H��@� ��t+����!h�WWY`ؚZ&V1 �k��!��ð�t���p��\��|ֽ @}5p!�hw�~c�� � �Ae�z�v��a�hD�y�斲NXA��uv�#�[����Z� ��RP� 7S��2gԱ-|�V�� ��Ho�+�2|��� ����2�a�k׸S�v�Q�R��\;>���i纐�� ��~3}S ,�O��������֌ŏܹ�dk�6��T�� �U�C5|��=�l�`֤�ċ~q�+�gH� ��N���m��΂���nJ��Me� �&=i^�GI��e�u` ����&tOHr7t�T#�)ɇ��9��A��K��S�?#�_GB��B?3�.񝜡�(nB(0�J9-O\��m1������X�- �a˔�Q���Dx&"޸h)�@l�.;I4 *����v�L����>��?��C�d[jr�;2S����ҵF=�  ���C��ʹB�$9.��=�m�ƧT���H!�w�Q�R`�����1 ���DG�'�=�S�)9i p l�4A��+�{�/��C� Nߝ 1��SF��䨤F&�ASO�r�Qb�-�4Y�)U a�:�m��e����4� ����HL�4Z�^¡wWq�^��E9��v���1J�_>�������]Q}� Z�6S�^�K;/B���Yė�, F��3~�ye@ ��_�O��l0񠚵��� ��|I7�lהFi[�`�����r�l�S���]`�߫%.�2�B2]�w����O$�U�!锹��%�O�u�Va�EL����_�6@}]�������}��(�����/ �/ �( ��>V�_��Vqm̜���E��@�Z��ܪm�� P%h��slg���U�Ӵ��@�C;J��3t�Hk�yڭ�m#s���Rz��9��k�'*�[XJ�W-�!�ȅ(nLح��b�~!��J�(�®L��" ;u?� ���I�S Nn��e��ln���]:�؍��v}���;Д�k�̳ S.D��e�� �2kZ�U�B��P+�lUzԵ�{���+H�&� CI@Ŋ�uJ�d�e�@�Y �{����S�@�S �zw���i}�#�:]�P��9�uL~Zܦm�i���I����E�d7�U1waq����O3[��G;.k�$��L�}B?��P�.O���8b����Ta��M\��=L��� �3�>E�[0��U ��H�v߽NV�$��*ʩ݂8��/?Q^���ldu_�V? &��N^Q5ɭ�g@ =��M��E���4�[�� ��f�^�ε1 .�k��bC,{��y-�=D��v���5U:�n�W#�Y��$Y��  /m�{�j�4���ySz!�w�}�z971C�����R�� �� pRQ�j0�AN�+��l ,�[���a5�wߡ��_爼M;��VfQ}[?�X }L޻;����`K�i��E��Q� A[�AL�}��ޱ��Q, �o ��͇>�b�Ow�n�_�~� ,X����5 NHKQ�x+���m7�!Q-��w���j���.�(��"?z����C��o�e��?Á��/�ipOhendstream endobj 198 0 obj 4772 endobj 205 0 obj > stream x��\K�$�q��� ��Ns��K�~Ȧ�� "G'��ۻ�5�;�i����Ȭ���ʪ�%-� ���32 _�t m�i8�Y��3|XkF� ��^a��P_�u𱶦�c�J��i��{�ǉ�dR�u�^ق�S/� a�ڊ>��>�G�[8�k�7���e�T.K� �]ν)���ͪ�$1���߃� T�#8 ��� u�Ɂ� J4��v����}Y�@0�p�NR; z;0&;}"RY���N�=o�q ����]O��?vw��Ϗ޲��fsϷ$$9��T�O�����k���댾)�й����p��� �� �I���/8�� ��ߵ�� _OJ!BB�i��a�i�X�q���Y^�~)�����Ǿ���'��#N'oc|Y��D.�iAG�Uy���~g1� ^�����` �}�0����[�}Ak�J%_��Nz�+*4�&���Ѐ������:��!�P���z|wu�����i��AC����\E�࿾-ˍ>@� �Պ�V>w�-& k�5 ��pZk��`&IxC]�Z�z+we�jV�?۔�d��,��sM� rv�_��Ŕ����N E�� ;����T�*߬�z�9w�s� $��Y�8* ���9㘢o '\g��|_h��L���:0�!%i�?˛�鉈s� :i� �'��,s�o�|/f�� ���u��_�8��7Y�6nCo�TR�&�Q�y�d����f�� ����ai4��b�d���Q�f�I���秀�^)Z��à��H&t3}��95���P��uR� ؍�Plط� ��;���z��j�לe��� oi�J��2,L�D�w���U~] �ݏx`o��� �� c~�:��ct�F�6$�Zdm�q˲ ��l ��x>h& I6�0J8E#��va�@���iP Mᖋ�&&N�&�@ ��yۗ��V@���%~+ t'�4�{6�̷b 0�m݃C�#:o$�;,`]���X��s������7��h�� #B�"rXOtM� |\��T�N)���"t��UIb��OM�R� Q��]��e�t��wk�2\��@D���'R�����/�O���b��,��������4��*r��H���č��)s� Yڄ $vGt)�c�N��[� ����P=��Y5 mS�K;n��������9�MTޞ ����0s�ö�k ����h;�_ ���u�mX7y SU��K���1v5L���v�E� ����Ŕ��FZB�9��a���F:��0����׾ ۠�}5�������*vlzL�mA^�j����0�?�ǘxh�K?�Wu:��D����k�5��p�Q�T���xr����6��g��R��p��.mS7 �����55a���q�~� �(�M��� �jmI����_���(�; 6.�Qr[� g���wz ��VHM f Jiq���B��@g�KW���8ߑ� �t?[� ��P.ط� �N����� :�e4k�,���Z�j��Ɖ��]�� ���O��a;9o ���%�`b�$�Gܵ�>R =z�����Ve�|*gb�"��:q?��=:��;[|�6�Gh�u���'�*�6y�͆����}WJc�۸�[���Qa�-��: A���ښ2^�I_+1�7ӻ�#�/� kf-o�jr8�8��v�0� ��_����J��q������!��B�čd:���(�����֝�!��sp��AE���J �#�ͼ�ǲ�z�_һ2Z���y�,�=;�� zR�:C�/k��~wJ���Ӝ���'���2r[/�"Tn�ɏX.����Iuqaz�� �بd�|)v*�by��羶vɯi�i��w.Xk)���S�� P��� �(@���\&���n"V�~�w�c�.p��ZyY��$QT��t� �R�K�ٹ��#RAL�\n�9 ��FM.������b�6�CM�> stream x��\K� �u�cw��"�Y����z?$$��8��@�% gaJ#Z)R����|�����ꞹ��d9� hԷ� ���G�W5ꃢ��~��� iux��� ڜ�2x����W7����1vL� �8�������BRa����CL5���V��G��e�M�/�nL6�E>�\*Em� �����?�&�d��_e4�Iq������c>=������-�;-k@ ���S8d�?��� �M���_o~���W��.ŵ�ʚSܥ�I|���:\n5�!��ᯏE�\ Ëʝ)'6>���e��J���7����I�!��z~��H4M��zQ�KIۙNJ���i!q��r� ��'��bK�@K�f�(������ЙUN���8Y���3�\�s��ج�c�x�K)�^�4E�yL�I��*8)�VŖ@9��$�����y�p۸�zJzk��,��¡�K���u�����טV�����lNv��j a�Z��� c )1v��N[�� ��AϯB��Lv; O�1���;��хyQ��';������:$%���:� ߖ�#�� H� i�� ޝ�]X g�Zп ̋������ 8� ��*!ωB9��]��T:� ���6��T�s��d$�߿)�� s�k%bc�\5��q�i$����FM� �,����"-:�I�ހ�\���� k��vD��5yz�r|� Wb�{�F�[��sܺ>3 ǃ��� '�8�3tw��[��A�2 �y[�GͶ � (���Z ��ݬ�6�UF��{A �Йo��M��U;�5�7ܒp�A5 D �Cz!�ShؿG Ӌ@���j�ȴ9��p tw��S�C�+��D� $fh� @P ���g�0b��3+P�L�Է�t ��K͸�^p�'��VOkp`�t6l�� �O�I; �`z�%��☎�ƙ��C ?����J?�o��Y�LzA�O�,P��2�����>-!� ^��"�>X��Y|{rr�����΂f�i�=L�7�# ��]���# ���/��e�k ��C��S$ B#pL��t&MްtN� �@�J��� �1[,��%���%�4' �^R\�3����9 �F }l >�;'2�;"�ʇa\ƾA�LG�Y��� �����i��������: }7iɳ��S�l�Ad�����B-�w uQF���xk-R���� ]}s�)��Ge.w���\�ɼ/��N9�xe�:���e5ěuQ�ې��%��btx�#=Y»���/��O���Qi��KQ���m�M� |��ύ��^���d �08�3R��l�o�˓�dT�9�!�̂2J���e,�| q�� �X(�� ʖ�5���� "j>_^Ze��R���Z�8.�99�%�p Ĕ�)2�$)�PF{7#S�3��������$��nQ�/�`����M��N�S���꺃n���E��`��� l�������@| [i��3M��B\6�p꿥���ð� ~S�~9�����tmf*���A��e[0\����M���� ����Hd��W�c�Z�*�>� ��K�Y���b]}.]'[}z;�E{q�8[rĨ�nʬ�=��m��S����U�KW�b�ҋS]պV)�� �i�t �hN��:�I]���{� ��kf=`Tp�F/��ݤ��?����K��)�m'AJ?޳!�ǝz� Q)��E8L�}���֥�;֎�e���r!h�K�B� �Һ\V%3l�X���p]_^�:65������0��N�3�:�d�g�x�R��:Ż���T�Ws�n�R�$گn��io> stream x��\K�]�q��,��*��9� ��!  �Ad$$�"�B/)Y$5"��0��U�}έ�ǝ;� �9s���Uϯ�Ϗ�jշ �՟߼���WZݾz����F�_o�o����sl��͚Uַ�_ޔ��J�5$}MX�2�����עj�.k���^���{��7_���6� ������5�` �İ�x��A� ����~u���*�|��N%g�������&-��g��a�����&�/��r��e�^/�-oc����q�7.��w�R�V�^H6��Ba�J�t� ��[��5{/v8;�`�oC��TVlqH�M���d�r�f1�;��5��|y� 0���� y��f�l�qy���n9�c��/��^%M^^�kۖ����t�ٚ��9�W�k�Ur �j�w͙�c��K���U�[�� UZ��-�z�2�G�'��Ѓs]N��{�L�e 0U������D8lN�=��R N�t,�%�� �O�uY �1�AZ���Hڜ,���e��4: ��V�����5�+�&���˯��'a�� E8��/`��[#ӖcE5�n� ����G1O�ւT)� ����V���� b �N��Sd3W6E��t�kT�N�ke��$��*F�3�{$gW�߶B�ec��||_G����7�cژC�(�yU:���8��pf �� f�ǇyM�#��}�h�&�k�19�hp�����5�OЩ��v IO��E�o�4��hl��?Ͷ�̳E?#Ǿ(� $�ZYT���2n�Jxs��1�J�F��4蝲���O�T�����0p ��Į>�~/� �� f�yc �V=�"u� �eH'����.-sq��������4��?|{jW0��τTm*���$V"�w� u0�f� � ��Uo��פ�N��ViS7���K�QIG`&�V�0�=� � 巅���_�ϖ�|�։��#��S�U���5e�p˭ų���#h0?X0 J鑢v�[�7�b2"L���շ?��V��k�ۇ��|��,�!96�S� �"jl�CDߛ�ı�lI2X?&ە} z�}I �O�o `I�!�E9yu���ư�F�K����>Uc� W �#�5��2ZB���J��C��s���6NSg׭]a$Q�o�����m���:ֵ�����g�@n�@ ^�� �!� 8��a�]@� ���4��}�I�¡��t���̻�SJ� �/�}uL�������i�]1C�9Ӷ� �:L��VjC��X�K �Q �m(+F��" ��7�vWF t�~��ۦ"3�&N@�����DN���� ��^S���8��jOb�];9�`.&pl_?/r��j/���»����-aB[v'[ 8����2��;�!�x�ØdP���N� U Ƣ�(8�\� .N �y�D�։1lF�*��c�RA5 �� �����/�! T2����aAYG�{� �K���ge�_q �p@��1C �4!ѳb�S��]��I���F�� ik0i!^�+b^��>X���� �c��U��শ�3�������q١ *�N�@���^��c2�>N��tfЍ���A��m�X>���>p��Lv���� ���2>� �{��d�u��ģ�D��f��>�L$��mr>\to�M'�܂�H�{��p�?���t ��t���gX�j,�)� Xݥ�� �R������ֈ����AP~hk,f�:Ss ���!V�'��>�G��_d\g;)FW��16Z�h�X֝v5`D� �������m �`�'g�D ,���Svm���xD�JJ�6��Xd�yA��B�g��>㈺��!�#����G �W�up/ i��� ǂ.�{�t�O]쥀�\~L�����[���{�v�|)����H��Ɣ}�g� ͇(=�-X7�3�@O�ܔ3Z +9�SE.����S!���a-���ㅟ�����4�>�J�Y�@Pe�1�Ҟ3�@ۧs��,:��bâ� �O�¡��h]?�vlK�X{��ӟ�D��P�8��#���>�N�zIi54�� I��8g[y�a �k�G��y)��t�b�ʀ[��q ��GИ��� K�D�Ȍ���x�*,�puTh��ȏ�NaLc���r7���_5�-i��HϏ��*�pU�fY�w���u�=Ve��}Yp@/� �9W\_�YӢ H-8E�����:;�%H��U�4�]ݤ��i {m��=3���G�A҃� u� u ��y&�V�y�z8�v�cX��o8O��� �� �u ��2��Ӗ}Q{�'���\�O� C��w vw7t��Hp��k��GKjoE!�!������s�5�C�0�J@g��ٝRޢ��Yٸ�Jqr���� �^� a��ƭ�g3ؓ�I�gH ����¬a>}� ����؀�&��M VK��.��:>�]/k�/q�vmF��*o4��k�=��N]�}��F ֌Y��U�A�/yz'�q��e��`��x�.�D��^�����m�r�pϼҝH���`�kBmک4�6���Ş0���1 BJ�/X ���~�*�@�/+a��Ų�'�*\��n��A�?��m[Tw��xEHg��)ɀQPﰛs(:|T�C���G�DD�7��?:&�zK8#���k��@���;��,���ЄC����@F��|~� Ƹ�V�I�4/4N�(�L���9; �L U�m��8��e���`��>ӱ;�(�^���B-��,f���W����F�/����iY������c�m��m�2#䂶L^[��Շ2S.FPsyG�Y� j daO��\z���:+���Cs\ �6��6��/���vm�KH q� ��{ ���y����]�������j�/�l@a�(� ���;욜��@� K�����?{���j�]��� \�u N���� wF�$ p0~i�vl��!�H �Z����ڜ�;��\��(�� ;���~�K ���c��(�FO ��.y�V�dE��΋S{� wX��m{A�U���K�����p���4�*�o(��S������� h��L��`'��i���e�;c�%͕= ��!G�jC�gVh3�Nr`a�E7Y�t��?I@g�� ��}���O a�� �O|}7-3�#�Y��]��q;(5��������MLs%��Xk�4��u�sHWS�, ���N�"�)k�R\��*u������?D��6S1@��k�@� ͙G3u~ϡ#�Z�����[}� ��|��ڪ9ړ$*�N�o�����3Kx~��T�����4[� s���3��OJ�> stream x��\Is%�qvH7 |��'\��!�j_�e�-Y�ȉ�#$ ���!E�B �x~�����������  i|Ө�����˥��K3�KC���?u��Ϭ�|ywA}��8�6�1G�)���2BRq�'�q(�KǙ}����1�d|�M�CN���5�$�ʻ�^�;��-���E���9��y���ѕ0��������=�g��S��Fg�W��r��oo�B�JIX�iL����`1X�(�v��`�pǪ^g���h�6&b;�`Vb��� �D���41�e�8� '�Aһd��b�̅hkT�� ���zR�XL�L>�!x��O!��y��I �}� ׷�;g����ߵ 2��y�U۔�2�11$�\+���'�K�O�B���(zI/H� ��f�����k��H�K1��^ig�2��� R�3&�@�u�p�#yJ7*��5_��U�X 'R��2��SPH�S%���e�}z�� ��5ῌ�3�k�7�J�}-�'�͓8�^_�c �~��)����  ���g %r@�� �j�r���嗔kaĦ6m)f�"'B��BZ�`�)���%���cO�0|Z��5 �X ��z 1zH�~���4�3� ?b��V�,�K�Dn�wm�Ta���VqB�%�m.��)&Zm��#�f�)���Oج ��L1d(:���$��y94ɒ�䖅/�] o�cN,t���7��D  4��UV��Jv��S��� *�tn[&���-�O��p#�*�.த�k@֒�![Z�;jس)%���� ����$/Ǥ1W2;W���R]�� %O5�&�z������ 1�(�A�ӘF����L.��2�� ���B���JZ_5P�S�$��+oU�F����"D�G�(޻�Щ/C�Q��T᤟��1�0�о{y ���rM �Kѓ�~��y�O��ϋ-򿝘v#��߯d��O� >D��}h��;*��&��g8�a ���G��Z���|� f*���\]�� ��� � �n]#J!�����Ni.2xˠ��b���Ȏ��yC &9� � yl1VoS� � �l���m���Զ���� �I�(^j �7E)ʵt��� �A`��7�jMY���Wy��l��s9�vV��� ��E��j�HnΪN�i[)�J��l��j?/����܄# �K�|�Uo�^z��f'�� �3�ۄl�T�5���a |���v�A8�@M�j���M]��b������?V��ۊ�|6 �o�I����� �F�)� �������{ە.���m ��� �̉�;�2!P�f[�K�vW�}lh�Ѯh���P�Hݒ�1�E��9Ce�z�N��0���E�Q�,7���ֽHU�دf���NN9��p�Q���z{���-���1�a���j������z6�4Y�u����߯W�ns����j�� �S��3�by�-���L�fO"���5t;�m.0�Ѝ�A��.M �'��,���G:$ ��Z�bPx�O�r�ޤ&I��sr@o�kA8�e}�Q�h=\��|~Z8���\D�"ǵ N�K�T��w �[�gTJx߳:Ng�֊Z ��ިr���I��f����]�-�5�*\������_�5�nX,�� /��;)A��Z��G�'odgZ�h�+�����g VL���Z�m�r�F�/��f�ĭf��{ Lv����%���}q� ����u�O��a���m1� �ܷ�N�� S'����r���\����_��1 ]DqDZD�]�dErc�V��'7�*?��Q��^��v�ee��)`醦�IHՈ�0�j �mHi3�\w[T_��bժ��+��d ��,vt�yf��j�K�����/bZ�w���Ι{٨��'P(��8�1�W��M�'4�����:�f΁�s�N$���� ��)���Jn�L-�Y~|��Gl7$BJ�������w`e���N�QI?N�/�kD�F�޹ ���ΘYB5�IA�3��l�Vp e�IP>�g�/A��r���Ny?`iݼ̞ԝ~��!��7e����~��������zh��M5y�g �gѶvN N�ZN%���˝��� t��P��G7> stream x��\[� �q�7>��i� `�=��Ł(��ȱ�X&���tH+"�w�h�7 ���.=3U=�gφHG�=���u������9�+����~���O�������ד�X��U��ׯ�������XM�W�^fE�����'�9F�D���^2�`��3OS&�E���Y�i}���[�z|wO"�C>��A�Hi)|v�:�����`�d�隭����1�͘��9�%�a|���R(�� }8F�J�q5๵Zp�8 ���F2_�i j� ��&e%����F-�B��a���� rT�h}��G�����Hۖߛ5�l?�7�=Z�|�̤Ϲ����U�GH� �gx�bJܷ�  �.`rO�ؗ��7�l8+C��� [ {���2�� �j �)�)��(�S��Epҥ�`@��|��ӥ K�J��cA�@����ڝp�b�r��YX�EJX�������)ܱ4���Z��;̏��`�f��q�dr B2� )M8�@�"��%%�zӿ�1���� Q����puw���`p1�8�IV�Hi��o�J.�yu�;�1 m��_����:�(�`�KgFT�0r�h�'�)m� U��� �d����E㜚{�яjh- k�m�G�56�a��3��"�"f�k��5�XTK�#WVv�e�4�m��m|�7��ؓ!�0i�>C�3L���0�Ō �2'�G��󟬒*l�݉� N�� GG봣"c��W^Bn\ذFID����ē�9��(�;�T��A���8�k�v�U8�����hģX^�?�R���s]�@�`�,�_��3֔6:�� ����� ��C�|�������:��p��h��]? �h�v��%�.�aNn��u��56&j���X;�r��s&�n^�6"���� >ο?Yc�=fd�y���ˡsŻ� \���j�/���g�?�UYг 6햳g�Z�H��Q��T��$�|!��,�y��a:�;%���SH�K~��6.��3 �J������I�M�XmzZ+�Z2��eh�Æ���|yh$� �-�Uh�ǣت ��v�9J��e���٧�F=��U�Bxԅ���p��B B�~�"c}"v�g��_mV�`kńFu��Xp �B+V���:�V���c��y+�Y9���ŦA�/{p��2Gp��6�2�*��M*��nW��B�1rx�Y�yp$���G;��H� .�!y���ۏ�%�Py�3��x�� ��Єr��y� ����G��9[`[`=����U�R/Ժ�yv��k xҀ�� 3S�k:����)tC� �28�O�b����&(�;1"0AU���71�Nd��{�U_4�`'�9H�lv��aX� ���9��P�� a�v� (AyF�j�NO#x-EϷ��Țl�;� ����c� ��i)`pIy��@���' ���l�iLv���j.AR2@�d�@�Ӳ�X���A,R֍�IJ���L�O����Z�� �C� @x0,������nD5c�N�� ��L�PR��3 ڥ�=Đ)���Y�;Y>x ���d�a��҅��g���j&}= G(� Z F �b:5 ����ͬڤ�� _Gp��쳵�l"�aP��kL���>�Tvq g�''�V�ك| �69Z�೔*it޵��Xaᰰ WO, z��i0 ?*n;�e#�=2 ۨ���+]G�%Ɋ�)�oa x{���q>��a����. D$ K^�#�Š�e�G��[��� ���߱G՚���k����Z��7"g��$ELpR���5�6�78 ������f��w�K��_ ��!UΤF�w��M�ޮ��1�E3 R  ׁ�#ܯ�����x�߷0��F��[N q�bw$,�g*��p��l�_Uj���B��[\y�H���D�~ � ;���,��09A�1�(j� ��= n�p ]Q��$�ˬ6� $�%c� �� �7Em���x�bbw��Θ������voY��U_�e�G� �+ӺW�}wX ���&ټ��2����)�A�o/6� J,#��%?u����F��%��V��D&�.K3!8.�G�;M�\*� &���h��Wf̷Ni�֋u���ų ����ka���)���}���0Q�آ[䅣T�:��mw@��;����[�i ��V�4[?���\Kn\�: ���ek)g��2is{����N�O�\� ���/-M?�J%֕#� ���Hf�[�o��\�" 8P�A��1yM�k��&� �1��80K�W�V��!E�{�u#&�zP�vt�4� r�k � h�X��k���l���up���+x�^�f�E���̳ќ���a ����tuq��l�l�� �v�v`t��@� ��m����1�M]g���?�%�v)�Fp �a��Y] ��:7�P��zwX� '1!So�Ȃw�d�u���%l���9#N���>�����xFl2�g|�V& ny|�:/����Γ̸g�+yQ���}g� �Ka��&��Kv ,�O\�2(��g �;��6�s�t# �X\r�� 5�VP�E�@��l��*� C�f��h�s�m�2qm���� �2�Y�2��� �Bwx'-�]� ��ث1�ܳ�hG7�ÙN;:�8W'�s���4� �fpCu�o{�{Ň�&HN�V�]��a��U"�9ğ�s+�R^}ᲄ#�Oe � .K8L��z�j����ap�,/XJ+�/0� |����RZ�5�R��Qp���zj{,��ً�[�/��6`)��n�K$K�8v w.�o��t/NS�%��]�1�`2��ݶ�`0S5��߮�e�� tS^ �t hKaQ:[C�W� ��Z�-b�mA� ��:%i��ل��8��Bη8��~�&� 0��)���\�뇘�N�F�Ӯ������c�]�⠥P:b�� ��@�g�^U O�6�mZN�1؆���aIVJ��8۞�6M쯝}h��+t� �8  ����@��JF�g�V8j� _J�͜Biޜ\VWWweo��i��� &�L�*^��ᜪ��h:�jj����Tb"a �w=��q�Q�݋ˠ�ޛ��e���G\�u�o�X�G\�:s�Q�0�֧쮮��XS��v:S\m �1zO��;wL�6�/���R�� M��q ��H���?r=xJѩ�0/�JT�3˝�TrW��i���!"^�}�#d E7��7 [E"~��umL����O=�?���j��"5����eEh��|���w����j�>�A3]?L�� �b���� ����S�?���i�@�r6�2��כ�;2�N�tC>��]$-�&�``ha�>�� ��F-����R&������G���H �����ȝ��-��wz�(����D�ž������� ?�w�TQ�>���EH���S�sسx��o���/�)��cM��lo����m� ��� %D����υ-`�@]��A�9�^ŉg�;i��K��wb�ˈ�鎘Q����=6�.躻�.�x>��)R1��C�������a�J�/8w��Ѭ!P/�,����NmѾf{e��[�զ�����l40�{a@�Y�U�!�e��l�/��E��/t��� }�g�� � 8�i��ŝ~Koq\˰3���`c��3Lsa�ҞB�\�� ��!Q�-㷺�) ꋠ��E�ěg˯�Ə���'�:� �VZ���}#Ŗ��i�Xj$�� �������!�M;�[,�(?��K�i�5�^DO�x�Q�7��vO��üݖ��ϷL0v��+�������E�O�>� g� �?��������t�I�\���BflP�P[���˄o�]D@1�O�J�?�|��� &[��~����d �]� G�. ��w��E7�;��˥��\�k @6E����Ӗ�id ħ���� �V��endstream endobj 250 0 obj 6040 endobj 261 0 obj > stream x��\K�$�Q���~�N�aͲ`:��!$ B�vfZ�l�,l�L�L���G�w��#2�#2��恰9LYvd���?>D|P�>(�W����_hux��� ڜ�2x�����7�G ��_ ~vG��!�9�w�ܔi���0f�фQc���7���x�9�Uv?���e������;&g�_ �h| q��G!�0����!�~{��q�Q��'0��0�=O��d��ȯ1�JQ[�������4�l ��Gc��J( ×rz���S���$��>k� ���SXdlF��� �M�v�7���� ���Z�Ye-9�R SJ-�5���� �H?]�F/ �@�r:� ��^o|Y�3 !�1a���wǜǠ� �iy�k����[gT @�v)7��K~$�� 2ߛ��1��R���HL#$���ߴ��&���f�u��#K��ax:�"��^Y3^[ ��? M�v@���+7�9�b��d��RM{�uچҧw�>�ߟ}�=��͋�=�4a�~B0���6B o� gJS+��q`��} n�d�����o�Q��T�G4:G�� �S �s��4$�!�¤8�N (�9!D١H��� ��I�9@���x�i�T�!G�r�� k��˃(�B.0ӓ��i�����s���X��k d��c�_1刳.K� ������}w�i��]�i�O�)��)���e ى� ����J0�M6��Ÿ�)ڢva�64g:,�  r�)/� ��f��r���tۼ�,�rR9ş���`�8��`tyE�nl*� 등�/KVG�'�x�L ���� |(�"�����Zʺo�D9$+�ck ,�) u̩Ѳ����QM�nK��?;�C�U�R*�7�"��y����WJ7\�5��H޾*a�'��ɵo!Vu �T ��"kZ��p�[Zl�l}��������F%�b^SH�iM��8���]51�����` �jW���FFD����B��߾EY�hWʊ�� j� Л�h�I�����xqr���v5�N�m������g ݛ pc��it1Вd�g�#DM���� bn���B9)e���]�p��5�c+�wqfq �;i�&W� �{�̌Uf���3���#�� = M��wҔf��Yvܴ���>�h���,3��2l�N�T� ���[ Ꮣ�� ���^�_S�C�a�\M?� ��H��b&ⲓ,�J��SF��,Z �Ꮵ�����i�9Èg�dk��������I ���B^ ���;��]�� ��Օ9�٫+s[�oj��BoMU�ʯ�����N[߳W��j��j޼_K�;r��(�=���*���ϕ�zS�� �Y�-T��iҚ���EW�.6ZF�M EN]�',� ��b�w�Xe=4C*���Bׅa 2�� }6��4O}}�Z�JSK���1����Ϩ}��A�/�� ��%���J�)gy��!^��������d�\ a��=-�nv�M����X ��U]� �u�^� �����@8Wc/q߷kp˥�5� nJ��X�,�70� ;.�H��3�� D1���� p����'�$׸.�� L�S�)y�t'š�h� ��6��� ��.խb��Z�V�L�M4�v.r�nv� �)�R�ŵ���y��\/��^jOA��ݝ�H�֑~eQ�k�@m:����Y6����h@������8�y@B���'i�5g73�� ��i�@�$�I`� +g'�� 2;���ӄ�l>#F碄D8ЊF'�g�&�3�>I�ʞ����� [�����>C8���&��d���x�6��4]�S�� Z��y�fzJ6+s�'x���e�D�܅S|B�M� ���|ɻ�t-�+n��w��N�DE�A���y���I�l+=)[t=R�j�����ݤy����0�R�\K��UD���#/�#�iRae���� 5�g���Ժ�O�'l�cu�)� ^R�����jOT��f��|�7�W�} k�V�bc�1���N��]�mxjm��]���૸[T�����ޜt.�� �/'�H ����樋w�3�˕��*�޺���;�� (���+A->�u)x;G@���Cy���p�>%f� ��!��� �25��y� ;ovj�0�=�'�3Wl2v�ŵ�:�o�}qT|� |z ���� �Y�Y~�Rܨ,i_nɲ3�몧L}jȔ�i��U&�-�R�pe���К��WU4�W�����~[�H6x]`�� ���)��Dłb���@\��Q� �>��]w���� �9���8����������2(Sw���B����� "�;��A2l�_� O�:\�)�Un�R�K;��e0yO̖=����������"e��xɩ͢' �Z��G��ޕI�.gSL��4{;>�k���j3�\#�ݎq:�����u���g�����ߦ`���s?u+/�ԭ��uZ8��.m��Ң �K69ָ�ث8�bϸ��G���$i�:���� Lz-�J$}_��:���������We dK ����ʀ��K8�&Bb]�dg�-e�8�@������\�T�U�6;�d;�� ��A(�i�]�f(���72Ю�eg����h:At_~����[�����Z>�7G04mx��҇���|5������� �0\-reJ��Wv07U7�/k�y�Y�4���޺F�XE��������� �S�! �S�9Bp�Ym/!in���ޱb��U蘀�`eܸ ,���;v��B�����|7Psof�C�� �O��H����3�w��v)y�ԃ�|2-Wgq^�R�"q}�^Фk�`��&:��9��վ}q�=L|Q5"3Sa��� ��d���n vz3>-NY]���� �mo�xM�� �gyY\�"/����H�&�~+�]g�s#ori k��i*/7������� �����]$U�P�^ ���� ]{;��> stream x��\[o%�q�7>�7�%� @?]���4����7 .~��5�7�?]X��e�ϛ��~�#,�9VS�� .�Sxe�1{�]:Z�._}���� �чj� >F�¿����ׯ.~��Z+ 6��g�+ǚ�/r>��1���� & !Ő���p��7~� �)�����:]��g���i���ʹp,.�З&�X���z� �� ����B��o�єl���T|zb�������^��˅7� � ф� �ϻ��iN���iRQ�&�.���l/���w]��yxl������ �u���{+0�� CvÝ�� q�l��?+��g�����l�� ]���7�S�u�J�b*0�/�[~8�4���5ɥ0�az^+�K� c͙�/8��4�[mK;�P����AMx� �`��i�M��t�W�d�s�����a�uc'�1>۶RCƍ�a�-h�)�hӱ�*A[��7���iam�f��5Gr�Ǥ#lw�@��CɮP�;V�R\�Ee`��Ҩ�J�C��#/$��>$�b��j �ļ��^~H�[�o��[�� �y��Ts ˒��boj��@�$,ͼ���-q�� ܴ �n��1Iﳒ����Sr:�Qa��4bO� ���z^ H[�g��k�����` ��k'N��G(d%��m ��=�$� g��O:R��[��^���t:L�=���-h �i���IRo�{�TM��mk��A�s7��ˈ-������1���z9��wdt0ap�1w'���� Y�𾳁� l�:�N� hI��΂:��hb�l�^4��$��Om� �T��=�F;��Ɓi�`�Wi��)���=��К�c��~!�f;e��Eń��?�x�l1��`Xs�Q�ץn�jC�ˈ7��� WxF�r�= �� %�0�`١�> � �S*(q��єO7��&�6:-�z0U���31��9c(� ��'��&H����饈l���ݝx8lC����kx�(N�����b�ǿ��'��;��4�Y*%��@�%׀膀�^�_��#�l�#��M� �o�%�;�4��0[�5'�NS$U7��D�s�B�9u[��~I>�k��" �r;k1��)� �N/�l%%g0? M���R���"��A����%k� �H���X7�t��Y�+�(�� ��Yy�Ȣ>[2ʌ �s ��&Um�lw�A�\!|���J��x;�Ά�P��x S�J'^��ٌ���L��ƑjPS�X��Z�I>#�����/O�a+M�l��=�*�(W�F�|� �^M�k���"��#g�d��CP{�-nqUj}�A�j﫭��&'+��%��]��,eD���{�4�>{�c���*��O��������� '�uu���"�Ki,0T��t��P�ɘ����Fu� �6@H$�)���l��X�����?5A!~�h�����j1`6l��4N�hi\+n s�W��#3� �4�L���Y,����� �!� �Q�R��t�-������.�Xj�B�X��=�_!���2�8�� �K�{n���и*��4���-��46ծ>K�K��X�ރ�EV���=�k � �d�txp���F�P�z�.J� �@2D�8;�NM%r��jZ��.ObTJ�� �}l�K��pe3�� �m�)e 9���hH I�� �-�N����i�.w�^W��l�ن � ܐ%:�0�R 7j�D�ޔ��al>����Z>��%�1t����#jr�~"eb���* B���}���� ���@]�^ � �bM\ٟ�.�p'+ *�e��s �;��{� t���Rs,�AM�Ѓ��c|S��Z��lv �_���kU (� U�p�:T��8��ܽ�|-���{�յ������t%q5�g�̶�|?�o�O����Q�~/����p�"&:d�v�vS���� 8l�15 /W4� o��-�����ާ|�잱���Q�Axc F�L'���~�+d�E�Tc�N����B��2��֬s�v  �r"��`��D��bH�^�:C��aR̴���#��L�drG�W��Sbf#��cϩG �]�Z7��f�y2+��R�u�f�{�&*U.Y�F8 ) =jL��}�α ��\���)s y��w%�J�K6��o�T8\�rQ#ϗR�߉'�DhQv {��t���Jv/�v#̹hy�4eڔ��1�E� #d+d��Ί����UJFLw@�?�=R������o{�oF ��� Jt�:xP�V�=�k���ub*�t� �t�%�`v�}���"/%��ȴ9�.~�� $����h���  $7��D�v�����2��1@"b�F G̜�׽iD� rZ���Tқ ���� ��?�إy̮Y � /�K��]8 1�߱b�' �b�|~+����v�X���ԫ�D[�a���6�MH'��� ���ֱ Z?��*�g \C��~NU�[[˙m ��[AV��Z�U��M�@r��]�@"`�JqK�2��DVpC�D�:�g1dQ/}D� �as�e�6^�vĄd_���p6X`�� ��2ݢ52 � ?E�)6�?-s�i>� ������ \�9a�BX�?��˖�p"Q��2����P�����V�3s��$��� pP�m���h�:a�R�����#w�Z[���un�ж�&*x.&זmu��j��VW�rx��M��M��� ���o��W"U��Q ��}�� y� �E^L ��w�` L��Ɍ'��/i�,�>�gnX��%�c s' Kqu'k���Db�I���޽k$�'�n�Pګ� �X3 �����ͪo��O�uT�ԍ�[�l0SJ�8���x�ⅷ?���b��B X���]4��E�m��[�n�M]LLwI���%Uf`�&g��&�i�s��!ެ;P׾Y�d�{q���}SX� �j���-��tZ��Tã�A_�|� ���y�v_?�ŢFqu�i+�/+��[}� /h��2*�;���o��7�ӱ� �� � �5�6�؟ �TOI{u릺�N�?�3Ǡ�QW ��ֹS���h W��W�؊����[m&����g�RN��F;CO��T������N�Ҋ�E�Vss� ]�.�޸���1�������r���З4��� ��j�6,���;�� �/ �W�l TYW6��v���k�l\����ywτ�����Tv���p�l-�����]��i�^���bm�h�0�+�|+��L�ɗL��O� ��6���~���~�����n���n�k6o�TtΖ�rxwȂ� ��F]�" � �vϦ��-_6!�ꌠ�`�P���}��;�Bs�;� �y�)��R�L���[�x�ې���jk���y�,���M3  ^�~�MrKՅ�-��B�HY����m��n��[�� �O��y����sW �d������y*�!x����]�]��h��\�57]EL���"��o� ~�����TZZ 1���o����\J��ɵ9�?�!a M�;��]��h�T�Um=�U�^w�����zo��w㡞�/m-��qfy��)w`^wOfe�-q'��#���/����Ӧʶ�~ z�X8v�KA?��������r�p6�W?�[u��A���g��AQ�c� ����`|��w�qY�'��,,���4�n�����z1=�ny,�u>�� 8xվ��O}�U pM�/�*��P�ʍ⟣��ݹs0�:�� �QEӴ$�f�L�ݨ�xS����}�9o����+�pendstream endobj 272 0 obj 5363 endobj 278 0 obj > stream x��\ێ �q�8_1/��N)�6`Y�+ʲ,qiȀ�j��]//�dk�����DDVud]�C�~1�������Ȉ'"3���������޾���k�_���F_k5�o_\}e��u�����=�����5%_��>��a쵳i�1\g�F�!������ 73{��1�6��:��7���7|u0��%���M��4����C �;�� Ls����5� ۇY�P᳭Ӈ���H�w�������9�P5�0rR`*�s���mI�lݪ;�&pk awR�� ���pH�a=,�k��ʊ�p&&�g`����� @�bVv���3 �^O���F�xn`�t��N�I)�7�%�л�Q�M�ݧ�� �4(��h��m�#�v�-a@�:���[ =�R� �3 � ��X�Y-�I�B��v� (�yN�y�iG�����}� df� M��a��_��k68 C�� �1�|��9[;�F���Y�l�3P��M�{�AQ�SO~j�T��6��0Y���.��MY��g�ԛ���nm鹖Y%岡Ӏ�٥== P�fbE :*�; Y M�]�r��C6�N$��b� j9��٭b�D8������m��=�d��#�=M���ȝ�Ķ��|�W wV0��-Õ� ���?t �����@�OGyW�׸�VT���;�6Ç;'i� � � 7� �w.��f���Sr�0��o �&�h ���@�N�]� ��WT�s�k~.�)��^ӆx���c��.�׻�c�4��o�o ���㏒��3�E+�� ���&���i�n�F�|��?���zݭ��s>�=�C2AQRD(�Ȯ�5�%ߍ� �Θ���tT�zZ8�7��{�����uR "#�Z�� ��ؙ��� q&yZ����f�Zx�t}�ѹ��rEk�y��N�氩G�1t�@��\^����� �3nNA�Z�PN����E⩩AfՕ��fI��k��qZhuK��fp�u�G;���z�N���Q=���*�ж�D� (�}���7Ҝ�bJW����3�2� �Y ��E"�F�m�^��Dj2�����T!���X_�Ǹ��i�Ocr]��BEc=�ڵ�Qca1�O*ҷ��D��@��3��U��p ��^n��w�M�K�ZS���J����,��Vr����6CP7W&� 6���ˤ�][S�;�t ��kR��C��,(��EG���'ҿ[.X8Ұs>ׄ T�Y����/�{G/C��;�7��֕��2 ���0�����ɧ��h�IR>�VK�A��� ��� b��D��2 9]�i��'[_��1am�ٰS=�vE�ɽ7���'h���b D�J$����@���f�G�d�( q��I��X��U%��� F/h���� �zS�Y�e�֊2��� �2 ���M�0�>�����N���(-����;� � �gU� ����C���@�Gk� #�W����#l�����V|k]]\�^խ�� Ю�r�z֤���Q������f$�_p.9���.83����%QW)�gy�l����m����1��LX��l��^!� ��I�>@C5 �8\���R1.��U(��M�ɔ�_F�+��` �r�G� �����դ��4`,oi���Ӵe6o\�iw?gr6]���9m?tY�P�e����M��,fK>�D�00�C�.�ht�z.��0]��9� CeL-��G���_S�>$����@o��5,٢�gaOϠ �j �tk�P���4��*隄��꾯o�e �}߉#�T}z�S�J��¦�؊���ǃ����8��� �bfw� ڥ�g� �3�&2�����&@0�X�#J�1V���h�����p���b�ЍM�7��7���j������s�?�丸Cg#��R&�V�`�錁 @�\l��� �y�JN�.25kr�?g�0x�L�VwW��;:- y16>�Vf7�7�{��87�`�:J�j��I���g���kN�qyzc�fF\�6�� Ɯ�_ړL�%�+E7]/��^�vS�o�6�x� �Q�� 3Lիv� h4�Uc�� _���NJ�����\ �QC�G�����@�����x6RV�k#���_�Cf��Z0"x��J�q�u^�%˱jmA���>�5�[�\�o��G29|fM�X�"�l,���w��Nf�����>�ٛ _:�T@c$|�(ÕHH�� sΏ�\D�D�4 $7cR�t�%%�����5Qd�Ѧ Nj�{�Folߪf��xc=��z)��$�t��d�S �{��j�&GD�G���������$�+��F���p�D�6�J �Ս��il�+P��T��@��Go��J� �Q��=� �K(,ph�J3dd������ [���{9�6U��0���� � е#�eEF7 ��[s� 3}8`�UP��5���'J��b�(��$�HVgzd����ۇ��d��_��> stream x��\ْ �u���|ż�:]�}�"i �(��X ���LO���p4 B�A��� �sofUݬ��&#Ț��\�z��K��KE��� ^\��+�.�yu��\��B�o/��/.{E#4��Ye}y���|�WZ�!��hB����zq� �٩޺���=ze�^}~���ŗ���9c���?�^�s6��L }P�]�c,�\�.v��h����*�}E�N%gb�z��x��O���u���Ƹ>�=�/��>w��������:�s�G���dl��ƪdBw�/�M�6+� _��Rt!u7�;�{orj^�����g��� ��Τ>�`L$c�R�;~�?xP2ww��LJg���-�{���M���A^���-�� ����{�#��E~��ŧ�|Ro!R�ַ)� �ck�"�nc�g4:C �0K���J�21 ��g;��R�@ 0(j�}����5}c�m�Yِ���t`lұ��PFa�';��Lw�kN����rƗ7� ,�  �_@r�� M��}�6��=�5��;�qߧ��o� �%>듇�]�B�n������ �J �>�.'�%y�0�ꀘrC� . +_,��v������˽}t�ʹ��;�`3���������f?�� ����at˪cQk��zZ���l3��ȍ�Zl���Cs"�dCo�I�x� yL�0sC氊���m �O(,� ��1�Q���� %�W���{2��GQ3a�PÀ�a%�v���PK;b�#o��I�jGW�M�� 0}4�ĉHp��x ���q�K��C�� ���� �@�ՠ�9���.�I��-�� ŗ/˺ ^�_�2a��j�IS��4���:��stil��b}{�[��{��{3P��yA��z (�Jd3�������4���c6k�i(�P9�Q�( 6C�� �ʍ��_���%���?��.��uH��� 8~c��7�/��% ���>%����> �٬  �=&m����o$�h���?̫�>��A�����_�'� �~mX�'��S��#Xa�t�)� �2���ā �6k��XoUC��Cȍ��1���l (l����`�`[OFꂓTeW ���>HA��Ƒ�p�g|8�B�� /����lc� x `��� Y]@����[j���Me�,�/��83�;���@�]����^�W .���Bݬ|C��bF:��=�$̻%dr�K�a�M��1�8Tc��#*Ճm0 !����2܁s_��¢u��%p�!d�x|�$�H ; bxU���­ ˲X� ����^Ez��+)CH� T���|���9�F��4�MF�P�XC�ͺΓ� �?""� �b{E˃N���!ӠH55d=@�,�:���x�����& ���i����� E7ϼB��VU�{ha9Y1� ��˵� F R��`� (�t��"3WQ�e�2L�;��Y��ȱ���D�L���= y�=L(��B��¾W��s:��T���}] ���0�(b [r|��{ )3+&�� ��SX��(T�2��`�Ӥ�>a�h:��c��{7DFx�(i�F�E�+#����ݡf� �퍍 �}����4�� ��cW��ѻ�4%�A�gc8i�!��Gi�|�%G�|:���E$�;c�� g|�}D(�ϻ}��֐:R^,'��ݦ} x��{�~Lk�� ��a#��Qڕp��sD E�C���?&� L�\rp���� ���9�fG¯��MҦ��̂�5�@�>pfD��N���d��7L}����nd���Q�`�� A��T:�P!�>wͧ�7�f�#�J�n�5ʟ���@i7Cz@�\��:Y8#�.'U�V�M J�s,�@� `�)zlS�B4W`��|�A��������Q �c|O� [ |�� ���yN�,/8�HF Y�2�[�d�uj��>�0: �}Oi�-������4��G0K��ۇ���m� �Y��˽���DP=����c3 O�v,)����Z���]��3��� �(�mX-��3�r�Ϟ�m4'��VDe�&�Mu�l�Զ�B�b��Q���x`5��f�IlO� �ߗ�>�L�2�Q,e@�%%��T���"�xrE��qE7X��RN+��eM9�A��1#L���r���R��`���*��f Yb!dT�`7+�v�ԋ��U�N�xs�UY�2[ �*'�!��,�ш�CA�۲0��FMJb̷���g�[.꠫ۨ){ʆ�R�){�_b��?s�pb2d�Z�,�(Ę������s�#n��fp шz�7�j�� 0VQ���S?ߖ]&���(�gJIκ��H��x �ỻ�������*�"�)�i�-�C�ի&4��H���0��,��#.���¸�;/L�K`�8M%*�w[!�F��3��|�%�Y� �t�~}K�?W_i)A;�J�d��/���9�)��Ѣ�|L���#�~��b����;l(��K�@�f -��w��ܦ��˒ӊ�^X�f����,j���"��� 3C����a�جr�F ���v�dP>o#�GHY��0�� %�O�Yi5 M�~ C�>Т渡T\�M1l�FA��2�kYm��UZZ�㛍L�� �" e p�Ӈ���Z� ��Ә:���j �5���.jm ���w� .E u �0�6�L9`N���@��� �6-�t��m���������A]#�ҫ��(��آ��?�14���,�Ě��Zh]�k�A�K�����q�������**� @`���t��5BOj=�M:��"+&R�H�i"��Ǆ�s��׆ h����=Ʋ�x|}f���b%_5��U�"��E�vn+����:�5�⠲L��XĬ�W�B u�jو�!3M-�����`��p�Ǜ xl敚��f4��Et"�`l⡓}s�f�� �Yy�Ū�g&���,�O;��L5�O��4jB�,F/S��įE^�A�p�6�_ �N+�� K;,Ӫ�ʼ���*��F]�m� � �o��!�4d�#[CVzLV��!+C�oG�z�1� �5xj~\�>i�s�#X(��`4Y��� :�W�(�o�|�m���R�x�@�� �mg�+����N}�$U}�R:�o��+���� �-�G񜚐s�z�����5�փT��QT�(5y*�mDg3X��in!!�����9�)lj����鶃�Qi#�?g�fhx�����T;Y}� ����K�&�{�Ch���Q ǲ��oR���h-�h�-�[�3� 2F)��,*\̔�; ur ���M�^�_�}ҰS=�Ìkb �l}L+]e q{8����|BJ�Ϫ�P1��p����,O%;!��G��Τ�B3݆f.�q�q�፺��,+�ܕ�su�����`�p�Ή!���P �ߊ1N�8�� �!;��M��������طFӽ Z4��.�2��C�)��ݺ��8��k���M���8�ɤm��ۜdɃw7�h��z#�f�Ҝ~2~��C���no��Ɔ��{n� s, ����v5�4C�K������ {�[Uhjj�ks2�F������~Ț]�� �7��q�\  s��Z�qo(O�2�r�2d ����=�|;Ԓ� ��V�L���r)0/z�I�Y�N��"����ͫ�j�VoJ�����Z�Bl�O=���N���"��~�i�8Z2l��Pbq�GQ��3��h�Og�h����j��C�����rt�xr˸���E��%�#�=@��Y���� �S�� �x���+ASh*6�c��Y7��H�/8���7���|�%�Ja݆�n� �4�����(�l�� ����B�^�N_��I+y ]:"�6�����z6��h�" �����aD���h��V!���&/ç�sL� XC)������N�)��;� x�ŉ�>�+��~Z;��8��Z����ʼ_����$�;2��@� g�G�  �V�⯒��I�o�9��BZG��.5K��^D��W �k�M������@��,CqwGŃXJ��K�4��F���]�m�z�]�O�����M��endstream endobj 290 0 obj 5445 endobj 312 0 obj > stream x��Zے � ���~��7��Ѥ��m����*A�=����L���G�� o�� {�j� �ۏ?�p���ϋ��>�8� �?�qRCR���mLC��������BG�hi�6��r���ód��|�8�� ����� �� ��'!��R Rq�)���B,�\����a�A#������I)�� 6 M�>�������i������a�� d��A���xD~iV����v���n����p�sv(i��K�����T�F��HcH�ȴ��Խ�BlPƶ���)#�U�Rϲ�mHl';���" �hC�1}n|t8H�X����G����L̲�I���z��w"�eꈗ_戉^[��X�=V�R���A�1��j�u��jg0IY)p�N e�8�AC����ÈC�4@AD`��ò}�2�e/��uV*R$ש�)����n��� �y�b��9��Q��M�u"���p�S�(`F��@ʻ��_T� �b1���}��P�>)y`�w}�B��r��ث;�c�bp�8 �7��J���TC$�d�!N� �_�W�@���V��1� y��3L@�d���[��+벋!=je��{2��K�a��v ��� .U�3R3�܅� ��鄵3��m�S��& � ���h��-.��@eS����� ޯ%~qիl�w���S�i�s��na�ο��TJ �W��׋���:��Q^�m��))C䎬�4 ��>��ϩ#�d):#�4��� ��������)2� j���FC����a��d�W���*�q���\�yZ!���4�;ɽ�8 �b��ÜX:�; ������=��J7 �4 �g �ӏ�v_�G�j��"������mT���Cm�O��km �-L߁�ΥҶ|��}� A�? ��6�1r/}�6�2�E��9�n�D=(d��kʉs߾u�L�%�#�5� ]���t��ć��-! | ����j=��/'��!no�%�4`> �0NB��p�dL������(�:6��T h��]���&m>w��~l�=*Z՟z29 Gڣu���DԡT/.N_��Y�Rb4�s��S �Q��M_m�3� }���{�\���$#P꫍�!�%��ά�5p���8��uto2ϩS�i�f*�]b@�]�Qo �Ќ!��^ iwE4�e��yq�c��N �%�`�d>�f�S�@h5�da��m�� � 1a\)ieQ �ca�� 9p�s!L�uy���ڱo a�y&7 /w���,��c*�}z��W��:DFB���|b�!A ��3U ��#x$/��ѹ1Rl�q� YOqh����@S��#��B��5�V� +�|>�]���C�Yt��0JЀk9������0��+���f3�2p �%8r�$h���D��n�Z�v�ޱ�-�n&3'3=H����,vL�.Ł_n�R�o*Y�dB��{�Ŗ~��z��Tk��� �BVˊU�k��F�c]��?lR�� ����5K�u�0�g1ɭ޳�)�&��,ay��tb�q׋���I�9#�a��3Z�a(7�u~QZ ��!祉�"!�|�҂jp�r�2 ]�ß�zO|g:��ḡ�Rl/f+�@�L��%�J���R'l���$z}�y%� �R|�V 6����h5׃�x �ˉ��Fѧ�7���fc��)3c퓌+�yn\Q,f��@rl��v ����r8�_�ݳ������Y=��g�>E��ˋ�2 op|��Wu�#����AֱP����NtH�\��ƭKY���W� \��/��{��C�A�|aN�K!�HIX.�f�*A�Y�Qnc�;�� 1�#�4 oѮ�E�C���c�(���뿱Q�[���5�sд�Od���+ ��X�Tټ)�l������_�@5�`k@'�@�it#��Q�⪎� �]W̽�M�m�:��|[������gXx���3��q�5 �yέ�}�  �[�CL[C��u�2 �=����l��,ֳ����bz�f���C3��u�a�߁)���[���$�{Zg�����n/�u����q�XXe��Y�-xl>D�/�6�*�&֟˩�ޠ�:��S���*w19|�SǺ!�em���^�F��+�6� n��O%��+�k͟f4~����%���gv �;޼���Kҕ \�9�ٸ�9����C�ebj�+��u R�|�#i�u)�� � *�`�Y��ϡq� Z��M���)G�a�9|��=:?G7�J��} ��I)3]��6�*������'�p�d���(�|��xҌ��gF#�š��m5�,M���� ���%�W+�;��)��G�W����G� �P��elݖ~�?��> /Contents 8 0 R >> endobj 17 0 obj > /Annots[22 0 R 23 0 R 24 0 R 25 0 R 26 0 R 27 0 R 28 0 R 29 0 R 30 0 R 31 0 R 32 0 R 33 0 R]/Contents 19 0 R >> endobj 37 0 obj > /Annots[51 0 R 52 0 R 53 0 R 54 0 R 55 0 R 56 0 R 57 0 R 58 0 R 59 0 R 60 0 R 61 0 R 62 0 R 63 0 R 64 0 R 65 0 R 66 0 R 67 0 R]/Contents 39 0 R >> endobj 70 0 obj > /Annots[74 0 R 75 0 R 76 0 R 77 0 R 78 0 R 79 0 R 80 0 R 81 0 R 82 0 R 83 0 R 84 0 R 85 0 R 86 0 R 87 0 R 88 0 R 89 0 R 90 0 R 91 0 R 92 0 R 93 0 R 94 0 R 95 0 R 96 0 R 97 0 R 98 0 R]/Contents 72 0 R >> endobj 108 0 obj > /Contents 110 0 R >> endobj 118 0 obj > /Annots[124 0 R 130 0 R]/Contents 120 0 R >> endobj 133 0 obj > /Annots[144 0 R]/Contents 135 0 R >> endobj 147 0 obj > /Annots[151 0 R 152 0 R 155 0 R]/Contents 149 0 R >> endobj 158 0 obj > /Annots[166 0 R 167 0 R]/Contents 160 0 R >> endobj 170 0 obj > /Annots[177 0 R]/Contents 172 0 R >> endobj 181 0 obj > /Annots[190 0 R 191 0 R 192 0 R]/Contents 183 0 R >> endobj 195 0 obj > /Contents 197 0 R >> endobj 203 0 obj > /Annots[210 0 R]/Contents 205 0 R >> endobj 213 0 obj > /Annots[217 0 R 220 0 R 221 0 R 222 0 R]/Contents 215 0 R >> endobj 228 0 obj > /Annots[232 0 R 233 0 R 235 0 R]/Contents 230 0 R >> endobj 238 0 obj > /Annots[243 0 R]/Contents 240 0 R >> endobj 247 0 obj > /Annots[251 0 R 252 0 R 254 0 R 255 0 R 256 0 R]/Contents 249 0 R >> endobj 259 0 obj > /Annots[264 0 R 265 0 R 266 0 R]/Contents 261 0 R >> endobj 269 0 obj > /Contents 271 0 R >> endobj 276 0 obj > /Contents 278 0 R >> endobj 287 0 obj > /Annots[306 0 R 307 0 R]/Contents 289 0 R >> endobj 310 0 obj > /Annots[317 0 R 318 0 R]/Contents 312 0 R >> endobj 3 0 obj > endobj 1 0 obj > >> /OpenAction [4 0 R /Fit] /PageMode/UseNone /PageLabels> 1 >] >> /Metadata 367 0 R >> endobj 6 0 obj >endobj 7 0 obj >endobj 10 0 obj >endobj 15 0 obj > endobj 16 0 obj > endobj 18 0 obj >endobj 21 0 obj >endobj 22 0 obj >endobj 23 0 obj >endobj 24 0 obj >endobj 25 0 obj >endobj 26 0 obj >endobj 27 0 obj >endobj 28 0 obj >endobj 29 0 obj >endobj 30 0 obj >endobj 31 0 obj >endobj 32 0 obj >endobj 33 0 obj >endobj 34 0 obj >endobj 35 0 obj > endobj 36 0 obj > endobj 38 0 obj >endobj 41 0 obj >endobj 44 0 obj >endobj 49 0 obj >endobj 50 0 obj >endobj 51 0 obj >endobj 52 0 obj >endobj 53 0 obj >endobj 54 0 obj >endobj 55 0 obj >endobj 56 0 obj >endobj 57 0 obj >endobj 58 0 obj >endobj 59 0 obj >endobj 60 0 obj >endobj 61 0 obj >endobj 62 0 obj >endobj 63 0 obj >endobj 64 0 obj >endobj 65 0 obj >endobj 66 0 obj >endobj 67 0 obj >endobj 68 0 obj > endobj 69 0 obj > endobj 71 0 obj >endobj 74 0 obj >endobj 75 0 obj >endobj 76 0 obj >endobj 77 0 obj >endobj 78 0 obj >endobj 79 0 obj >endobj 80 0 obj >endobj 81 0 obj >endobj 82 0 obj >endobj 83 0 obj >endobj 84 0 obj >endobj 85 0 obj >endobj 86 0 obj >endobj 87 0 obj >endobj 88 0 obj >endobj 89 0 obj >endobj 90 0 obj >endobj 91 0 obj >endobj 92 0 obj >endobj 93 0 obj >endobj 94 0 obj >endobj 95 0 obj >endobj 96 0 obj >endobj 97 0 obj >endobj 98 0 obj >endobj 99 0 obj >endobj 106 0 obj > endobj 107 0 obj > endobj 109 0 obj >endobj 116 0 obj > endobj 117 0 obj > endobj 119 0 obj >endobj 122 0 obj >endobj 123 0 obj >endobj 124 0 obj >endobj 129 0 obj >endobj 130 0 obj >endobj 131 0 obj > endobj 132 0 obj > endobj 134 0 obj >endobj 137 0 obj >endobj 144 0 obj >endobj 145 0 obj > endobj 146 0 obj > endobj 148 0 obj >endobj 151 0 obj >endobj 152 0 obj >endobj 153 0 obj >endobj 154 0 obj >endobj 155 0 obj >endobj 156 0 obj > endobj 157 0 obj > endobj 159 0 obj >endobj 162 0 obj >endobj 165 0 obj >endobj 166 0 obj >endobj 167 0 obj >endobj 168 0 obj > endobj 169 0 obj > endobj 171 0 obj >endobj 176 0 obj >endobj 177 0 obj >endobj 178 0 obj >endobj 179 0 obj > endobj 180 0 obj > endobj 182 0 obj >endobj 185 0 obj >endobj 190 0 obj >endobj 191 0 obj >endobj 192 0 obj >endobj 193 0 obj > endobj 194 0 obj > endobj 196 0 obj >endobj 199 0 obj >endobj 200 0 obj >endobj 201 0 obj > endobj 202 0 obj > endobj 204 0 obj >endobj 207 0 obj >endobj 208 0 obj >endobj 209 0 obj >endobj 210 0 obj >endobj 211 0 obj > endobj 212 0 obj > endobj 214 0 obj >endobj 217 0 obj >endobj 218 0 obj >endobj 219 0 obj >endobj 220 0 obj >endobj 221 0 obj >endobj 222 0 obj >endobj 223 0 obj >endobj 224 0 obj >endobj 225 0 obj >endobj 226 0 obj > endobj 227 0 obj > endobj 229 0 obj >endobj 232 0 obj >endobj 233 0 obj >endobj 234 0 obj >endobj 235 0 obj >endobj 236 0 obj > endobj 237 0 obj > endobj 239 0 obj >endobj 242 0 obj >endobj 243 0 obj >endobj 244 0 obj >endobj 245 0 obj > endobj 246 0 obj > endobj 248 0 obj >endobj 251 0 obj >endobj 252 0 obj >endobj 253 0 obj >endobj 254 0 obj >endobj 255 0 obj >endobj 256 0 obj >endobj 257 0 obj > endobj 258 0 obj > endobj 260 0 obj >endobj 263 0 obj >endobj 264 0 obj >endobj 265 0 obj >endobj 266 0 obj >endobj 267 0 obj > endobj 268 0 obj > endobj 270 0 obj >endobj 273 0 obj >endobj 274 0 obj > endobj 275 0 obj > endobj 277 0 obj >endobj 280 0 obj >endobj 281 0 obj >endobj 282 0 obj >endobj 283 0 obj >endobj 284 0 obj >endobj 285 0 obj > endobj 286 0 obj > endobj 288 0 obj >endobj 291 0 obj >endobj 292 0 obj >endobj 293 0 obj >endobj 294 0 obj >endobj 295 0 obj >endobj 296 0 obj >endobj 297 0 obj >endobj 298 0 obj >endobj 299 0 obj >endobj 300 0 obj >endobj 301 0 obj >endobj 302 0 obj >endobj 303 0 obj >endobj 304 0 obj >endobj 305 0 obj >endobj 306 0 obj >endobj 307 0 obj >endobj 308 0 obj > endobj 309 0 obj > endobj 311 0 obj >endobj 314 0 obj >endobj 315 0 obj >endobj 316 0 obj >endobj 317 0 obj >endobj 318 0 obj >endobj 319 0 obj >endobj 320 0 obj >endobj 321 0 obj >endobj 322 0 obj >endobj 323 0 obj >endobj 324 0 obj >endobj 325 0 obj >endobj 326 0 obj > endobj 327 0 obj > endobj 138 0 obj > endobj 127 0 obj > endobj 347 0 obj > endobj 125 0 obj > endobj 13 0 obj > endobj 186 0 obj > endobj 348 0 obj >stream x�]�1� EwN� H�R;D,钡U��L��z�i��÷�l���y@�(�E� ��uh",~���C�rj�N �U�*�_Tx�м v㫚����T[�f������p�5�쬕����f��f�-����x�(�7��k�� .�DT�1��G�Y�9�����Y� ��Y5 endstream endobj 114 0 obj > endobj 349 0 obj > endobj 350 0 obj >stream x�]�=� �wN� �[)CĒ.ZUm/@������/8I��҇��s1��њȋGp��ȵ�*�� �'��eU͕�xUX�g�p������ ��r���\Jz�v8����A�Y_���Z0����I �M'H�ӕH()a�)a�)aK��5���� �a m��(vNk,�.��gOb_� _5 endstream endobj 112 0 obj > endobj 351 0 obj > endobj 352 0 obj >stream x�]O�� ���4^.�҃ƨ?@ai8��^ ������Βa> endobj 353 0 obj > endobj 11 0 obj > endobj 354 0 obj > endobj 102 0 obj > endobj 355 0 obj > endobj 100 0 obj > endobj 356 0 obj > endobj 47 0 obj > endobj 357 0 obj > endobj 358 0 obj >stream x�]O��0 �����P���. EU� ��2�D! ���  :�����g����m�_�Xց�$i�,� ��x�> endobj 359 0 obj > endobj 360 0 obj >stream x�]��n� @�|p�n�*.���U�� `*!����}�i�j��x��n> endobj 361 0 obj > endobj 174 0 obj > endobj 362 0 obj > endobj 42 0 obj > endobj 363 0 obj > endobj 163 0 obj > endobj 364 0 obj > endobj 142 0 obj > endobj 365 0 obj > endobj 140 0 obj > endobj 366 0 obj > endobj 139 0 obj > endobj 328 0 obj >stream x��UkLW���̬���0�Bv� �,*����!*��ъ� �,��"kiUb��j5�e�F���X�$��� R5�Z4Q�ڤ�>ws֦w���� ����w^��� �h� �s �sݎU�Q3]�U�� XH ����w��Y��P���'B�OA�v �@�����}�2�DDF���*�lp��w�'� k��{��g�nt�)4[��8��U��-t�:�9�6�3� 7���������&� ��tVB�9��V�OM[���p�bsx�XBF��d4�+YLld �"���D�qd���%hΞ�5�u���3 �� �����-T�+��`He�"�K(�M؇��5���E�RNK�W�Pݷ��� �/5�D�D��5� )�aH� �g�y���(����:A9h�aҖWQ&���n{������LU��V5�K�DQ�l��w��� ���j�֮�`��K����v���X,c��a�Dt�4��H�� 6�����n�ͿC�F�CD� d�8�� Al/�A7XT�0�GC� �S��P��G�'�%�]��>�wO�]m���K�����,���kS�¨n)0�'�m��h��v���C�0���3��L�� ۯ6U H�`����7:p,�kwL�r�Y�ϟ1=/��g8jx� ��  �T@�ӥ��֧������h��#�:�xBr ,;D�*`]�~{�ݺ3��_i���/מz�j>����d���a��g����#����旹W�J��X������C����0���u ;��ç� �}K�\'4���_ƙ˸��N1� C��u]�rւ�B8�)_�J]�'ĩ�\�� 3� ���B�2A��Y�W�s��#��Ձb���,�e��e�(h�/�Nr�Nۦ˔_U]�T:!��X}%$ XwH��}��ď�z0 ��'�o�a6( endstream endobj 128 0 obj > endobj 329 0 obj >stream x�]�]lSe��Y����Y�r�1d�}�Fu���� ~@7bB� ד���]���J�v}Yۍ����ւ]�2�݇� �ˌTnH�w�zO=7���x��y��y����8&.�p �hV(� ~�����e� :����r(A�8��{�� �������8�5�}�:���th8j�� ��\�XK�}�:��FF��R ��YW���uj��l��& ���mh�Z��*�T�3v����2����6�F �N}��r�)KS[p�[�Y��� m��vڨ�P�� ðr�Jͼ�ag��c�'x �(����~�Uv�l��K��� �jG��"ކ�.a�G��Q��}���  ��o�~w�����0�kʹ֚V��� ��ǃÑ q����7$F\�����I͡ޯ`���H���Y�~�Ч�����.z���RQo=�v�~p��91����a��& ��l�F�����~���͢X6k���Z��8�H���4qZ�[�FfC�`$�S`�e��[��?�=]��� �u�$�> endobj 330 0 obj >stream x�MOQHSa�����4�=U��Ҙ0�-Ia������v�&]��떍f �۾�iR�d[Ј��� Aa=�˂�e!Ĥ׾�ͷ��p�y8�| 1�q&w���}Ң ��#�(,����L � ��/�Pj�+ 8p�� �\��VG'��_�6O+u�\]v���p� E��M�ө券���ـŀ�{�)f�ߌ�R�Lgb �G�����^B��|��I��ź���ؕjW�B��p���+\ endstream endobj 14 0 obj > endobj 331 0 obj >stream x��Wy\ٖ. �*����%qE\@[���*.͢�"� *[�*� � \�B�7\��FT�m��M����.���n�w���~=��7�̼?�$u+�.��|�&��0�����K�� �\ h O�M��ø? �8㷥��Bd&@f�g�3��� �-�I���&sL�PF�E���ls.X8��^8G���g�Tjv�J����TF���@;�cx���8I��`Ul���r�$��� Q�� ?E4�Ϗf�h�@v�_t�B��`����"$B�����1�6���*�_D�=��rp� eL�::�U���x���@�mxH �b����C�ک�����*vF�lx��O��� ���BŲ�`�:r���:&�N d?hn 38���� =a��\�"2Z�v�q������ X�� Z ����+|�.�9 �u��� &��$b 1�XOl ��C����W%�GLN�*n�^� �n ���:`!��'��ϋa/��y��5r=8���� ���nD3/A��2�j�o����=����u�2�i��V��n)�ԣ` s�R��˯m}*g^vŝ��j�F�w�l� %cIG&}��6�o�'~��H�O�zj. �l���"g�v�_�h�1-�'��k�� )USzJ���Ǥ���&�(.w�� Y���V��.���ٕ�{7���o1��q)��wh~ȄR~��Iq�s�ckc����ׂ��f��@=��ʘn�v���Q(>K�����R�@���_�������C�}{�P���9OC^ �(�D�/ Yĕ!��>�zq*_�!��� w�}�u_��(o��5� `������R�jN�쐩�~�Z�+���ZiK�P��Y�5�(�(�vu��#�_�0KQ ��x��.� �� K��T��� ]H�ۡ��5q�% # C�� ����=H�ꭺQ�,-i���:�� ���L�AR��b&z���R8���/���R�O�>�_����y4p�I_sV�v�㝴p��$�ٱr��!���P��}�Wa�9�]�ߌ��.,=�/���dI=�� ���o��U�]cGF�#z��U/o�n�i���Ev� �FYC�ざdy��Dt`���gKk���O�5 )��3_D�}�k��U �_-1&9Ҙ����!B��ɸK�ͻ;n!)L������}6k�C�|�xO �xjh��9��s]�x>G-Y� Uגf���Aj�o$t���K�� �V�SX�� ����j�(��%FJ�by�v�h�,?6'*#6]��B�(17��f|�w���[����� *}�A�/ɯ�}�\�I� ��9UZKc�S��8��n!�`�$�)P�h�5�a6Xu\9�T�^&�S�$�"�����]��� ��پ]��� h���TZ?_�a����>�ET���H�y8�2��#�'���Zb���� l��f�� ޗ�� ���n����Uj�K3�����[�74W={��21x�16Fw#�l��?�"� [R��p��`��2����g��͡��/� �r��㒌ٴ��j f>�qK��g��`6�/v�+w����ixL�&��{'� ���0G����d��o��mBC!�\^p�~8��vv�ñ���E�v@#ѭ���h|I�U�Ҧ����`R�ke>�N�� �o%i#��"�FZ� �uz��Eѩ'�n�$QPZBX��~>g�y�i�JP�T�_�;!Л��:̫������ό�z��>{rs �A.�s��@�8s�C6$� 9��}����)�:��{R I l��=0I�bs���P2�;1-���_� ��8 ? �c-0��yIke��n�>I`���,�� �454f�.�X��xj�f:��ӹ��*TT��v�R7Kue�������ߜ�]�}���&A��s�t�k��9T x�����k�����P�}e���\cu3:{�X�v��W3PDe�ݻg������ߺ ��m!����5S I��jĝ�Ƃ�E��5굗i��RP�ϝ�^2�׳���K�+��dl}��������;��!�69��2��C*% � é뉵�SP�t���a�i�,�tKc/��`W`0���~�\�΍ؔ�懤8�,mj��5��߾�^� ���ͱŔ��]1�Ue�u�de9q� ��j�?qٳ ��|���d9�Y�;$�7�K�5��o`��` _\�n��Mnۃ +e�� �jDm���v��7�y�{��IXr�=$?H1//���V ãON�ç�_��Rܑꢢ�> endobj 332 0 obj >stream x���{PS� ��%$��eY�A4�nwW+���m +���RQ��&0 ��$��CB !��4B�  ��Etѵ�,��R�kk�vv;��s����eۙ����y���;���|q,�ۊ� {[�Ea��2 Ǣ,X8s�CxnXx���b��,�{ ;�}��� �|z��X�^�>��c�CB9 �8�2�|����H��9Х��r���_�-U��KU�h��U�ȋ.M��%�dJғ���0JG��Y���p߁$��b7�%�E��h2����-�G�cN¥�;�C��.~�[^������� ��&�g�|� N3���k�P C�a j� ]&O��b��ng�����Z"dqFނ��g3ٍ�*f�f[u���g��.�y �:/��H��a��Y���銪�̗^�B��� ��kL�5H2KO0$� $b�*5�VS��Z�:Ϛ��,��6��j�L������QF�F�߯f�Q_� .@�� ��D�E�S�D��Fo���B֭���e� QWW��Q�'\`4V�je�AȄ��p1 ��p��k�G��� J�����up;���l���;$㪐1h�K� N�}C�,(�9њ2@�J ��-J?� _>��A>z �qݏ/�N� ��6m��qx�",�s��$�̴Xt����$@gW:��������$nLj���*.Q(+�5�1 �I�@��*�� !yd&�գ�m���Pgs���́=u�{��f+q�s����^�S�b��^���l� x�� �a�� ���� �e�c�M@��*�;��4boI�����_� �%�N�{��M->�o�w���Ti��r��G� �z�> �|Q��A��ˣ�ѩ�۹ N�Jy��B k �:u=�� � bs@��@�F���/�Ra���ӣ�. \����3N����1��/�\B���褖�F�7� ��[�L$mh��/�z�� �xSԛ�9C`� �; ��Xp�2����j��I�����}��:���w2��ե-��� ,Ij�.�D�4}�!�ȇ�p4�&��{�R�> endobj 333 0 obj >stream x� ��O�` �ߗ����B���^�&@ 43Ə,�6�(&^���![G[W��PGaoq�N6�BK�01b��Wv�'��ĳ�vN�ϓ�sz�g����W��E� ��w� 7�_��ہ��}{���{������ a�_y��X� �1��)����r4�[��Z��:�S��=��k���4�M��m����� ��FzK*�f���ޯϯ_�Zqg��ҒRN^���Д�Sx��6 �+� =]I$S��ziJ�I�Y�V����~i3W�0�L�EQ˩��(~o��dd����ZϭIX H~��R��H��y��j �O4z'����) ���� endstream endobj 113 0 obj > endobj 334 0 obj >stream x�UTmLSW>�b{����a�k{g�ӄ!�9��Ad�ʆ�X(��V��� ��@J#���E�� c@$�p3��������,�cn�?65f���[�]��8���{��Ũ\��X n�[0: �:0&s�\ T�|��ޝGR�}�MH� 9Bb�$����r��A�e��=~��pJ���|VN��t>;33��V]ն:��&9�Z�����]�$?�|�S��U�sX-?C7� ���N7S~@��~E>�qf�H^_6dO��_�٪2���� q��nE�^6&+�y����:�賬V򌳫oq���% �cp��/M\4.�����֞�W�� ���� � �3o��~J��eF�� XXZ�B�|q^YI  !�m�C���|���'����˦�����Zc~�L�Wuj�H7��h��>ҥD;C�P���d��"L�K� ����������2���p� ��J3�R� �� o 0�$^�����Ӈ�w`��H�W����u �O�����`pOg�����]��b� ��ܶ���%���������iҽ%���:���Z���8 ��ɠUT��!��ߏ3��a�M�S��Ck���38;4�.Ql��r�q���U?�Ť���,���`�+ξ� WK�����'���h�'�QO^]�7m{��ټ��l�V�^�Ki�ѵ�-c�{���nK��O�y��s�s�s��n����F�6R��ڀ� endstream endobj 105 0 obj > endobj 335 0 obj >stream x� O_Lq ��C���&Yp>�:�\SZ���j�?˭�V� O��'SD ��(�"j�,]l���C[�Z[n��i�zЧ � ����> ����� (��� -w�����O��E��������r�����y � ��BD��&��?՛-v+�A2x�Q�ku�� �Lu��� +e4��C��ɚN��l�Ǝ��'�r���e�Jm�4[;.j*p�bH�a#�=D~�lb�FM���U ��L[�7����`�j�� {��L픉b�+E��""m5�,�#(@ #y٥�L�l���8��3�M��2ŢLd�$���}�/�n�0IU�����d���E�)BC~0��T$>�� W__�M�M@�ۢ�:��0a�CӴJ(�L�R_lx��v���;0g�����ǀ�7�}�P��97��$��> endobj 336 0 obj >stream x��y XS���@Ω*�H�mG�����u@�EA�*2�� @dq�y��(ep@Q��V����uh��v�t{_�����}���" ���^k��_�Z�P��(�����!~�cW�|"�ƥ��@��n�g&]) �)��������Ie~�2��]������2/i���K��+������"��\JN�:����V���0���k~|x�ϸ`������#�� ��~~����Us�.s��څ��:_� �>|a�"�b���H��Q�ˢ� d˽W����[��� �:h���q�'|^0q����)6��N�>�vX�UΈY#G麍�Q�`j95�B���SC)GʖF���S�(+ʉA9S#���(j 5�r��Rc��� 5�ZGͣ�Q_R���jeO}N-�Q��Ŕ5���L-��P�(ʁ�J�S�To�՗��4K��TJEYRB�%�P)3�S�Om�ʏ���Rݩ �*��E�)'� ʔ��In�A�|�=�9�N2�f&1�����t �c$L6��' >I�>��� �=n�\�S�kq��!����N��C �>-}G�}m�k�]�v��/��;A������-C-�- -�,� � ���m������b��x`���>-���g�� ��Kħ%$�$�K-�� 2 �9�}�$�07�F:Hl���He! �l�=�[�7 ��:�13>=���^���C�����91ni*#� �&3(=5/�NTz��L�E���� U�b&���� �g�׿�?�q�I�s�! gs��G���m�'��N'�����L�� �0SЍ�m)Yj�#4rI�ة�Su���-jg �F�ڢ��ʦ ���#�����m57�"��\g ��$��Z�j�� ��}!v.Ɏ�&�效���ط��d#�Y��H��͑Q�-�* � ;q} :��Y�my�̭ ��gw��{�Z ZżK��*�fR2�F�ip\�C��e�p�i�m7��p[LѾ��i ���v+x耑K����]��s+�&��@� �p[�z��g��X�f3n��� �sw�\o�~x�`$�s'. �����10�F5��y����t��]ǲr�},����4��-�"�r{���!ĩ�o׼��dKYF^��d�P� �t��(����Г�whD�2#� �#���=0 `��h`��Ʌf� #�\�!#�A�� �/F�X{v��������2�%����|B�׸�"L��.[ǤM=����a_ �|6�~��g���'��r~ �� pz�I$� �g��sn Z/�(\T��� i� F ��"���@h_��r ާ��1�3 �p:���m6!��~�����1x �|c�[B��~Ďh7p��0�,���-tC_���fĔ���j&��"?�U��hsxmMn�Ǡ�ka�%��t��0�@� ��{�h�����\_�t�^̶`�����`u��(m��8l,�k��W�RMZlJ�(v�]H��*�:�/����Ex���{�Hi�Z#S�'����I��P8�sS���,N�$�s$�~����/���(ļ��V��p)�ʉr3�� k��4�/ �H!��ނ��`�Ai,*��FPv�9K6���� ���� �J{�n@ӐC����k�"d�荸>�G��?*fc��9IfI���=ԥ�Uk6�H�������W#�s�h%�K��t�͠a�����A �}ä5 Z���c�"�c���t��H�&F�� ���*QUB�cfBvR1I ]��f[�� �B-b|�����'�vB��6�{J�P� ~�]���"j�a��+~+:��'�zŽ���k�G\��cI�|��z�ߪY�)Wc�=?���gW`�w���1�=��-�j=�_���"����6�&� ^On[�?SP������ٸ�q����� ��!a���NLY��Ү���IUiCVb�}��mk'�'�0]\�T���ğ)�P �Z��=յ�4��m�%�] gq�G�y:����S�=~��@z��^�����'�$ kW�C�D�� �-f ���+��A|�:�R{�VsL9`���No�%eIf���я7L 0�i�ı�.��B �7� �4(�b��=�a=�$�����D���N=_���t�ц �����Y�n�q�� � Xop��e�ޤjȞ�l�� N��TQ�MH74��� Ugk2��ێ��v�-8��l J��D�*L�;� �c4�/�?*����˒��� ��R����u��Y ��Q�V����A�1� ��G���tڊ�ե~�#߆=C��������t����-]��xMh��l=G(IC�����x�� �����,ef���w��~� ��3�)^0��h���� �'L�ʃzSG.��HY �3� ��\7Atf|aA] j�i���X�>o���㑒���͈���q;?�� /� ��f�LI; J�-A�0Y�k7�͎��l�0�������qq�| ���Hfi�;����+ B���ZQ�W�����D" �v�no^w���*�Kw����cp���0J��X��3+1=�c����C�y�����H�*�����-Z�����d� �R�I�Un/,>@��j~�O��t�LƽWbS��3��[v�*su[�j���񐴮_�i�=X$� �AM2����8���紽 k�H �����N�����2�������OOӏ���'������3���ә/,��P? ��g?a��~�T��{?��)��Ma��/_-9a_"I�٧�K�? L��� �[/�Դ�.�6�Si���M� � `_?5l�D��7� ��y}����]���J����pM\כ�~)�x��3�~�!F�GK�ӣD-�]��#EG���¤�h�D�7v8@�[gvt4JZ����`�>�[^e���U�o��������t�|�SK��ƌ�ުJ�$ٴ�+`���0 l�亿w� f��e �N$=�#�������M��ᓻ������܆�0��}��moN4�6�ԬȊ�V!�dG�n��G��'��0� ���O�m�R�x0XG���y�I*ڒcL�Pe��\�"���n�i�Y����F�x�o��>h"g�X�f� ��� &Y�ơ� $`+ ������n߭L`TX�OCr� �P� _�����|�J�͸z8X V����Beq� �^��Ӓ��0t������CF:�^x�ɛ���%����/흤#�]:��\�1��v��%���^�W���L�ӈ�L~8R���1�� !�E.���u���� InH*g������'�C���8���|�97�k�E&  ��}{GɶRT)�����N � ���9���6��j�Pmr�HM��/X��~!�����Ҙ e�u��+� j�iܹ�����a�q�ZC~��� �FT��$��:��"�ǧ6���� v�ۀ[�Oq�c4�e � �� A�.�,���;�&b�.�HU�����He ��(!>CU>V�����;�kaLa�6&�a��oڪ�Mx�-,K�n�N �%�HLQ��|aT����R���0#1uS�8�:�e��/��� �w7�Ɋ�� E�_�sN� endstream endobj 103 0 obj > endobj 337 0 obj >stream x��VytSe!] ��qZ���*h��� (b�S�])M�4ݒ4�K�tK�/_��I��tIۤ-iK�e_������Ό��ۑ33| 3g �8s u�,�;�����߽�����@"� #*1)i��{+�G��9�L@���ɋ�`~D��y7 ���a�BX| �`$j�Ŗ(�T�Ky%{U�j��6�K`?��S؛�\qi!G�N�%\>��/�]��R.�`�z�� *7>��\._��K�żW'��D ;�+�e�"�v��`'s�\� �53�D!�RJp��$aW,(�V ���K��,%�T�)P�J���bF�g��Q������8�jw�0'!Sx�X@�j������3y�av�~/U�X�եWe�h��.�(���*W}�XU��^�zXY�3d� �C��� �� �{��� 腶-J�x�N����*��m� �X�LzN4�J�4��r�����b����o9kbw6��ķ� ]��F{��xx�'��9�.�#� y�VT����6%N_���Fq�F�O� �U9�JdԂ� ��s�?�7lO�%�a�_��g�= �&=Q{.'� J�XV]J�����P�w�͐ɀ)!&L���uiII��Dg�q��O>�sߖ��-� +�m�]� me�НUÌ�����U&�&_fQO�HŁ(��Oa�M� ��"�d�A��ඏ����w����Z�� ��}]�]Jk�O� ���[e�e�҆v��*Y���,����G-{�0�g�D | . ���S� �k r \�R Y���@��� �L�(Q��N����}7�n� �z�-�ҍj-�K��)&�&�|������z?kz�1a�87}���K[Q �� f�3�� ˢ0�F�����1�� ��e�z� q��kXz�1��}z>��/��#��G���q�ҩ��b�S�y8dS�����-��[�N|n���Vݑ`~:O�?�e�gӃD�A&�= X�E廣��\���D�k�?7d�? A�9Z�u endstream endobj 101 0 obj > endobj 338 0 obj >stream x�M�_LSgƿá=�r�F��2���� �ʲDQCT�1��tn�t���Y9X��c�P�`��#�V.�' ���$6�X��1.ęcdcY�Œ-��u ;x�W;t�Oýj�n/��;��#�H�;�S�;�j�jć�j�֣�� �E���Aw@S�_�[�f=� �ZPm�Oh�.���jx�fc�6�*� �h�!��h�B�fX�6s!n����E��4�m9nv�dWy%�2��-�?�mg�0; {;�w���ð��Z�5����BV.��5�l��0[�}�2~�O�6܃�&�K����Wp�}�r%� xއ����-Q?�f7����)��?���b���;��:k��7�~�����%���Č�%�����M���> endobj 339 0 obj >stream x�m� PS��o $E�-X��i����Z\��u�7��l�/�@X%��$ ��A0 ���K[�bk]�U_}U���j���v���:o�ܙ3�̹�������w��(J �ָnsZ0�6��$�&����*T�`-k�c����J[��;�SB� 2F�&*:A,w��?����y� ǅ 8;������E:��Ƀ#���$�q{�H�� x�L�|>j��V���+ �Bk9�2S��x�� �����f�̪Ъ���ѓ9� �V��FA�]Uѝ���p�j:�tvU�B��������A� ����� �Kv�q����.3ٽ���������f���T�e��ۼL ���`��6���cٽ�Z8=�&m���f�%���rq��PR�� �q�oϐAɬ��B*y��|����1#��9]���K��%�������hiJd��?Q �9K����b+�2G� ?]Sknn+�ʦ�����~����ټz��J� � )�@V�� �/U���܉PVV`ޡݨI�8ؓ ��q��]n� �0���ܺE�ŵ:sv-�C��f$�X}F ������w7�;�+K� T��h- .>�}Ng�)�� � � :y= �� #���h�gNuEf��ph���\�y�t �_ ɤ )Q�1� 2���S>�����V*yE�~5 �hs ����,�&3]�Ou�4� ����� 4�Rj�����W�PmW׫��.כz}��K~W� W1�Ë�#� e&i~qu]+��a��@S��@rB(Ӓ �K� ��0_��_I�x���Y����z�\�n�ＰuY+`��qj[yr�5�i�U ��݂�$�\c`�)��"7�-*o��z�䕟P�O  �� �[�_L���^���$^�k��"�+L55��Д� �����o��#��� ���{����V>O0�Ѧ��At� �1���枚��#����5���'.�|�!�@&��J��kF����$ �!+\�v)O�� 1I��_%�@7 5����#��K��C*���r6��aȬE����ۍH�`D��Qp�K�f¥C�p`�� &�@��\�HU�L����d=�o�v���մ�7�"��o +�.!Vd�wK�_��'�(���^��}�Y�pz̹9�Z�A�?�T 4����B]NQ>k��1��%��].�:ӳ� B}��3)�68��.�(�3(�r��h� Ɖ�8�Upn`�p` �f�*�`����QJ�3����y�J���,�I}X��Z�� �,[��\L ��K��E�QT � ħ���4�S����L��T��� -P1d���?����)"�X#��#�r�"��Xl �?� m��`me,���7X���� 0k�� endstream endobj 189 0 obj > endobj 340 0 obj >stream x�cd`ab`ddds� �4�T~H3��a�!�������a�n �n ��� }O� ��=J����1����9���(3=�DA#YS����\G����R�17�(391O�7�$#57���Q�O�L-�Tа�())���///�K�-��/J���Q(�,�PJ-N-*KMQp��+Q�K�MU �ML:�����)�槤�e�200000v1012�������q�����/��d���G�hOϔyݳ8�g/�Mjȩi����ǵ�����K�zkcϔ� �'��/�i�����t��\��/��b^ ʽh� endstream endobj 46 0 obj > endobj 341 0 obj >stream x���}Lg ��z@D��:i���J���E�#3 ǌ��C6��,�����kim[{��>\�i ��EM��1 ˆ�4�%[ܢa[��Y�T��,Y�s�J�+�2�e�,��{^�����~��s(�#BP���7V�g����(�Nĭ� �^ܛ:� 1P�3�.w�����*8� R���6�����ӳ�jtz �jUɭ����r�6���J�ZC�*�\K�ɍJJ#7 5Y�S�(���Z�4�����fs�\c(�ѭ��l#�*��|�2P��:E��F�\C��$K�S�N�o3R4Y�;E�Z�^O�NS���r�֙��i�)�pd��0�L¤UP*m�J�2ZԔ����R�Bj:�A-7(D(N�H7HVWԏ��!9�&d� U�_�^��X?v��.�c�q�e�G�W�0�j��V;jH$FG���`x����p g�8) ����1[��2@z���d�;��'x��v�8��^Go�e�Qxu �$���ŗ��Y�3��S�t���ga�?H�� �^W�L0� �w]� p��B]�.��A:���ÀN�=�d�ɸ �D� ��ɼv�?�'��2= �U�J2p�oY泲��y�{�.��8��%f_����ɦ��:�a��P�b���T����%��;s��\�/�n�(ٷ{�ᦫC&��l ?�#^{R�\S������ɭ� ܜ�������U���� J"�� D�_l���v��3|���vF g_'7�ǊcNx �#aW�"�q���i�P��C������\X� U) �! �uغ;;��t���!��-e��X�� ʊ ��=�y�ڈx�`j�� ��[a����B�V"ȟI%z� endstream endobj 175 0 obj > endobj 342 0 obj >stream x��XTTW���8�k����\53�-��Fl�AE�.�;������Ф� Hp��bo�{�b/�F����;�������[�-�b����{�������#�:Q��xIH`�����?oWÕ�| �׈�J8 ���B %B݅�{�ھ�V���{�z���а|�`��zZ`P����+�|����㭇���#�m�׺�/�T���+�g�ʃ�.!�n��6~~� �����0�Ŏ��{+�ݽC����.�rwOo�^��f "wqs�w����?���|��;����|I�����rQa��6��#�����!row���b��c�Ǝo=��C�� )�kʎ�O�S���7�@j15�r�SK�!�#5�r�������0j)5����Q�)Kj95��I��l���,j5�M͡�Ps)+j 5��O-��S�T Փ���E�P,�%%�L)3JBqTo�ES]���?Ճ ��@P��X#(�l�`tK�F�k�坚E3DM����U���o�i��.^]�]k� ��ޣ�s�G���q�񳞞=�~Q��^MBMZ���/��r�x�8�t�i��M��f=̆�噵J�K.r8m�{G�������7����믪�j�&@����H�⨏��8��7 ��' ���*� ��Ԃ�ݜ �F4�����Q��;��)*�o�O�$9@����!+�|lƖ�Ӫ��Ƕ����% lA:���ưUV����]�!�:qy�mE�$�k��@��Jsj��(,�Qy�qߢ���pf=��\���,{� ��Ɋs)e*���}�f!2�wG5�:����:~ P�ޚ�p��-> њꬬK�cC� t��p� ��ȝ�K��%f#� ����%o���X*vCAy�%�*��_j�w��$b�6�;~g�7��2l���㐴%V�Ј8P]��� z ��V��(޳v\����Ok�g���!�@?K��&�I-Ui�K޽�~��&��=��4XdZ|����i#0�;���{R����)�Z[%K��)�E���yK0Ũh6m?L0"O �T��$_n����˟hE0�� �� �ݶ �c���� C�UI��1�� ��L�R�'h���D ��_(��� �8q�����Q �� F��Ȳ��1�2v�!1{'%6*? '��]�q��U�_��|���,���t����D���g9��ߟ$I�R�0��d�~� ;��?A���c��yЊ�iv��C�Ύo��7�����EY� ��ܬ{�`��������6��UVؒ�Y%��tj@��R�Gpߎ 2��hU���Qc~������_�֓)�L�| �|&HŊUA��(%���������Hs�A٪�d��I��M��u˖ ;O� �j�b/�j[��v��3��� ŭ�Mi����\ �&p��� N �ţ�̄,� � ��'8*X!c��Bw]�y��n鮋$�M�J&��9����V�YƘ?�ԩ�x�FX~�艄�f�!>�����r���"6�� �p���px܁�/���e� ����iYQ�N\ޤ�؁(��P�H���p���7+���Q��? `�����q �=�n�~�ù�'���v1��-IމQri��j��e�b{L��4�I`�173%��|�!~�N�� �{���> 1S��\�[0�i��p;�,=2G� 1e�Jk�,���g�Nx ��o�_�y�:_��>� )���8J+��� �ͦYUn�Q9Z.�V]��)��$���mOD�4��OD4�ٶ�?�D��Ԥ 45 cb{ǘ�4�֙v a?r�%��K%���P �8�QT+� Y':CK�n�f�t�sW�pJ��Ā�~�%)�iW� mM1&;���W`>�"1�~�3��$5�������KL2��Ѻu��h ��*.��K�u���d�BBNK�� �8�.��G�5�|��*�Vz��{m �`� ���n��2����{��Ԏk��7�H�C'1P ��nK�˗FO��p �|��)����/��;qrș�2UzR�/玭T��\?�7!�sbAz�)���[��o�e���-|z��#i���BV��50Jo*i�˥ @h~1υ�-|+�\�d���;�c�L����7v�ݥ�i ���A�6+m ��s�Ǡq 6�8u�Q /Kq� q1��ݛ�+Ux�X�=��I�S�v�=��^ȧ�ǈ��� �oQ����dk�m-�)Qz��k �W���+ف�n|@L*1;c��� �u���@�l� w��0��7ǥ�^�E;p�_w$P�|ޛ����3�D�~X*�I�G��'uM;w\B��e��e�+*���&���ٖ���ʋ���H�ަ�9��7GV���'��>3�sk42c>��x3 d�L��&ܟ� ������gY}8z���)S ���׾{!�.Y���e��/�_�E�m��W��T��~�v����Ou���\xg��r�{ԳE�B b���֢c�k��}>n�GAeoT���L�y} "9ٝ��*�c�e��k�'��w�� ��1��_��}�Zv>�0}?�c 1��xu-���?!�!wp'�ե~�^�#k.� C�P��8w}�dֆ5���5����Fs�ÿb���t���hp�����^ j7�> 7x� ���o ����x� �� �\˰�5�1���L�+mJT�K�� ����؋7 �0w'�m��U|2X�( � [�����i���_�������0���۹�*:C��^__ ����L��c�YZ Ԅ� hvwN��LE�r3_��#�1�YuyD9�ޙ ��{W������I�H|O|�4U9�w�L��m�,��տ�^��9=�� �W�.}O�T�2���/F/�h�X��Yc�@�$ĕ��5�޾ ��;�+v� . $���`�U�����B���(��5�m������qM�Z�dͮ .h"3��f�����SK�`l���� � �OC'�z����ƅ�ӷ��̵�_�p �$$s���G�����#����������.�F$)��S�ȍA��s���M"{�Dw�V�*�J�6�"~}��� �$H��X�kB�=�Bȯ1�TM� 4x0bML���4|�6��0���'��隷uկs�Ʀm7����Uܜ�$Äܣ�[��e웊حn�t�$�q��.� o�`���|��8�a^�4�B��A���s �KB��F�\�$�2g�����an�9�Hь����Gx&����V6���4[�0G�V�{VRQ?4c�0ߴ�mѩ-G�?| C�jT�n� ��y�o���,���pբ���5 V�`��͞���G{G�h��Ec/�G�H4b�4,fR���_� ������j&61���W�η �i��q3 �}1�p���L���k"����{��l���>������}mm��4T� �,�����c*�O }�MZ��A�>��"%�N ��$b7�a�:W�DkuǙ�Tڙ?$�A㲶C����ᑪ��4ݷ�����B0���"P�Wɝ�|Jp�����r!�:���&%���B�)�;�U���� ��w����R�:L�.��s�h1��= �ޥx�wV( �����c!̇�bt�Du ��N�[�!��&���Jl샍d>X���8�8���"/�Pk�u1?��u :��#����iA���%����z�ՙ�V��W� 4�;�Ò +�����_Q� &Kޜ��1!��>�/N��A�U�a¾�ٲ�����_���7t�W3w���د"�� '�$[Z޴s�1�4o��6v�̉+}�P�,6!X�k�N�{@4�~�ۊlil��� ���*'B�� wO��&���٤$��XLk��f����f�q��]��� A����͇t�d*~��� L��g�O�Ռ}����ĩ�A�R�g_�Q��ܿ�1i���UwCe)��b ��u�d R���`��zwY�$2�K�J?v�9���4����|Jާ��rBu�'Ϡ��+�0��(-�W� q�lv��U�n�����A� &�5�>#JW����M���W1g3�o��l���D�k�j���W�f9y�`�Y�����V�Ϗ� �H= ?I�=��B8%⳨�b�~&�N������3s��yh9򪌸�R���p�UM�@�����#� -u���: ����%���C��c��l�a����k���x���qߌq^]-c���� [S ��4mO�dv��I���i��N?�\ �. �x2;�d5�NX�cN��7^]*q�8�7��$����Y!CV�/K�� ��!�8��&'�5���c p�\b����c �s��[���YF3D�K ��p�"t5��~����GOgM�Ai�o+|CbЦ�ѵ �-`�L%��U����}���l�B�����i���2c4�%� i�7Ԫ�9��FE�y��q�> endobj 343 0 obj >stream x��V{xSu�>!% �t'c��9E u��"��rS��^B��$m�^ҦM����N��{�izoZ�B�� ��tD�qF�QY��Yf�q.��N8�y����g��'On���}���{�WB�� $ɬ̬�m��N�]"R- d������2���*�@��4wYƪի�_����O��بV�T�ٚ��l}�R��� �e��檔zC��uz}�++**��V�>���_����A�$�$ �O�H!ޖDf̚1*}Fz*eG��o� �U�zh���g��9/�������M���+��y/�;/0i·昰"&A��4^�~}Y��zp��Kc�%�S�;2���k�/�����@/�m�_��7@��f P}���Y�ɷ �� �%���:`����=�����;� ��z�4��Ϗ�d�:�]_E9.���!�̫3T�jv��� �;�q1 ��q��! ����r�\{�NK�C ]������F��=~p� �L���^� }Y� ��DV.�su �A D]鿕}�:�lj��``H�� x�F/5�jg� � _fb�� �J WȮ {p��-���>��Rp9�i��6�AG��h%Ǌ��X~T��elwOu�ˉ�@�آR��n��.-ς�����>�5SiB��~%�CW�Cw�B�@��+i2�JK�jy{�L�Uԗ@>��S4��_����}+��o��� ����ݙ�^ �P��+���"Y{��"�u��h���As�|� =���k�h�����r_ �������� �Vo�](�2(����*���Z��aꯃxB�s��˃C�� ��1��.��L!�a�G �R�L��s����nqY�'�0&q����i �U��l��\Yp�k� K��� �e� 먶Q���E��1��Qd��� Cǿ �IN}�4�&2��j����j�� ldi�����c��[ÛŇE��X\�����y7�����j��-%`rG��+��� � ��x�" �� ŸP���|,���߸����;����SL�.��0��� (����qqɊ��~=��� � ,|��Ѽ��� ������lo�; �2V ޛS�򍚋'�����o���c�#@�߰���u`�F�aJ0�/䡑��� ��p��V1L&x �0ٓ0� @O��r(�ed1e��t�Dh���� w�PVa����:�`]nU�Fa7��6���a�3�5� $���!�pf�U�5Q�s�'�҅�t� \| rO�N�Wy\Ax�PTb�chmL��M�q�k���� ���VQ*>���������������o�J�b�h� �� �֨��A֜b1;��vLx4&�uS* ���̓J�� �;)�pAW.qF.C|l���7/���C4ǖ?c���iQ-]�P�i�}�p4�M���"Cn�pſ�n��J#U���%z]Ye��A��3� y���~��>{f�4� 4�q80vN�[�,�x�G�^�?�&��r��oq�l��z����`D�ALg���\8����/���V+V�^�l�(�� |�8/�%�e���x�(�s�X����;}@�ݛ1mG������i�jZ+��8ɳq_g�j���T��JE τU��e������ԉ��[�˸�h��?K\�O#)N\��~����?J���qm������*8��J�����T� ��1S�*�55Z��3ؽ�em�E؈���2Ẽs�c� �t��u��T)h��B�*�A�Gw��V�Rf��Z#�O� ll��u�S4�2sN:��\��n����$� �~{����}�YJ� h��S��U��N�@�{T�S�کt�t��h���$� ������h\�O���nG�� �A �����])�{A�[ڡє�j4 ��� �x���voOTrIxH*lO�%�2��N��6cg(�?e��N`�%\��7���DK^�p> endobj 344 0 obj >stream x�cd`ab`ddds� ���T~H3��a�!����+�G>k7s7�� ӄ�� ~����)���ȘW��;�9���(3=�DA#YS����\G����R�17�(391O�7�$#57���Q�O�L-�Tа�())���///�K�-��/J���Q(�,�PJ-N-*KMQp��+Q�K�MU ;NL:�����)�槤��f�%��%�f�e�e�T2000320,c�b�����A���Q�G����+�1>������{���y� �wW����VU�]V:�{�> endobj 345 0 obj >stream x�mS{PSg��M�K#؟� ۂ��z�x �/����Q�"�������ܘZZV�)ޫ��W��W��$��W'%��7��⢂yf�V�Ph�D%�)-*Vh��+�+�ڲu���!�@]�P�ٻ1.^n(�*�ي �F��-O+-�ʷ���l a�Z�.�iyf�n��� ���*I�V��ݰ�ɸ��:�G��Q���&���X@�HBD��= >a!�ȉ�5��y&~�ƿ� r �,f/�� ���O~�z8��� 2�c�AU�� � �|�����-ǋq�?�Cb$�|8`oꨯ3������Ֆ��G���&��a�n�2](�V� ��|A ~��߭��|��9��j�%��sʫ;�n�[aS�Ε�Ӛ��&x�����Sk�UO�g0� �=x��*\�� ��QC�� �>�9n���+Ǵ�槇��\4�zހ �dBr]� 7�i ܜ�� � ⡙���YmifF"��L���=B��� sm�t�Jx��&|��-8�M����}Y��u4��|������`m�z�|:t�����؍k��p�n4r�i��a�4?+�}}��/j�����7ђz ��������s0yn�0�l�ܣ��i*�� �-Ǉ�� j���3�P� m {^���v����v�Y����FiFr�q $�=�؏���Y7Ɋ�y�\��;�92*�Z�O�Qa �tz� ��Q�c� M&Eu endstream endobj 141 0 obj > endobj 346 0 obj >stream x�u�mL[��}c �)u��ޠK�uIG^J(ME3��m*�@�&�JC 1o�y� ��`_�؏�_16c�� (!) ��TE,i:-�6�k�.��nSu.����T�2�_������{���?/f� �����܌�o��� f�&f X��6Jb!��1c�7�o���`哸b+/� j�F�ÕYW�QTWV�D����^��8�"ڟ��!:$�(��ŵ�|��J"���Lt���Z�҈v �R��_y��Z�*�+S����I��UU�c�D�$9#z��V%:"�KD��R ��u��F�D!ʯ;#QԞ��T�J�\.VT��x�����+ � �x��+��e�x9�t� E� �x�i^ o�H&f6m�4�/�)���M����ɴ ���̮(�c��|[��.4�4j[�z���e��暦��w  ��'���� C���g�����>}��42��O+\FEC���o��a�_ �4�U��u���Bhy�k�g���#�����s��?�V>��>�&��} ��Y���MR��� ��b�~� �5��� ���P��YO�:������"��w��׍����|�~QX �r�������� =��V��YZ}1��/e�&Xu���P���Kvx|�~��H +0�(���5�~sQ��`� |�$�ݢ����ŒJ@�&��C�~�oy�;�}�)L9��Аm��!�s[��+e�E �&����J{ ��QX��>7� �9h ��M&�����N[���/h%7�v���H�1w2�|�G|�����ja��wC�����c���;���9p����+ � �'�� ����%��OK)�3�L��|���w������� �KC��5jv��LJY�N�)@��~&3��8V �> endobj 367 0 obj >stream GPL Ghostscript 9.50 2023-10-27T13:50:37-07:00 2023-10-27T13:50:37-07:00 LaTeX with hyperref endstream endobj 2 0 obj >endobj xref 0 368 0000000000 65535 f 0000169680 00000 n 0000251597 00000 n 0000169455 00000 n 0000165041 00000 n 0000247838 00000 n 0000169880 00000 n 0000169921 00000 n 0000000182 00000 n 0000003560 00000 n 0000169962 00000 n 0000193555 00000 n 0000213842 00000 n 0000190611 00000 n 0000203372 00000 n 0000170004 00000 n 0000170036 00000 n 0000165201 00000 n 0000170079 00000 n 0000003580 00000 n 0000009187 00000 n 0000170122 00000 n 0000170165 00000 n 0000170309 00000 n 0000170453 00000 n 0000170603 00000 n 0000170752 00000 n 0000170901 00000 n 0000171050 00000 n 0000171192 00000 n 0000171333 00000 n 0000171475 00000 n 0000171616 00000 n 0000171764 00000 n 0000171911 00000 n 0000171954 00000 n 0000171986 00000 n 0000165455 00000 n 0000172029 00000 n 0000009208 00000 n 0000014309 00000 n 0000172072 00000 n 0000197568 00000 n 0000239210 00000 n 0000172115 00000 n 0000196397 00000 n 0000229998 00000 n 0000195151 00000 n 0000226303 00000 n 0000172158 00000 n 0000172201 00000 n 0000172244 00000 n 0000172381 00000 n 0000172517 00000 n 0000172657 00000 n 0000172798 00000 n 0000172939 00000 n 0000173075 00000 n 0000173213 00000 n 0000173353 00000 n 0000173493 00000 n 0000173635 00000 n 0000173778 00000 n 0000173915 00000 n 0000174051 00000 n 0000174193 00000 n 0000174333 00000 n 0000174486 00000 n 0000174639 00000 n 0000174671 00000 n 0000165744 00000 n 0000174747 00000 n 0000014330 00000 n 0000020108 00000 n 0000174790 00000 n 0000174949 00000 n 0000175109 00000 n 0000175260 00000 n 0000175411 00000 n 0000175568 00000 n 0000175725 00000 n 0000175874 00000 n 0000176021 00000 n 0000176159 00000 n 0000176298 00000 n 0000176445 00000 n 0000176591 00000 n 0000176730 00000 n 0000176869 00000 n 0000177015 00000 n 0000177161 00000 n 0000177307 00000 n 0000177444 00000 n 0000177581 00000 n 0000177726 00000 n 0000177870 00000 n 0000178010 00000 n 0000178152 00000 n 0000178295 00000 n 0000178438 00000 n 0000194843 00000 n 0000225149 00000 n 0000194332 00000 n 0000222263 00000 n 0000193165 00000 n 0000212822 00000 n 0000178481 00000 n 0000178514 00000 n 0000166091 00000 n 0000178619 00000 n 0000020129 00000 n 0000025645 00000 n 0000192269 00000 n 0000211177 00000 n 0000191643 00000 n 0000210313 00000 n 0000178664 00000 n 0000178697 00000 n 0000166257 00000 n 0000178828 00000 n 0000025667 00000 n 0000030835 00000 n 0000178873 00000 n 0000178918 00000 n 0000178963 00000 n 0000190432 00000 n 0000202475 00000 n 0000189979 00000 n 0000201225 00000 n 0000179101 00000 n 0000179146 00000 n 0000179288 00000 n 0000179321 00000 n 0000166447 00000 n 0000179489 00000 n 0000030857 00000 n 0000052790 00000 n 0000179534 00000 n 0000189597 00000 n 0000199401 00000 n 0000198921 00000 n 0000245753 00000 n 0000198464 00000 n 0000243740 00000 n 0000179579 00000 n 0000179721 00000 n 0000179754 00000 n 0000166629 00000 n 0000179898 00000 n 0000052813 00000 n 0000058384 00000 n 0000179943 00000 n 0000180081 00000 n 0000180219 00000 n 0000180264 00000 n 0000180309 00000 n 0000180450 00000 n 0000180483 00000 n 0000166827 00000 n 0000180651 00000 n 0000058406 00000 n 0000064818 00000 n 0000180696 00000 n 0000198100 00000 n 0000242903 00000 n 0000180741 00000 n 0000180786 00000 n 0000180924 00000 n 0000181065 00000 n 0000181098 00000 n 0000167017 00000 n 0000181266 00000 n 0000064840 00000 n 0000070431 00000 n 0000196897 00000 n 0000231571 00000 n 0000181311 00000 n 0000181356 00000 n 0000181497 00000 n 0000181542 00000 n 0000181575 00000 n 0000167199 00000 n 0000181667 00000 n 0000070453 00000 n 0000104820 00000 n 0000181712 00000 n 0000191024 00000 n 0000207992 00000 n 0000195859 00000 n 0000229408 00000 n 0000181757 00000 n 0000181894 00000 n 0000182031 00000 n 0000182173 00000 n 0000182206 00000 n 0000167397 00000 n 0000182439 00000 n 0000104843 00000 n 0000109689 00000 n 0000182484 00000 n 0000182529 00000 n 0000182574 00000 n 0000182607 00000 n 0000167563 00000 n 0000182736 00000 n 0000109711 00000 n 0000115025 00000 n 0000182781 00000 n 0000182826 00000 n 0000182871 00000 n 0000182916 00000 n 0000183054 00000 n 0000183087 00000 n 0000167745 00000 n 0000183203 00000 n 0000115047 00000 n 0000120416 00000 n 0000183248 00000 n 0000183386 00000 n 0000183431 00000 n 0000183476 00000 n 0000183618 00000 n 0000183756 00000 n 0000183894 00000 n 0000183939 00000 n 0000183984 00000 n 0000184029 00000 n 0000184062 00000 n 0000167951 00000 n 0000184217 00000 n 0000120438 00000 n 0000126332 00000 n 0000184262 00000 n 0000184402 00000 n 0000184540 00000 n 0000184585 00000 n 0000184726 00000 n 0000184759 00000 n 0000168149 00000 n 0000184875 00000 n 0000126354 00000 n 0000132330 00000 n 0000184920 00000 n 0000184965 00000 n 0000185102 00000 n 0000185147 00000 n 0000185180 00000 n 0000168331 00000 n 0000185361 00000 n 0000132352 00000 n 0000138466 00000 n 0000185406 00000 n 0000185546 00000 n 0000185681 00000 n 0000185726 00000 n 0000185864 00000 n 0000186008 00000 n 0000186152 00000 n 0000186185 00000 n 0000168545 00000 n 0000186314 00000 n 0000138488 00000 n 0000144669 00000 n 0000186359 00000 n 0000186404 00000 n 0000186541 00000 n 0000186680 00000 n 0000186817 00000 n 0000186850 00000 n 0000168743 00000 n 0000187033 00000 n 0000144691 00000 n 0000150128 00000 n 0000187078 00000 n 0000187123 00000 n 0000187156 00000 n 0000168909 00000 n 0000187259 00000 n 0000150150 00000 n 0000155715 00000 n 0000187304 00000 n 0000187349 00000 n 0000187394 00000 n 0000187439 00000 n 0000187484 00000 n 0000187529 00000 n 0000187562 00000 n 0000169075 00000 n 0000187641 00000 n 0000155737 00000 n 0000161256 00000 n 0000187686 00000 n 0000187731 00000 n 0000187776 00000 n 0000187821 00000 n 0000187866 00000 n 0000187911 00000 n 0000187956 00000 n 0000188001 00000 n 0000188046 00000 n 0000188091 00000 n 0000188136 00000 n 0000188181 00000 n 0000188226 00000 n 0000188271 00000 n 0000188316 00000 n 0000188361 00000 n 0000188507 00000 n 0000188652 00000 n 0000188685 00000 n 0000169265 00000 n 0000188731 00000 n 0000161278 00000 n 0000165019 00000 n 0000188776 00000 n 0000188821 00000 n 0000188866 00000 n 0000188911 00000 n 0000189057 00000 n 0000189203 00000 n 0000189248 00000 n 0000189293 00000 n 0000189338 00000 n 0000189383 00000 n 0000189428 00000 n 0000189473 00000 n 0000189518 00000 n 0000189551 00000 n 0000199710 00000 n 0000201450 00000 n 0000202718 00000 n 0000203715 00000 n 0000208276 00000 n 0000210543 00000 n 0000211534 00000 n 0000213066 00000 n 0000214405 00000 n 0000222526 00000 n 0000225410 00000 n 0000226654 00000 n 0000229618 00000 n 0000230320 00000 n 0000232025 00000 n 0000239504 00000 n 0000243139 00000 n 0000244038 00000 n 0000246002 00000 n 0000190344 00000 n 0000191397 00000 n 0000191908 00000 n 0000192019 00000 n 0000192666 00000 n 0000192915 00000 n 0000193435 00000 n 0000194231 00000 n 0000194733 00000 n 0000195062 00000 n 0000195538 00000 n 0000195627 00000 n 0000196025 00000 n 0000196113 00000 n 0000196687 00000 n 0000197481 00000 n 0000197974 00000 n 0000198350 00000 n 0000198832 00000 n 0000199300 00000 n 0000249970 00000 n trailer ] >> startxref 251792 %%EOF

