# [AI Mistakes: Who’s Responsible? | Built In](https://builtin.com/artificial-intelligence/responsibility-for-AI-mistakes)

AI Mistakes: Who’s Responsible? | Built In Skip to main content Artificial IntelligenceConsumer TechData Science+6Artificial Intelligence+8When AI Makes a Mistake, Who’s Responsible?Dissecting accountability is not easy, considering all the hands it takes to create AI.Written byJuan RamirezPublished on Sep. 05, 2023Juan RamirezHead of revenue and strategy at BraveUpJuan Ramirez is head of revenue and strategy at BraveUp. Image: Shutterstock / Built In As AI systems grow increasingly autonomous and sophisticated, they become more unpredictable, potentially leading to unintended outcomes.Why Does AI Accountability Matter?Artificial intelligence (AI) is a powerful force reshaping various facets of our lives. However, this technology has its pitfalls. From data bias to security vulnerabilities, AI systems can misstep in numerous ways, raising questions about responsibility and accountability. As we delve into the labyrinth of AI, we need to ask: When AI fails, who is to blame, and how can we ensure accountability and transparency in AI systems?For instance, consider a scenario where an autonomous vehicle mistakenly identifies a pedestrian as an inanimate object and causes an accident. Or, imagine a scenario where an AI-powered chatbot provides incorrect medical advice to a patient, leading to a serious illness or even death. In these situations, who bears the responsibility for the AI’s mistake?Read More About AI and the FutureWill AI Ever Become Ubiquitous? Potential Pitfalls in AI SystemsAI boasts immense benefits but also harbors potential pitfalls. Understanding these missteps is the first step toward ensuring accountability and transparency in AI systems. The Brittle Nature of AIBrittleness means that AI can only recognize a previously encountered way. When exposed to new patterns, AI can become easily deceived, leading to incorrect conclusions.An example of this brittleness is AI’s inability to identify rotated objects correctly. Even when AI is trained to recognize a specific object, such as a school bus, it can fail to identify the same object when it is rotated or repositioned. AI’s brittleness also makes it susceptible to adversarial attacks. These attacks manipulate the input data, leading to incorrect outputs. For instance, minor alterations to stop signs can cause AI to misinterpret them, or slight modifications to medical scans can lead to misdiagnoses. The unpredictable nature of these attacks makes it a significant challenge to protect AI systems. Embedded Bias in AIAI’s promise of impartial decision-making often needs to be revised due to embedded biases. These biases stem from the data on which AI is trained. If the training data contains biases, the AI system will also reflect them in its decision-making process. This can lead to discrimination on a massive scale, affecting millions of people. Catastrophic ForgettingAI systems are prone to a phenomenon known as catastrophic forgetting. In this scenario, an AI system, after learning new information, entirely and abruptly forgets the info it previously learned. This overwriting effect can significantly hinder the system's performance and effectiveness.An example of catastrophic forgetting can be seen in the development of AI for detecting deepfakes. As new types of deepfakes emerged, the AI system, when trained to recognize these new types, forgot how to detect the older ones. This highlights the need for AI systems that continuously learn and adapt without losing previous knowledge. The Potential Threat to PrivacyAs AI systems become more integrated into our daily lives, there is an increasing risk of privacy breaches.AI’s deployment of data presents a dilemma. On the one hand, data is required for AI to function effectively. On the other hand, indiscriminate use of data can lead to privacy breaches. Users must understand where and how their data is being used, whether stored securely on the edge or at risk in the cloud. Dissecting AccountabilityAccountability is fundamental in human morality, underpinning the rule of law and guiding how restitution is calculated. It is a necessary component of professional activities in business and government. But who is accountable when an AI model decides with a negative impact?Accountability entails that more than just the AI system can explain its decisions. Still, the stakeholders who develop and use the system can also do so and understand their accountability for those decisions. This forms the basis for human culpability in AI decisions.Identifying the responsible party for an AI error can be a convoluted task. Is it the AI developer who coded the system? The user who relied on the AI’s advice? Or the company that owns the AI system? Or, in an even more radical view, should the AI system itself be held accountable? Accountability of the Algorithm CreatorThe algorithm’s creator could be held accountable if there’s an error in the algorithm. This error could lead to a stable deviation or unpredictable behavior, causing erroneous outcomes. In these cases, the algorithm’s creator, like a car producer held accountable for a construction failure, could be liable. Accountability of the Data SupplierTraining an AI system involves feeding it with sample cases and expected outcomes. If the training data set is small or biased, it can lead to errors. If an AI system is trained on biased data, it might perpetuate these biases in its decisions. Accountability of the AI OperatorThe person operating an AI system could make an incorrect judgment call interpreting the AI’s outputs, leading to errors. The operator could be held accountable for these errors, much like a driver would be held responsible for ignoring warning signals in a car. AI and Legal PersonhoodAn alternative perspective suggests considering AI as a legal entity that could be held liable for its mistakes. This idea is based on the view that legal personhood is a concept that grants certain rights and responsibilities to entities such as corporations.However, this perspective is not free from controversy. Critics argue that AI systems, while capable of making decisions, lack consciousness and therefore can’t be held responsible in the same way humans can. The concept of AI being granted legal personhood is still relatively new and has received mixed reactions from the public. Supporters of this idea argue that AI systems, while not having consciousness, can still be held responsible for their decisions. In contrast, opponents of this notion point to the fact that AI systems lack the capacity for moral agency and should not be held accountable in the same way as humans.To move forward with a legal framework for AI, it is essential to define the concept of a responsible party unambiguously. This definition should consider all parties involved in the decision-making process, including developers, users and operators of AI systems. Furthermore, it must consider the implications of granting legal personhood to AI systems. Finally, it must outline how accountability for errors can be enforced in practice. The Role of Vicarious Liability TheoryThe vicarious liability theory, which holds principals or employers responsible for the actions of their agents or employees, could be applied to AI. In this context, the creators or operators of AI systems could be held liable for the system's actions.However, this theory must be revised when dealing with genuinely autonomous AI systems that generate their algorithms and make decisions without direct human intervention. The concept of vicarious liability could be used as a starting point for this new framework, but it must be adapted to address the unique characteristics of autonomous AI systems. In particular, it should consider how to differentiate between errors caused by the system and those caused by its operators. Additionally, it should evaluate how such a system could be regulated to avoid potential misuse or abuse. Ultimately, any legal framework for autonomous AI must ensure that these systems are held accountable while allowing them to operate safely and securely in our society.More About AI and RegulationWhat the FTC Investigation Into OpenAI Means for the Future of AI The Need for Legal ReformsAs AI technology advances, it’s clear that existing legal frameworks need to be equipped to handle the challenges posed by AI. Accountability in AI is a complex issue that requires comprehensive legal reforms. As the broader socio-technical system in which AI exists is still in its formative stages, laws and regulations must catch up to ensure accountability and trust in AI.In conclusion, accountability in AI is complex, with multiple parties involved and potential missteps lurking at every turn. However, with robust regulations, transparent practices, and a commitment to continuous learning and adaptation, we can navigate this labyrinth and ensure responsible parties are held accountable when AI stumbles. It’s a challenging journey but crucial for AI’s responsible and ethical use in our society. AlgorithmsArtificial IntelligenceConsumer TechData ScienceEditors' PicksExpert ContributorsLegalMachine LearningMachine Learning Algorithms Expert ContributorsBuilt In’s expert contributor network publishes thoughtful, solutions-oriented stories written by innovative tech professionals. It is the tech industry’s definitive destination for sharing compelling, first-person accounts of problem-solving on the road to innovation.Learn MoreGreat Companies Need Great People. That's Where We Come In.Recruit With Us Built In is the online community for startups and tech companies. Find startup jobs, tech news and events. About Our Story Careers Our Staff Writers Content Descriptions Company News Get Involved Recruit With Built In Subscribe to Our Newsletter Become an Expert Contributor Send Us a News Tip Resources Customer Support Share Feedback Report a Bug Tech A-Z Browse Jobs Tech Hubs Built In Austin Built In Boston Built In Chicago Built In Colorado Built In LA Built In NYC Built In San Francisco Built In Seattle See All Tech Hubs © Built In 2024 Learning Lab User Agreement Accessibility Statement Copyright Policy Privacy Policy Terms of Use Your Privacy Choices/Cookie Settings CA Notice of Collection