# [As the creator  of an AI model, what are my responsibilities?](https://www.linkedin.com/pulse/creator-ai-model-what-my-responsibilities-mark-rogers)

As the creator of an AI model, what are my responsibilities? LinkedIn and 3rd parties use essential and non-essential cookies to provide, secure, analyze and improve our Services, and to show you relevant ads (including professional and job ads) on and off LinkedIn. Learn more in our Cookie Policy.Select Accept to consent or Reject to decline non-essential cookies for this use. You can update your choices at any time in your settings. Accept Reject Agree & Join LinkedIn By clicking Continue, you agree to LinkedIn’s User Agreement, Privacy Policy, and Cookie Policy. Sign in to view more content Create your free account or sign in to continue your search Sign in Welcome back Email or phone Password Show Forgot password? Sign in or By clicking Continue, you agree to LinkedIn’s User Agreement, Privacy Policy, and Cookie Policy. New to LinkedIn? Join now or By clicking Continue, you agree to LinkedIn’s User Agreement, Privacy Policy, and Cookie Policy. New to LinkedIn? Join now Skip to main content LinkedIn Articles People Learning Jobs Join now Sign in Source: ATOS 2019 As the creator of an AI model, what are my responsibilities? Report this article Mark Rogers Mark Rogers Scaling tech businesses, ML, marketing analytics Published Jun 14, 2023 + Follow I have spent much of the last five years working in Cambridge UK building machine learning models to help clients understand the subject matter of complex pages on the web so that they can decide whether to advertise on them. What is a machine learning “model”? It is a kind of recipe used by a computer to work out the answer to a question which might normally be done by a human. Is this person the real holder of this passport? How much power will be needed by the national grid at this time of day? Is this video OK for a child to watch?Models are built by using large volumes of human-annotated data sets, and then evaluating their performance against a test set of data. If a model needs to determine whether a ball is red or green it can be optimised to highly accurate in predicting whether a ball is red or green - in which case it has high Precision - or highly accurate in finding all the red balls (even if it misidentifies a few along the way) - in which case it has high Recall.We ourselves have built many such models and we have encountered many everyday challenges as we strive to make the model’s "prediction set" coincide with reality. AI systems like ours are used to address increasingly important areas of human activity. The success of a model in predicting reality, reproducing it, raises for observers a moral issue. Should AI promote or prolong worldly injustices? Should an AI model do more than reproduce existing reality? What is the “duty of care” of the creator of an AI?A couple of years back some colleagues and I discussed this issue as a development team. At the time we thought it would be helpful to share these principles and to start or join a conversation about it*. We formulated the first and second laws of the model builder:As a model builder I have a duty to explain and account for my model’s behaviour.My AI should be subject to the same laws and responsibilities as the human being or corporation who created it. It should not be possible for a developer or corporation to avoid responsibility by blaming an algorithm. An AI has the same social responsibility as the person or business who created it. Laws, but also social mores, should be respected. The decisions made by AI should not tend to increase social problems: gambling; anorexia; bulimia; risk of cancer; alcoholism; “hate”; social exclusion; gender or race discrimination. My data should be lawful (i.e. subject to relevant laws for example in relation to personal information).The designer of an AI has a duty to make its determinations as transparent as possible. AI systems can take on the characteristics, positive and negative of the data that is used to train them. The only corrective against this tendency is to offer transparency: to share the data sets used and the weightings that the model used. It should be made clear how much human involvement there was in this process. Therefore these principles should be supported:My data and methods should be inspectableThe decisions made by an AI should be subject a) to legal appeal; b) to broader political and social review. As AIs become significant players in making important decisions, it must be possible to review those decisions, the data sample on which they are made, their settings and their legality and equity.These principles left us with some open questions:What if the model is “wrong”? It reinforces injustice, for example. How do its victims seek redress?Does the model have to change, as society and laws change? And does this imply loading the data in some way (for example to remove gender, racial or social characteristics which society wishes to address)?If an AI results in illegal outcomes, what action needs to be taken? The data and the law may pull in different directions. Does the data have to change, or must the AI stop being used?It left us with the overarching question:At the moment where a model is created does its creator have the obligation to to understand whether its operation is legal?The team suggested some other pieces of information that could/should be made public:How long did the model take to train, and did the creator get better results at any other point? Can I see all the local data mining?Can I see a report on the testing output?Tell me what percentage of other models reached the same decision.How much data did you use to come to the decision? Would more data or less give you better/worse results?Is there any other way (apart from this AI) to get the same answer?If I asked the same question at any other point in time, would I get the same answer?Where could I get more data from?What impact will historical data, encoded in an AI, have on the future?Thanks to my former colleagues Stray Toaster, Nina Friedrich, Ph.D. Rebeca Corona Escamilla Wanshun Wong *The sensitivities of our working environment meant that it was not possible to publish this at the time Like Comment Copy LinkedIn Facebook Twitter Share 16 5 Comments Arthur Meadows Product Management / Product Marketing 8mo Report this comment Thanks Mark Rogers - I like these principles. I'd be most interested in the progress in solving the unanswered questions you list. Like Reply 1 Reaction 2 Reactions Mark Rogers Scaling tech businesses, ML, marketing analytics 8mo Report this comment Given the conversation about #aiethics at the #Meetup last week in #Cambridge I thought this might be relevant/helpful Colin Millerchip Arthur Meadows Sam Dods Tom Sharrock, PhD Like Reply 1 Reaction 2 Reactions John Bloomer Independent Adviser and Non Executive Director in the Agricultural Technology Industry 8mo Report this comment This is a valuable contribution to the current debate not only on the legal regulation of AI, but also the ethical and moral responsibilities for AI developers, both of which are important. This has been a feature in the development of technologies for agriculture and food, including genetic modification (GM) and genome-editing. As consumers can ultimately reject a technology, even if it is approved under legal regulation, we neglect the political and social, ethical and moral, at our peril. Thanks Mark Like Reply 1 Reaction 2 Reactions Steve Jennings Healthy Longevity Innovator | Business Growth Catalyst | Regenerative Future Optimist | Pay It Forward Advocate | Speaker 9mo Report this comment Thanks for taking the time to write and share this Mark Rogers Like Reply 1 Reaction 2 Reactions See more comments To view or add a comment, sign in More articles by this author No more previous content Why “free trials” for enterprise users are a no-no Aug 7, 2023 Using Agile to test a hypothesis Sep 21, 2018 Twitter gets around to advertising to 500m users who don't log in Dec 11, 2015 Get hidden keywords from Google Jun 18, 2015 No more next content See all Others also viewed The Imperative for Explainable AI: Unveiling the Black Box of Machine Intelligence Neeraj Kumar 9mo AI Can't Lie, But Can You Trust It? Andrew Mayer 6mo AI's Dark Side: Tripping on its own algorithms - The Risks of hallucinations and misinformation Fred H. 8mo How AI is reshaping the world: the Good, the Bad, and the Ugly Iman Sheikhansari 8mo Testing the limits of AI Ramachandran S 6y What is Artificial Intelligence? Ian Pavlik 5y What next for AI? Kate Clarke - ACA 6y Maintaining Control in the Face of Artificial Intelligence This Anthro Life Podcast 3mo What If AI Is Only a Cost and Not a Profit? Vlad Vasilev 9mo Show more Show less Explore topics Sales Marketing Business Administration HR Management Content Management Engineering Soft Skills See All LinkedIn © 2024 About Accessibility User Agreement Privacy Policy Cookie Policy Copyright Policy Brand Policy Guest Controls Community Guidelines العربية (Arabic) Čeština (Czech) Dansk (Danish) Deutsch (German) English (English) Español (Spanish) Français (French) हिंदी (Hindi) Bahasa Indonesia (Indonesian) Italiano (Italian) 日本語 (Japanese) 한국어 (Korean) Bahasa Malaysia (Malay) Nederlands (Dutch) Norsk (Norwegian) Polski (Polish) Português (Portuguese) Română (Romanian) Русский (Russian) Svenska (Swedish) ภาษาไทย (Thai) Tagalog (Tagalog) Türkçe (Turkish) Українська (Ukrainian) 简体中文 (Chinese (Simplified)) 正體中文 (Chinese (Traditional)) Language